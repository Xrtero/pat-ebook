# Chapter 7. General Directional Bottom-Up Parsing
As explained in Section 3.2.2, directional bottom-up parsing is conceptually very simple. At all times we are in the possession of a sentential form that derives from the input text through a series of leftmost reductions. These leftmost reductions dur- ing parsing correspond to rightmost productions that produced the input text: the ﬁrst leftmost _re_duction corresponds to the last rightmost _pro_duction, the second cor- responds to the one but last, etc.

There is a cut somewhere in this sentential form which separates the already reduced part (on the left) from the yet unexamined part (on the right). See Figure 7.1. The part on the left is called the “stack” and the part on the right “rest of input”.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062127941.png)

The latter contains terminal symbols only, since it is an unprocessed part of the original sentence, while the stack contains a mixture of terminals and non-terminals, resulting from recognized right-hand sides. We can complete the picture by keeping the partial parse trees created by the reductions attached to their non-terminals. Now all the terminal symbols of the original input are still there; the terminals in the stack are one part of them, another part is semi-hidden in the partial parse trees and the rest is untouched in the rest of the input. No information is lost, but structure has been added. When the bottom-up parser has reached the situation where the rest of the input is empty and the stack contains only the start symbol, we have achieved a parsing and the parse tree will be dangling from the start symbol. This view clearly exposes the idea that parsing is nothing but structuring the input.

The cut between stack and rest of input is often drawn as a gap, for clarity and since in actual implementations the two are often represented by quite different data structures in the parser. Note that the stack part corresponds to the open part of the sentential form when doing rightmost derivation, as discussed in Section 5.1.1.

Our non-deterministic bottom-up automaton can make only two moves: shift and reduce; see Figures 7.2 and 7.3. During a shift, a (terminal) symbol is shifted from the rest of input to the stack; **t**1 is shifted in Figure 7.2. During a reduce move, a

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062135245.png)

number of symbols from the right end of the stack, which form the right-hand side of a rule for a non-terminal, are replaced by that non-terminal and are attached to that non-terminal as the partial parse tree. $\mathbf{NcNb}\mathbf{ta}$ is reduced to **R** in Figure 7.3. We

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062135660.png)

see that the original $\mathbf{N_C N_b T_a}$ are still present inside the partial parse tree. There is, in principle, no harm in performing the instructions backwards, an _unshift_ and an _unreduce_, although they would seem to move us away from our goal, which is to obtain a parse tree. We shall see that we need them to do backtracking.

At any point in time the machine can either shift (if there is an input symbol left) or not, or it can do one or more reductions, depending on how many right-hand sides can be recognized. If it cannot do either, it will have to resort to the backtrack moves, to ﬁnd other possibilities. And if it cannot even do that, the parsing is ﬁnished, and the machine has found all (zero or more) parses.

We see that a parent node in the parse tree is identiﬁed after all its children have been identiﬁed: the parent **R** is not identiﬁed until each of its children $\mathbf{N_c,N_b}$, and **t**a have been recognized and put on the stack. This order of creating and visiting a tree is called “post-order”.

## 7.1 **Parsing** **by** **Searching**

The only problem left is how to guide the automaton through all of the possibilities. This is easily recognized as a search problem, which can be handled by a depth-ﬁrst or a breadth-ﬁrst method. We shall now see how the machinery operates for both search methods. Since the effects are exponential in size, even the smallest example gets quite big and we shall use the unrealistic grammar of Figure 7.4. The test input is $aaaab$.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062137281.png)

### 7.1.1 Depth-First (Backtracking) Parsing

Refer to Figure 7.5, where the gap for a shift is shown as $\lceil$ and that for an unshift as 7. At first the gap is to the left of the entire input (frame a ) and shifting is the only alternative; likewise with frame _b_ and _c_. In frame d we have a choice, either to shift, or to reduce using rule 3 . We shift, but remember the possible reduction(s); the rule numbers of these are shown as subscripts to the symbols in the stack. The same happens in frame e. In frame $\mathbf{f}$ we have reached a position in which the shift fails, the reduce fails (there are no right-hand sides $\mathbf{b,ab,aab,aaab}$ , or $\mathbf{aaabb}$) and there are no stored alternatives on the b. So we start backtracking by unshifting (g). Here we find a stored alternative, "reduce by 4 ", which we apply (h), deleting the index for the stored alternative in the process. Now we can shift again (i). No more shifts are possible, but a reduce by rule 1 gives us a parsing (j), indicated by a $\triangleleft$. After having

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062149956.png)
enjoyed our success we unreduce(_k_); note that frame _k_ only differs from frame _i_ in that the stored alternative 1 has been consumed. Unshifting, unreducing and again unshifting brings us to frame _n_ where we ﬁnd a stored alternative, “reduce by 3”. After reducing (_o_) we can shift again, twice (_p_, _q_). A “reduce by 2” produces the second parsing(_r_). The rest of the road is barren: unreduce, unshift, unshift, unreduce(_v_)and three unshifts bring the automaton to a halt, with the input reconstructed (_y_).

### 7.1.2 Breadth-First (On-Line) Parsing

Breadth-ﬁrst bottom-up parsing is simpler than depth-ﬁrst, at the expense of a far larger memory requirement. Since the input symbols will be brought in one by one (each causing a shift, possibly followed by some reduces), our representation of a partial parse will consist of the stack only, together with its attached partial parse trees. We shall never need to do an unshift or unreduce. Refer to Figure 7.6, where the gap is indicated by a (non-directional) **|**.

We start our solution set with only one empty stack (_a1_). Each parse step consists of two phases. In phase one the next input symbol is appended to the right of all stacks in the solution set; in phase two all stacks are examined and if they allow one or more reductions, one or more copies are made of it, to which the reductions are applied. This way we will never miss a solution. The ﬁrst and second **a** are just appended (_b1_, _c1_), but the third allows a reduction (_d2_). The fourth causes one more

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062153941.png)
reduction (_e3_) and the ﬁfth gives rise to two reductions, each of which produces a parsing (_f4_ and _f5_).

### 7.1.3 **A** **Combined** **Representation**

The conﬁgurations of the depth-ﬁrst parser can be combined into a single graph; see Figure 7.7(_a_) where numbers indicate the order in which the various shifts and reduces are performed. Shifts are represented by lines to the right and reduces by upward arrows. Since a reduce often combines a number of symbols, the additional symbols are brought in by arrows that start upwards from the symbols and then turn right to reach the resulting non-terminal. These arrows constitute at the same time

the partial parse tree for that non-terminal. Start symbols in the rightmost column with partial parse trees that span the whole input head complete parse trees.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062154783.png)

If we complete the stacks in the solution sets in our breadth-ﬁrst parser by ap- pending the rest of the input to them, we can also combine them into a graph, and, what is more, into the same graph; only the action order as indicated by the numbers is different, as shown in Figure 7.7(_b_). This is not surprising, since both represent the total set of possible shifts and reduces: depth-ﬁrst and breadth-ﬁrst are just two different ways to visit all nodes of this graph. Figure 7.7(_b_) was drawn in the same form as Figure 7.7(_a_). If we had drawn the parts of the picture in the order in which they are executed by the breadth-ﬁrst search, many more lines would have crossed. The picture would have been equivalent to (_b_) but much more complicated.

### 7.1.4 **A** **Slightly** **More** **Realistic** **Example**

The above algorithms are relatively easy to understand and implement; see, for ex- ample, Hext and Roberts [15] for Dömölki’s method to ﬁnd all possible reductions simultaneously. Although they require exponential time in general, they behave rea- sonably well on a number of grammars. Sometimes, however, they will burst out in a frenzy of senseless activity, even with an innocuous-looking grammar (especially with an innocuous-looking grammar!). The grammar of Figure 7.8 produces alge- braic expressions in one variable, **a**, and two operators, **+** and **-**. **Q** is used for the operators, since **O** (oh) looks too much like **0** (zero).
![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062156295.png)
This grammar is unambiguous and for **a-a+a** it has the correct production tree which restricts the minus to the following **a** rather than to **a+a**.
![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062156977.png)
Figure 7.9 shows the graph searched while parsing **a-a+a**. It contains 109 shift lines and 265 reduce arrows and would ﬁt on the page only thanks to the exceedingly ﬁne print the photo- typesetter is capable of. This is exponential explosion.
![7 General Directional Bottom-Up Parsing 7.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062211974.png)

## 7.2 **The** **Earley** **Parser**

In spite of their occasionally vicious behavior, breadth-ﬁrst bottom-up parsers are attractive since they work on-line, can handle left recursion without any problem and can generally be doctored to handle e-rules and loops. So the question remains how to curb their needless activity. Many methods have been invented to restrict the search breadth to at most 1, at the expense of the generality of the grammars these methods can handle; see Chapter 9. A method that restricts the fan-out to reasonable proportions while still retaining full generality was developed by Earley [14].

### 7.2.1 **The** **Basic** **Earley** **Parser**

When we take a closer look at Figure 7.9, we see after some thought that many reductions are totally pointless. It is not meaningful to reduce the third **a** to **E** or **S** since these can only occur at the end if they represent the entire input; likewise the reduction of **a-a** to **S** is absurd, since **S** can only occur at the end. Earley noticed that what was wrong with these spurious reductions was that they were incompatible with a top-down parsing, that is: they could never derive from the start symbol. He then gave a method to restrict our reductions only to those that derive from the start symbol; the method is now known as _Earley_ _parsing_. We shall see that the resulting parser takes at most $\mathbf{n^3}$ units of time for input of length $n$ rather than $\mathbf{C_n}$ for some constant **C**.

Earley’s parser can also be described as a breadth-ﬁrst top-down parser with bottom-up recognition, which is how it is explained by the author [14]. Since it can, however, handle left recursion directly but needs special measures to handle e-rules, we prefer to treat it as a bottom-up method with a top-down component.

We shall again use the grammar from Figure 7.8 and parse the input **a-a+a**. Just as in the non-restricted algorithm from Section 7.1.1, we have at all times a set of partial solutions which is modiﬁed by each symbol we read. We shall write the sets between the input symbols as we go; we have to keep earlier sets, since they will still be used by the algorithm. Unlike the non-restricted algorithm, in which the sets contained stacks, the sets consist of what is technically known as _items_, or _Earley items_ to be more precise. An “item” is a grammar rule with a gap in its right-hand side; the part of the right-hand side to the left of the gap (which may be empty) has already been recognized, the part to the right of the gap is predicted. The gap is traditionally shown as a fat dot: $\bullet$. Examples of items are: $\mathbf{E} \rightarrow \bullet \mathbf{E Q F}, \mathbf{E} \rightarrow \mathbf{E} \bullet \mathbf{Q F}, \mathbf{E} \rightarrow \mathbf{E} Q \bullet \mathbf{F}, \mathbf{E} \rightarrow \mathbf{E} Q \mathbf{Q} \bullet, \mathbf{F} \rightarrow \mathbf{a} \bullet$, etc. It is unfortunate that such a vague every-dayterm as “item” has become endowed with a very speciﬁc technical meaning, but the expression has taken hold, so it will have to do.

Items have quite different properties depending on exactly where the dot is, and the following types can be distinguished.

- An item with the dot at the end is called a _reduce item_, since the dot at the end means that the whole right-hand side has been recognized and can be reduced.
- An item with the dot at the beginning (just after the arrow) is known as a _predicted_ _item_, since it results from a prediction, as we shall see below.
- An item with the dot in front of a terminal is called a _shift_ _item_, since it allows a shift of the terminal.
- An item with the dot in front of a non-terminal does not have a standard name; we shall call it a _prediction item_, since it gives rise to predictions.
- An item with the dot not at the beginning is sometimes referred to as a _kernel item_, since at least part of it has been conﬁrmed.

Although all items fall in at least one of the above classes, some fall in more than one: the types in this classiﬁcation are not mutually exclusive. For example, the item $\mathbf{F} \rightarrow \bullet \mathbf{a}$ is both a predicted and a shift item.

An _Earley_ _item_ is an item as deﬁned above, with an indication of the position of the symbol at which the recognition of the recognized part started, its _origin_ _position_. Notations vary, but we shall write @_n_ after the item (read: “at _n_”). If the set at the end of position 7 contains the item $\mathbf{E} \rightarrow \mathbf{E} \bullet \mathbf{Q F} @ 3$ , we have recognized an **E** in positions 3 through 7 and are looking forward to recognizing **QF**.

The sets of items contain exactly those items 1) of which the part before the dot has been recognized so far and 2) of which we are certain that we shall be able to use the result when they will happen to be recognized in full (but we cannot, of course, be certain that that will happen). For example, if a set contains the item $\mathbf{E} \rightarrow \mathbf{E} \bullet \mathbf{Q F} @ 3$, we can be sure that when we will have recognized the whole right-hand side **EQF**, we can go back to the set at the beginning of symbol number 3 and ﬁnd there an item that was looking forward to recognizing an **E**, i.e., that had an **E** with a dot in front of it. Since that is true recursively, no recognition will be useless; of course, the recognized **E** is part of a right-hand side under construction and the full recognition of that right-hand side may eventually fail.

#### 7.2.1.1 **The** **Scanner,** **Completer** **and** **Predictor**

The construction of an item set from the previous item set proceeds in three phases. The first two correspond to those of the non-restricted algorithm from Section 7.1.1, where they were called "shift" and "reduce"; here they are called "Scanner" and "Completer". The third is new and is related to the top-down component; it is called "Predictor".

The Scanner, Completer and Predictor operate on a number of interrelated sets of items for each token in the input. Refer to Figure 7.10, where the input symbol $\sigma_{p}$ at position p is surrounded by several sets: $itemset _{p}$, which contains the items available just before $\sigma_{p}$ ; $completed _{p+1}$, the set of items that have become completed due to $\sigma_{p}$; $active _{p+1}$, which contains the non-completed items that passed $\sigma_{p}$; and $predicted _{p+1}$, the set of newly predicted items. The sets $active _{p+1}$ and $predicted { }_{p+1}$ together form $itemset _{p+1}$. Initially, $itemset { }_{p}$ is filled (as a result of processing $\sigma_{p-1}$ ) and the other sets are empty; $itemset _{1}$ is filled from the start symbol.

The Scanner looks at $\sigma_{p}$, goes through itemset p and makes copies of all items that contain $\bullet \sigma$; all other items are ignored. In the copied items, the part before the dot was already recognized and now $\sigma$ is recognized; consequently, the Scanner changes - $\sigma$ into $\sigma \cdot$. If the dot is now at the end, the Scanner has found a reduce item and stores it in the set $completed { }_{p+1}$; otherwise it stores it in the set $active _{p+1}$.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062228411.png)

Next the Completer inspects $completed _{p+1}$, which contains the items that have just been recognized completely and can now be reduced as follows. For each item of the form $R \rightarrow \cdots \bullet @ m$ the Completer goes to $itemset _{m}$, and calls the Scanner in a special way as follows. The Scanner, which was used to working before on the terminal $\sigma_{p}$ found in the input and $itemset { }_{p}$, is now directed to work on the nonterminal R recognized by the Completer and $itemset _{m}$. Just as for a terminal it copies all items in $itemset _{m}$ featuring a $\bullet R$, replaces the $\bullet R by R \bullet$ and stores them in either $completed _{p+1}$ or $active _{p+1}$, as appropriate. This can add new recognized items to the set $completed _{p+1}$, which just means more work for the Completer. After a while, all completed items have been reduced, and the Predictor's turn has come.
The Predictor goes through the sets $active _{p+1}$, which was filled by the Scanner, and $predicted _{p+1}$, which is empty initially, and considers all items in which the dot is followed by a non-terminal. We expect to see these non-terminals in the input, and the Predictor predicts them as follows. For each such non-terminal N and for each rule for that non-terminal N \rightarrow P \cdots, the Predictor adds an item $N \rightarrow \bullet P \cdots @ p+1$ to the set $predicted _{p+1}$. This may introduce new predicted non-terminals (for example P ) in $predicted _{p+1}$ which cause more predicted items. After a while, this too will stop.
The sets $active _{p+1}$ and $predicted { }_{p+1}$ together form the new $itemset t_{p+1}$. If the completed set after the last symbol in the input contains an item $S \rightarrow \cdots \bullet @ 1$, that is, an item spanning the entire input and reducing to the start symbol, we have found a parsing.

Now refer to Figure 7.11, which shows the item sets of the Earley parser working on $\mathbf{a-a+a}$ with the grammar from Figure 7.8. In this and following drawings, the sets $active_{p}$, $predicted_{p}$, and $itemset_{p}$ have been combined into one set; the internal division between activep and $predicted_{p}$ is indicated in the drawings by a dotted line.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062234518.png)

The initial active item set $active _{1}$ is $\{\mathrm{S} \rightarrow \bullet \mathrm{E@Q}\}$, indicating that this is the only item that can derive directly from the start symbol. The Predictor first predicts already) and from the last one $\mathbf{F} \rightarrow \bullet \mathbf{a} @ 1$. This gives itemset 1 .

The Scanner working on $itemset _{1}$ and scanning for an a, only catches $\mathbf{F} \rightarrow \bullet \mathbf{a} @ 1$, turns it into $\mathrm{F} \rightarrow \mathrm{a} \bullet 1$, and stores it in $completed _{2}$. This means not only that we have recognized and reduced an F, but also that we have a buyer for it. The Completer goes to the set $itemset _{1}$ and copies all items that have \bullet F. Result: one item, $\mathbf{E} \rightarrow \bullet F @ 1$, which turns into $\mathbf{E} \rightarrow \mathrm{F} \bullet @ 1$ and is again stored in $completed _{2}$. More work for the Completer, which will now copy items containing $\bullet E$. Result: two items, $S \rightarrow \bullet E @ 1$ which becomes $\mathrm{S} \rightarrow \mathrm{E} \bullet @ 1$ and goes to the completed set, and $\mathrm{E} \rightarrow \bullet \mathrm{EQF} @ 1$ which becomes $\mathbf{E} \rightarrow \mathbf{E} \bullet Q F @ 1$, and which becomes the first and only member of $active e_{2}$. The completion of $\mathbf{S}$ yields no new information.

The Predictor working on $active _{2}$ has an easy job: $\bullet \mathcal{Q}$ causes two items for Q, both with @2, since that is where recognition will have started, if it occurs at all. Nothing spectacular happens until the Scanner processes the second a; from itemset 3 it extracts $\mathbf{F} \rightarrow \bullet \mathbf{a} @ 3$ which gives $\mathbf{F} \rightarrow \mathbf{a} \bullet \mathbf{Q}$, which is passed to the Completer (through $completed _{4}$ ). The latter sees the reduction of a to F starting at position 3 , goes to $itemset _{3}$ to see who ordered an F, and finds $\mathbf{E} \rightarrow E Q \bullet F @ 1$. Given the F, this turns into $\mathbf{E} \rightarrow \mathbf{E Q F} \bullet @ 1$, which in its turn signals the reduction to $\mathbf{E}$ of the substring from 1 to 3 (again through $completed _{4}$ ). The Completer checks $itemset _{1}$ and finds two clients there for the $\mathbf{E}: \mathrm{S} \rightarrow \bullet \mathrm{E} @ 1$ and $\mathbf{E} \rightarrow \bullet \mathrm{EQF} @ 1$; the first ends up as $\mathrm{S} \rightarrow \mathrm{E} \bullet @ 1 in completed _{4}, the second as \mathbf{E} \rightarrow \mathrm{E} \cdot Q \mathrm{~F} @ 1 in active _{4}$.
After the last symbol has been processed by the Scanner, we still run the Completer to do the final reductions, but running the Predictor is useless, since there is nothing to predict any more. Note that the parsing started by calling the Predictor on the initial active set and that there is one Predictor/Scanner/Completer action for each symbol. Since $completed _{6}$ indeed contains an item $\mathrm{S} \rightarrow \mathrm{E} \bullet @ 1$, there is at least one parsing.

#### 7.2.1.2 **Constructing** **a** Parse** Tree

All this does not directly give us a parse tree. As is often the case in parser construc- tion (see, for example, Section 4.1), we have set out to build a parser and have ended up building a recognizer. The intermediate sets, however, contain enough informa- tion about fragments and their relations to construct a parse tree easily. As with the CYK parser, a simple top-down Unger-type parser can serve for this purpose, since the Unger parser is very interested in the lengths of the various components of the parse tree and that is exactly what the sets in the Earley parser provide. In his 1970 article, Earley gives a method of constructing the parse tree(s) while parsing, by keeping with each item a pointer back to the item that caused it to be present. Tomita [162, p. 74-77] has, however, shown that this method will produce incorrect parse trees on certain ambiguous grammars.
The set completed 6 in Figure 7.11 , which is the first we inspect after having finished the set construction, shows us that there is a parse possible with $\mathbf{S}$ for a root and extending over symbols 1 to 5 ; we designate the parse root as $\mathbf{S}_{1-5}$ in Figure 7.12. Given the completed item $\mathbf{S} \rightarrow \mathrm{E} \bullet @ 1$ in $completed _{6}$ there must be a parse node $\mathbf{E}_{1-5}$, which is completed at 5 . Since all items completed after 5 are contained in $completed _{6}$, we scan this set to find a completed $\mathbf{E}$ with origin position 1 ; we find $\mathrm{E} \rightarrow \mathrm{EQF} \bullet @ 1$. This gives us the parse tree in frame a, where the values at the question marks are still to be seen. Since items are recognized at their right ends, we start by finding a parse for the $\mathbf{F}_{\text {?-5 }}$, to be found in $completed _{6}$. We find $\mathbf{F} \rightarrow \mathbf{a} \bullet @ 5$, giving us the parse tree in frame b. It suggests that we find a parse for $\boldsymbol{Q}_{\text {?-4 }}$ completed after 4; in completed 5 we find $\mathbf{Q} \rightarrow+\bullet @ 4$. Consequently $\mathbf{Q}_{?-4} is \mathbf{Q}_{4-4}$ and the \mathbf{E}_{1-?} in frame b must be $\mathbf{E}_{1-3}$. This makes us look in completed 4 for an $\mathbf{E} \rightarrow \cdots @ 1$, where we find $\mathbf{E} \rightarrow \mathrm{EQF} \bullet @ 1$. We now have a parse tree (c), and, using the same techniques, we easily complete it (d).

#### 7.2.1.3 **Space** **and** **Time** **Requirements**

It is interesting to have a look at the space and time needed for the construction of the sets. First we compute the maximum size of the sets just after symbol number p. There is only a fixed number of different items, I, limited by the size of the grammar; for our grammar it is $I=14$. However, each item can occur with any of the origin $positions@1$ to $@ p+1$, of which there are $p+1$ . So the number of items in the sets just after symbol number p is limited to $I \times(p+1)$. The exact computation of the

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062252399.png)
maximum number of items in each of the sets is complicated by the fact that different rules apply to the first, last and middle items. Disregarding these complications, we find that the maximum number of items in all sets up to p is roughly $I \times p^{2} / 2.$ So, for an input of length n, the memory requirement is $O\left(n^{2}\right)$, as with the CYK algorithm. In actual practice, the amount of memory used is often far less than this theoretical maximum. In our case all sets together could conceivably contain about $14 \times 5^{2} / 2= 175$ items, with which the actual number of 4+3+3+1+2+3+3+1+2+3+1= 26 items compares very favorably.

Although a set at position p can contain a maximum of $O(p)$ items, it may require an amount of work proportional to $p^{2}$ to construct that set, since each item could, in principle, be inserted by the Completer once from each preceding position. Under the same simplifying assumptions as above, we find that the maximum number of actions needed to construct all sets up to p is roughly $I \times p^{3} / 6$. So the total amount of work involved in parsing a sentence of length n with the Earley algorithm is $O\left(n^{3}\right)$, as it is with the CYK algorithm. Again, in practice it is much better: on many grammars, including the one from Figure 7.8, it will work in linear time (O(n)) and on any unambiguous grammar it will work in $O\left(n^{2}\right)$. In our example, a maximum of about $14 \times 5^{3} / 6 \simeq 300$ actions might be required, compared to the actual number of 28 (both items for $\mathbf{E}$ in $predicted _{1}$ were inserted twice).

It should be noted that once the computation of the sets is finished, only the completed sets are consulted. The active and predicted sets can be thrown away to make room for the parse tree(s).

The practical efficiency of this and the CYK algorithms is not really surprising, since in normal usage most arbitrary fragments of the input will not derive from any non-terminal. The sentence fragment "letter into the upper leftmost" does not represent any part of speech, nor does any fragment of size larger than one. The $O\left(n^{2}\right)$ and $O\left(n^{3}\right)$ bounds only materialize for grammars in which almost all nonterminals produce almost all substrings in almost all combinatorially possible ways, as for example in the grammar $\mathbf{S} \rightarrow \mathbf{S S}$, $\mathbf{S} \rightarrow \mathbf{x}$.

### 7.2.2 **The** **Relation** **between** **the** **Earley** **and** **CYK** **Algorithms**

The similarity in the time and space requirement between the Earley and the CYK algorithm suggest a deeper relation between the two and indeed there is: the Earley sets can be accommodated in a CYK-like grid, as shown in Figure 7.13. The horizontal axis of the CYK matrix represents the position where recognition started; its vertical level represents the length of what has been recognized. So an Earley item of the form $A \rightarrow \alpha \bullet \beta @ q$ in itemset _{p} goes to column q, since that is where its recognition started, and to level p-q since that is the length it has recognized. So the contents of an Earley set is distributed over a diagonal of the CYK matrix, slanting from north-west to south-east. Completed items are drawn in the top left corner of a box, active and predicted items in the bottom right corner. But since predicted items have not yet recognized anything they occur in the bottom layer only. When the reader turns Figure 7.13 clockwise over $45^{\circ}$, the Earley set can be recognized by stacking the boxes along the arrows at the bottom.

When we compare this picture to that produced by the CYK parser (Figure 7.14) we see correspondences and differences. Rather than having items, the boxes contain non-terminals only. All active and predicted items are absent. The left-hand sides of the completed items also occur in the CYK picture, but that parser features more recognized non-terminals; from the Earley picture we know that these will never play a role in any parse tree. The costs and the effects of the top-down restriction are clearly shown.

The correspondence between the Earley and CYK algorithms has been analysed by Graham and Harrison [19]. This has resulted in a combined algorithm described by Graham, Harrison and Ruzzo [23]. For a third, and very efficient representation of the CYK/Earley data structure see Kruseman Aretz [29].

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062306047.png)

There is another relationship between the Earley and CYK algorithms, which comes to light when the Earley data structure is expressed in array form (Section 4.3). The table is shown in Figure 7.15, where the vertical axis enumerates the items and the entries contain the lengths recognized for each item. We see that the entry for item $\mathrm{S} \rightarrow \mathrm{E} \bullet$ at position 1 holds a 5 (among other lengths) indicating that the whole input can be parsed using rule $S \rightarrow E$ from position 1 .

When we compare this table to the one in Figure 4.21, we see that the items take the positions of non-terminals, which suggests that they can be viewed as nonterminals in a grammar. And indeed they can. The grammar is shown in Figure 7.16; the items enclosed in \{ and \} are names of new non-terminals, in spite of their looks.

We see that the grammar contains three kinds of rule. Those of the form $A \rightarrow\{A \rightarrow \bullet \alpha\}|\{A \rightarrow \bullet \beta\}| \cdots$ predict items for A and correspond to the Predictor. Those of the form $\{A \rightarrow \cdots \bullet X \cdots\} \rightarrow X\{A \rightarrow \cdots X \bullet \cdots\}$ correspond to the Scanner when X is a terminal and to the special Scanner inside the Completer when X is a non-terminal. And those of the form $\{A \rightarrow \cdots \bullet\} \rightarrow \varepsilon$  correspond to the Completer.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310062307773.png)

The most important point, however, is that no right-hand side in such an item grammar contains more than two non-terminals. As we have seen (page 116), the CYK parser has $O\left(n^{3}\right)$ (cubic) time requirements only when operated with a grammar with at most two non-terminals in any right-hand side. (Since such grammars give rise to binary trees as parse trees, they are said to be in binary form.) But the Earley parser has cubic time requirements for any grammar, and we now see why: the Completer/Scanner mechanism chops up the longer right-hand side into steps of 1 , in a process similar to that in Section 4.2.3.4, thus creating a binary form grammar on the fly!

### 7.2.3 Handling ε-Rules

Like most parsers, the above parser cannot handle \varepsilon-rules without special measures. $\varepsilon-rules$ show up first as an anomaly in the work of the Predictor. While predicting items of the form $A \rightarrow \bullet \ldots @ p$ as a consequence of having a \bullet A in an item in $active _{p}$ or $predicted _{p}$, it may happen to create an empty prediction $A \rightarrow \bullet @ p$. This means that the non-terminal A has been completed just after symbol number p and this completed item should be added to the set $completed _{p}$, which up to now only contained items with origin position $p-1$ at most. So we find that there was more work for the Completer after all. But that is not the end of the story. If we now run the Completer again, it will draw the consequences of the newly completed item(s)

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070011787.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070012803.png)
at origin position p. So it will consult $itemset _{p}$, which is, however, incomplete since items are still being added to its constituents, $active e_{p}$ and $predicted _{p}$. If it finds items with occurrences of $\bullet A$ there, it will add copies with $A \bullet$ instead. Part of these may require new predictions to be made (if the dot lands in front of another non-terminal), and part of them may be completed items, which will have to go into $completed _{p}$ and which will mean more work for the Completer. The items in this set can have starting points lower than p, which bring in items from further back, to be added to itemset p.
And of course these may or may not now be completed through this action or through empty completed items. Etc.

#### 7.2.3.1 The Completer/Predictor Loop

The easiest way to handle this mare's nest is to stay calm and keep running the Predictor and Completer in turn until neither has anything more to add. Since the number of items is finite this will happen eventually, and in practice it happens sooner rather than later. (This is again a closure algorithm.)

The Completer and Predictor loop has to be viewed as a single operation called "X" by Graham, Harrison and Ruzzo [23]. Just like the Predictor it has to be applied to the initial state, to honor empty productions before the first symbol; just like the Completer it has to be applied to the final state, to honor empty productions after the last symbol.

Part of the effects are demonstrated by the grammar of Figure 7.17 which is based on a grammar similar to that of Figure 7.8. Rather than addition and subtraction, this

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070017889.png)
one handles multiplication and division, with the possibility to omit the multiplication sign: $\mathbf{aa}$ means $\mathbf{{a}\times{a}}$.
The parsing is given in Figure 7.18. The items pointed at by a $\triangleright$ have been
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070020210.png)
added by a second pass of the Completer/Predictor. The $Q \rightarrow \bullet @ 2$, inserted by the Predictor into $completed _{2}$ as a consequence of $\mathbf{E} \rightarrow \mathrm{E} \cdot \mathrm{QF} @ 1$ in $active _{2}$, is picked up by the second pass of the Completer, and is used to clone $\mathrm{E} \rightarrow \mathrm{E} \bullet \mathrm{QF} @ 1$ in $active _{2}$ into $E \rightarrow E Q \bullet F @ 1$. This in turn is found by the Predictor which predicts the item $F \rightarrow \bullet a @ 2$ from it.
Note that we now do have to create the full activelpredicted set after the last symbol, since its processing by the Completer/Predictor may insert an item of the form $\mathbf{S} \rightarrow \cdots @ 1$ in the last completed set, indicating a parsing.

7.2.3.2 Modifying the Predictor

Aycock and Horspool [38] show a way to avoid the Completer/Predictor loop. Before parsing we determine which non-terminals can produce the empty string, in other words, which non-terminals are nullable; we will see below in Section 7.2.3.3 how to do this. The processing of items during parsing is then arranged as follows:

- The items at a position $p$ are put in a list, $list_{p}$. This list is initialized with the items produced by the Scanner moving items from $list t_{p-1}$ over token $\sigma_{p-1}$.
- The items in the list are treated one by one by a Driver, in the order they appear in the list. If the item has the dot in front of a terminal, the Driver offers it to the Scanner; if the item has the dot in front of a non-terminal, the Driver offers it to the Predictor; otherwise the item has the dot at the end and is offered to the Completer. Any item resulting from this that must be inserted into the present list is added at the end, in order, unless it is already in the list, in which case it is discarded.
- The Predictor is modified as follows. When presented with an item $A \rightarrow \cdots \bullet B \cdots$ it predicts all items of the form $B \rightarrow \bullet \cdots$ as usual, but if $\mathbf{B}$ is nullable it also predicts the item $A \rightarrow \cdots B \bullet \cdots$.

It is clear that the loop has gone: each item gets treated exactly once. It is less obvious that this arrangement produces exactly the item sets that would have resulted from the Completer/Predictor loop.

When working on $list _{p}$ ， the Driver examines the items in turn and distributes them over the Scanner, the Predictor and the Completer. There is a fundamental difference between the Scanner and the Predictor on the one hand and the Completer on the other: the Scanner and the Predictor use only the item they are given (plus the next input token for the Scanner and the grammar rules for the Predictor), but the Completer combines it with other items from far-away and near-by places. The Completer takes one item of the form $A \rightarrow \cdots \bullet @ q$ from $list_{p}$ ， goes to $list _{q}$ ， finds all items of the form $B \rightarrow \cdots \bullet A \cdots @ r$ in that list and puts corresponding items $B \rightarrow \cdots A \bullet \cdots @ r$ in $list _{p}$. Now that is fine as long as $q<p$, since then $list _{q}$ is already finished, and the scan will find all items. But when $q=p$, $list_{q}$, the list that is scanned, and $list_{p}$ ， the list under construction, are the same, and the Completer scan may miss
$$\begin{aligned} \mathbf{S}_{\mathrm{S}} & \rightarrow \mathrm{A} \mathbf{A} \mathbf{x}
 \mathbf{A} & \rightarrow \varepsilon\end{aligned}$$
which produces only one string: $\mathbf{x}$. Let us assume for the moment that we are still using the original Earley Predictor. The starting list $list _{1}$ is initialized with the item $\mathbf{S} \rightarrow \bullet A A x @ 1$. The Driver examines it and passes it to the Predictor, which predicts the item $\mathbf{A} \rightarrow \bullet @ 1$ from it, which is appended to $list _{1}$. The Driver immediately picks it up, and offers it to the Completer, which scans list _{1}, combines the item with $\mathbf{S} \rightarrow \bullet A A \mathbf{A} @ 1$ and produces $\mathbf{S} \rightarrow \mathbf{A} \bullet \mathbf{A x} @ 1$. The list l_{i s t} t_{1} now looks as follows:
$$\begin{array}{l}\mathrm{S} \rightarrow \bullet A A x @ 1
 \mathrm{~A} \rightarrow \bullet @ 1
 \mathrm{~S} \rightarrow \mathrm{A} \bullet \mathrm{Ax} @ 1\end{array}$$
Next the Driver turns to $S \rightarrow A \bullet A x @ 1$, and gives it to the Predictor. Again the predicted item A \rightarrow \bullet @ 1 results, but since it is already in the list, it is not appended again. So the Driver does not find a new item, and stops.
We see that the resulting state does not accept the input token $\mathbf{x}$. So it is wrong, but we are not surprised: the original algorithm would have gone on processing items until nothing changed any more. Soon it would have passed the item $A \rightarrow \bullet @ 1$ to the Completer again, which would then have produced the item $\mathbf{S} \rightarrow \mathbf{A A} \bullet \mathbf{x} @ 1$, and all would be well. In particular we see that one inference from the completed $\varepsilon-item A \rightarrow \bullet @ 1$ is drawn, but later inferences are not, because each item is treated only once.
This is where the modified Predictor comes in: by predicting an item $A \rightarrow \cdots B \bullet \cdots @ p$ from $A \rightarrow \cdots B \bullet \cdots @ p$ when $B$ is nullable, it provides the inference from $B \rightarrow \bullet$ (or any other item that causes B to produce $\varepsilon$ ) even when the item is out of sight because it has already been processed. Applied to the item S \rightarrow A \cdot A x @ 1 it produces two predictions: \mathbf{A} \rightarrow \bullet @ 1, which is not appended, and \mathrm{S} \rightarrow \mathrm{AA} \bullet \mathbf{x} @ 1, which is. This yields the correct list _{1} :

$$\begin{array}{l}\mathrm{S} \rightarrow \bullet A \mathrm{Ax} @ 1
 \mathrm{~A} \rightarrow \bullet @ 1
 \mathrm{~S} \rightarrow \mathrm{A} \bullet \mathrm{Ax} @ 1
 \mathrm{~S} \rightarrow \mathrm{AA} \cdot \mathbf{x} @ 1\end{array}$$

Aycock and Horspool [38] give a formal proof of the correctness of their algorithm. Figure 7.19 shows the lists for the same parsing as in Figure 7.18.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070033691.png)

Another way of avoiding the Completer/Predictor loop is to do ε-elimination on the grammar, as described in Section 4.2.3.1, but that would make the subsequent construction of the parse tree(s) much harder.

#### 7.2.3.3 Determining Nullability

A simple closure algorithm allows us to find out which non-terminals in a grammar can produce the empty string (are nullable). First we scan the grammar and any time we find a rule of the form $A \rightarrow \varepsilon$ we mark A as nullable. Next we scan the grammar again and whenever we find a rule $P \rightarrow Q_{1} \cdots Q_{n}$ where $Q_{1} \cdots Q_{n}$ are all marked nullable, we mark P as nullable. Now we repeat the last step until no more non-terminals get marked. Then all nullable non-terminals have been marked.

### 7.2.4 Exploiting Look-Ahead

In the following paragraphs we shall describe a series of increasingly complicated (and more efficient) parsers of the Earley type. Somewhere along the line we will also meet a parser that is (almost) identical to the one described by Earley in his paper.

#### 7.2.4.1 Prediction Look-Ahead

When we go back to Figure 7.11 and examine the actions of the Predictor, we see that it sometimes predicts items that it could know were useless if it could look ahead at the next symbol. When the next symbol is a -, it is kind of foolish to proudly predict $\mathbf{Q} \rightarrow \bullet+@ 2$. The Predictor can of course easily be modified to check such simple cases, but it is possible to have a Predictor that will never predict anything obviously erroneous: all its predicted items will be either completed or active in the next set. Of course the predictions may fail on the symbol after that; after all, it is a Predictor, not an Oracle.
To see how we can obtain such a improved Predictor we need a different example, since after removing $Q \rightarrow \bullet+@ 2$ and $Q \rightarrow \bullet-@ 4$  from Figure 7.11 all predictions there come true, so nothing can be gained any more.
The artificial grammar of Figure 7.20 produces only the three sentences $\mathbf{p}$, $\mathbf{q}$ and$ \mathbf{p q}$, and does so in a straightforward way. The root is $\mathbf{S}^{\prime}$ rather than $\mathbf{S}$, which is a convenient way to have a grammar with only one rule for the root. This is not necessary but it simplifies the following somewhat, and it is common practice.
The parsing of the sentence $\mathrm{q}$ is given in Figures 7.21(a) and (b). Since we are now using look-ahead, we have appended an end-of-input marker **#** to the input, as explained on page 94. Starting from the initial item, the Predictor predicts a list of 7 items (frame a ). Looking at the next symbol, q, the Predictor could easily avoid the prediction $\mathrm{C} \rightarrow \bullet p@1$, but several of the other predictions are also false, for example, $A \rightarrow \bullet C @ 1$. The Predictor could avoid the first since it sees that it cannot begin with q. If it knew that $\mathbf{C}$ cannot begin with a $\mathbf{q}$, it could also avoid $\mathbf{A} \rightarrow \bullet \mathbf{C} @ 1$. (Note by the way that $itemset { }_{2}$ is empty, indicating that there is no way for the input to continue.)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070058837.png)
The required knowledge can be obtained by computing the FIRST sets of all nonterminals and their alternatives in the grammar. The FIRST set of a non-terminal A is the set of all tokens a terminal production of A can start with. Likewise, the FIRST set of an alternative \alpha is the set of all tokens a terminal production of $\boldsymbol{\alpha}$ can start with. These FIRST sets and a method of computing them are explained in Sections 8.2.1.1 and 8.2.2.1.

The FIRST sets of our grammar are shown in Figure 7.20. Since s has three alternatives, we need FIRST sets for each of them, to see which alternative(s) we must predict. FIRST(A) and FIRST(B) are already available as the FIRST sets of the non-terminals, but that of $\mathrm{AB}$ must be determined separately.

The use of the FIRST sets is very effective (frame b ). The Predictor again starts from the initial item, but since it knows that q is not in $\operatorname{FIRST(A)~or~FIRST(AB)}$ ,  it will avoid predicting $\mathbf{S} \rightarrow \bullet A @ 1$ and $\mathbf{S} \rightarrow \bullet A B @ 1$, and just predict $\mathbf{S} \rightarrow \bullet B @ 1.$ Items like $A \rightarrow \bullet C @ 1$ do not even have to be avoided, since their generation will never be contemplated in the first place. The item $\mathbf{S} \rightarrow \bullet B @ 1$ results in three predictions, all of them to the point.

As usual, $\varepsilon-rules$ have a big impact. If we add a rule $C \rightarrow \varepsilon$ to our grammar (Figure 7.22), the entire picture changes. Starting from the initial item $S^{\prime} \rightarrow \bullet S @ 1$ (Figure 7.23), the Predictor will still not predict $\mathbf{S} \rightarrow \bullet A @ 1$ since FIRST(A) does not contain $\mathbf{q}$, but it will predict $\mathbf{S} \rightarrow \bullet \mathbf{A B} @ 1$ since FIRST(AB) does contain a $\mathbf{q}. Next \mathbf{A} \rightarrow \bullet \mathbf{C} @ 1$ is predicted, followed by $\mathbf{C} \rightarrow \bullet @ 1$, but that is a completed item and goes into $completed { }_{1}$. When the Completer starts, it finds $\mathrm{C} \rightarrow \bullet @ 1$, applies it to $A \rightarrow \bullet C @ 1$ and produces $A \rightarrow C \bullet @ 1$, likewise completed. The latter is then applied to $S \rightarrow \bullet A B @ 1$ to produce the active item $S \rightarrow A \bullet B @ 1$. This causes another run of the Predictor, to follow the new $\bullet \mathbf{B}$, but all those items have already been added.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070136697.png)

An interesting problem occurs when we try to parse the empty sentence, or actually the sentence **#**, since an end marker is appended. If we follow the above algorithm, we find that the look-ahead token **#** is not in any of the FIRST sets, for the simple reason that it is not part of the grammar, so no rule gets predicted, and the input is rejected. One way to solve the problem is to decide to predict an item only when the look-ahead does not contradict it, rather than when the look-ahead confirms it. A FIRST set containing $\boldsymbol{\varepsilon}$ does not contradict the look-ahead **#** (in fact it does not contradict any look-ahead), so the rules $\mathbf{S} \rightarrow \mathbf{A}$, $\mathbf{A} \rightarrow \mathbf{C}$, and $\mathbf{C} \rightarrow \varepsilon$ get predicted. The resulting parsing is shown in Figure 7.24; we see that $completed _{1}$ contains $\mathbf{S \rightarrow A \bullet @ 1}$, so the input is recognized, as expected. The next section shows a way to improve this algorithm and indeed predict only what the look-ahead confirms.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070138000.png)

Once we have gone through the trouble of computing the FIRST sets, we can use them for a second type of look-ahead: reduction look-ahead. Prediction look-ahead reduces the number of predicted items, reduction look-ahead reduces the number of completed items. Referring back to Figure 7.11, which depicted the actions of an Earley parser without look-ahead, we see that it does two silly completions: $\mathrm{S} \rightarrow \mathrm{E} \bullet @ 1$ items stems from the fact that they are only meaningful at the end of the input. Now this may seem a very special case, not worth testing for, but the phenomenon can be viewed in a more general way: if we introduce an explicit symbol for end-of-file (for example, \#), we can say that the above items are redundant because they are followed by a symbol (- and +, respectively) which is not in the set of symbols that may follow the item on completion.

The idea is to keep, together with each item, a set of symbols which follow after that item, the reduction look-ahead set; if the item is a reduce item but the next symbol is not in this set, the item is not completed but discarded. The rules for constructing the look-ahead set for an item are straightforward, but unlike the prediction look-ahead it cannot be computed in advance; it must be constructed as we go. (A limited and less effective set could be computed statically, using the FOLLOW sets explained in Section 8.2.2.2.)

The initial item starts with a look-ahead set of **\[#\]** (look-ahead sets will be shown between square brackets at the end of items). When the dot advances in an item, its look-ahead set remains the same, since what happens inside an item does not affect what may come after it; only when a new item is created by the Predictor, a new look-ahead set must be composed. Suppose the parent item is
$$
P \rightarrow A \bullet B C D[a b c] @ n
$$
and predicted items for B must be created. We now ask ourselves what symbols may follow the occurrence of B in this item. It is easy to see that they are:
- any symbol C can start with,
- if C can produce the empty string, any symbol D can start with,
- if D can also produce the empty string, any of the symbols a, b and c.
Given the FIRST sets for all non-terminals, which can also tell us if a non-terminal can produce empty, the resulting new reduction look-ahead set is easily computed. It is also written as $FIRST( C D[a b c])$, which is of course the set of first symbols of anything produced by $C D a|C D b| C D c$.
The Earley sets with reduction look-ahead for our example $\mathbf{a}-\mathbf{a}+\mathbf{a}$ are given in Figure 7.25. The computation of the sets follows the above rules. The looktwice. Initially it is inserted by the Predictor from $\mathbf{S} \rightarrow \bullet \mathbf{E}[\#] @1$ , which contributes

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070146924.png)
the look-ahead **#**, and which results in the item $\mathbf{E} \rightarrow \bullet \mathbf{E Q F}[\#] @ 1$. When the Predictor processes this item, it predicts items for the $\bullet \mathrm{E}$ in it, with a look-ahead of $\operatorname{FIRST}(\mathrm{QF}[\#])$; this contributes $\mathbf{+-}$. These items include $\mathbf{E} \rightarrow \bullet E Q F[+-] @ 1$, which together with the item from $\mathbf{S} \rightarrow \bullet \mathbf{E}[\#] @ 1$ results in the first item we see in $predicted _{1}$.

Note that the item $\mathbf{S} \rightarrow \mathbf{E} \cdot[\#] @ 1$ is not placed in $completed _{2}$, since the actual symbol ahead ($-_2$) is not in the item's look-ahead set; something similar occurs in $completed _{4}$, but not in $completed _{6}$.

Now that we have reduction look-ahead sets available in each item, we can use them to restrict our predictions to those confirmed by the look-ahead. Refer again to the grammar of Figure 7.22 and the parsing in Figure 7.24. The initial item is $\mathbf{S}_{s}^{\prime} \rightarrow \mathbf{O S}[\#]$, which gives rise to three potential items: $\mathbf{S} \rightarrow \bullet \mathbf{A}[\#], \mathbf{S} \rightarrow \bullet \mathbf{A B}[\#]$, and $\mathbf{S} \rightarrow \bullet B[\#]$. Now we get $\operatorname{FIRST}(\mathbf{A}[\#])=\{\#, \mathbf{p}\}$, $\operatorname{FIRST}(\mathbf{A B}[\#])=\{\mathbf{p}, \mathbf{q}\}$, and $\operatorname{FIRST}(B[\#])=\{q\}$. And since the look-ahead is **#**, only the first item survives. This improvement does not affect our example in Figure 7.24, but in general this use of the reduction look-ahead set in the prediction of items creates fewer items, and is thus more efficient.

#### 7.2.4.3 Discussion

As with prediction look-ahead, the gain of reduction look-ahead in our example is meager, but that is mainly due to the unnatural simplicity of our example. The effectiveness of look-aheads in Earley parsers in the general case is not easily determined.

Bouckaert, Pirotte and Snelling [17], who have analysed variants of the Earley parsers for two different look-ahead regimes, show that prediction look-ahead reduces the number of items by 20 to 50% or even more on "practical" grammars.

Earley recommends the reduction look-ahead, but does not take into account the effort required to compute and maintain the look-ahead sets. Bouckaert, Pirotte and Snelling definitely condemn the reduction look-ahead, on the grounds that it may easily double the number of items to be carried around, but they count, for example, $\mathbf{E} \rightarrow \bullet \mathbf{F}[+-] @ 1$ as two items. All in all, since the gain from reduction look-ahead cannot be large and its implementation cost and overhead are probably considerable, it is likely to be counterproductive in practice.
The well-tuned Earley/CYK parser by Graham, Harrison and Ruzzo [23] features no look-ahead at all, claiming that more speed can be gained by efficient data structures and that carrying around look-ahead information would interfere with doing so.
McLean and Horspool [35] describe an optimized Earley parser, grouping the Earley items into subsets corresponding to LR states.

### 7.2.5 Left and Right Recursion

It is interesting to see how the Earley parser reacts to left-recursive and rightrecursive grammars. As examples we will use the simple grammars $\mathbf{S} \rightarrow \mathbf{S a} \mid \varepsilon$ (leftrecursive) and $\mathbf{s} \rightarrow \mathbf{a} \mathbf{S} \mid \varepsilon$ (right-recursive) on the input $\mathbf{aaaa\cdots}$ The Earley parser handles left recursion extremely well:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070156476.png)
We see that the result is dull but very efficient: each next item set is constructed with a constant number of actions, so the parser takes linear time. (Note by the way that after the single prediction in itemset1 no further predictions occur; so all items have origin position 1.)

The behavior on the right-recursive grammar is quite different:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070156609.png)
The number of completed items grows linearly because the parser recognizes n Ss in position n: $\boldsymbol{\mathrm{a}(\mathrm{a}(\mathrm{a}(\mathrm{a} \ldots \mathrm{f})))}$. This means that the parser has to perform $\mathbf{1+2+3+4+\cdots+n=n(n+1) / 2}$ actions; so the time requirements are quadratic. Although this is much better than the $O\left(n^{3}\right)$ from the general case, it is much worse than linear, and somehow it seems wasteful.
When we follow the algorithm through this example, we see that it collects and keeps a lot of information it never uses again. Let us look at position 4 where we have just shifted over $\mathbf{a}_{3}$, which caused the $\mathbf{S} \rightarrow \bullet \mathbf{a S}_{\mathbf{S}} \mathbf{3}$ in $itemset _{3}$ to be transformed into $\mathrm{S} \rightarrow \mathbf{a} \mathbf{S} @ 3$ and to be inserted in $active _{4}$. The Predictor predicts from it the items $\mathbf{\mathrm{S} \rightarrow \bullet aS@4}$ (into predicted 4 ) and $\mathrm{S} \rightarrow \bullet @ 4$ (into completed 4 ). The Completer takes the latter item, "reaches out" into itemset 4 to find items with the dot in front of an $\mathbf{\mathbf{S}, and finds \mathbf{S} \rightarrow \mathbf{a} \cdot \mathbf{S} @ 3}$, which is transformed into $\mathbf{S} \rightarrow \mathbf{a S} @ @ 3$, again a completed item. The process then repeats itself two more times, bringing in the items $\mathbf{\mathrm{s} \rightarrow \mathrm{aS} \bullet @ 2}$  and  $\mathbf{\mathrm{S} \rightarrow \mathrm{aS} \bullet @ 1}$.

The point to observe here is that, except for the final one all these completed items were temporary results, which cannot be used again by any other action of the parser, and we might as well not store them. This suggests that when a completed item pops up, we should do all the further completions that result from it, and keep only the final results. But there is a snag here: in the above example each completed item led to just one other item, but in the general case a completed item for say a non-terminal A may find more than one item with the dot in front of A, and soon we would be bringing in more and more items. So we restrict our eager completion to completed items whose processing by the Completer results in only one new item; we can then safely discard the original completed item. We keep, however, the original item if it was produced by the Predictor rather than by eager completion, since we will need these items later to construct the parse tree(s). Such a chain of eager completions can stop in one of three ways: 1 . the result is a non-completed item;
2 . proceeding further would result in more than one item;
3 . we cannot proceed further. The last situation can only occur when we have reached the initial item set.
With this space-saving optimization, our parsing looks like:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070205693.png)
and indeed the process now requires linear space. Unfortunately it still requires quadratic time: the subsequent Scanner action over $\mathbf{a}_{4}$ will produce $\mathbf{S} \rightarrow \mathbf{a} \bullet \mathbf{S} @ 4$, which produces $\mathbf{S} \rightarrow \bullet @ 5$, which will make the Completer visit item sets 4 through 1 , so the time requirements are still quadratic. The point to notice here is that in doing so the Completer repeated the work it did before on item sets 3 through 1 . Once an $\mathbf{S}$ has been completed in itemset $t_{4}$, the actions described above will be repeated, with the same result: the production of the item $\mathbf{S} \rightarrow \mathbf{a S} \bullet @ 1$, discarding all in-between items. More generally, once an A has been completed with origin position i, the eager completion that follows will always yield the same result. So we can avoid this waste of effort by recording at each position a list of so called transitive items. A transitive item B: $A \rightarrow \alpha \bullet \beta @ j$ in position i means that if a non-terminal B is recognized in position i, eager completion will yield the item $A \rightarrow \alpha \bullet \beta @ j$.
With this new item type in place, the Completer completing a B in position i will first check if there is a transitive item for B in that position, and if so, use it. Otherwise it proceeds as described above, doing completion possibly followed by eager completion. If the eager completion results in exactly one item, that item is stored as a transitive item in position i, with the label B. In short, we memoize the result of eager completion provided it is unique. This avoids all duplication of work, and the parser requires linear time on the above and similar grammars:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070206820.png)

This improvement was invented by Leo [32], and is much better than one would expect. It can be proved that the modified Earley parser runs in linear time on a very large class of grammars, including the LR( k ) grammars for any k (Section 9.4), and even the LR-regular grammars (Section 9.13.2). It should be pointed out, however, that the modified Earley parser has much more overhead than a made-to-measure LR (k) or LR-regular parser. On the other hand, it will avoid duplicate work even on grammars outside these classes, which is of course where its real usefulness lies.
In our zeal to remove items that cannot play a role in recognition, we have also eliminated some items needed for constructing the parse tree. Leo's paper [32] shows how to modify that part of the Earley parser to cope with the deficiencies. It also describes how to handle hidden right recursion.

## 7.3 Chart Parsing

Chart parsing is not an algorithm but rather a framework in which to develop and experiment with parsers. It can be seen as an abstraction of Earley and CYK parsers, and produces a wide variety of parsers similar to these. It is used extensively in natural language processing, where its flexibility and easy implementability in Prolog are appreciated.
The main data structure in chart parsing is the chart, a set of Earley items in our terminology but traditionally interpreted and represented as labeled edges in a graph. This graph has nodes (vertices) between the input tokens (and before and after them); each edge ( $\operatorname{arc}$ ) runs from one vertex to another somewhere on the right of it, or to the same vertex. The nodes are numbered from 1 to n+1. Edges are labeled with dotted items; an edge running from node i to node j labeled with a dotted item $A \rightarrow \alpha \bullet \beta$ means that the segment between nodes i and j can be parsed as $\alpha$ and that we hope to be able to extend the edge with a $\beta$. We shall write such an edge as (i, $A \rightarrow \alpha \bullet \beta$, j).
Although there are different ways to treat terminal symbols in chart parsing, it is convenient to make them single productions of non-terminals. A word like "cat" in the algorithms need to handle non-terminals only. This approach also abstracts from the precise value of Noun in an early stage.
An edge labeled with an item with the dot in front of a symbol is called an active edge, and one with the dot at the end is called an inactive item; the terms passive item and completed item are also used for the latter. Figure 7.26 shows a chart with
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070207881.png)
one active edge, representing the hypothesis $\mathrm{A} \rightarrow \mathrm{BCD} \bullet \mathrm{E}$, and three inactive ones, representing the fact that B, C, and D have been found. C happens to produce $\varepsilon$. The dashed arrow to the right symbolizes the activity of the active edge, looking for an $\mathbf{E}$.

7.3.1 Inference Rules

In its most abstract form a chart parsing algorithm is specified by three sets of inference rules, where an inference rule is of the form
If the chart contains edges $E_{1}, E_{2}, \cdots$ it must also contain an edge E.
One can say that edge E is required by edges $E_{1}, E_{2}, \cdots$; the edges $E_{1}, E_{2}, \cdots$ are called the "conditions" and E the "inference".
One set of rules is for completion, another is for steering the parsing process; it can specify top-down, bottom-up, or left-corner parsing, or yet another parsing regime. A third set of inference rules is for initializing the chart. The complete parsing is then defined as the transitive closure of the rules over the chart; for transitive closure see Section 3.9. Since the rules in these three sets can easily be changed almost independently, this setup allows great flexibility.
7.3.2 A Transitive Closure Algorithm
The inference mechanism and the transitive closure algorithm are easy to program, but a naive implementation is prone to looping on problematic grammars (those with loops, left recursion, infinite ambiguity, etc.) We will therefore present a robust im- plementation of the transitive closure algorithm for chart parsing. In addition to the chart it uses a list of newly discovered edges, the agenda; the order in which the edges are kept and retrieved is to be specified later. The basic idea is that at all times the following invariant holds:

If some edges in the chart and some inference rule require the existence of an edge E, then E is present in the chart and/or the agenda.

This immediately implies that when the agenda is empty, all edges required by edges in the chart are in the chart, and the transitive closure is complete. Also we will have to initialize the chart and agenda so that the invariant already holds. The agenda mechanism is summarized pictorially in Figure 7.27, where the normal arrows indicate information flow and the fat ones represent actions that move edges.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070210046.png)

The transitive closure algorithm is very simple:

```
until the agenda is empty do:
 extract an edge E from the agenda;
 if E is already in the chart: discard it;
 otherwise:
  apply all inference rules to E and possibly one or more 
   edges from the chart, and put the resulting edges, 
   if any, in the agenda;
  put E in the chart;
```

Several things should be noted here. First, the algorithm does not specify the order in which edges are obtained from the agenda: the agenda can work as a stack, a first- in-first-out queue, a priority queue, etc. Second, if $\mathbf{E}$ is already in the chart, it can indeed be discarded: all inferences from it and the chart have already been made. Third, the algorithm does not specify the order in which the inference rules are to be applied; the order is immaterial, since the results go into the agenda and do not affect the chart or each other. Fourth, the edge E is put in the chart only after all inferences have been drawn from it and put in the agenda, to avoid violating the invariant given above. Fifth, no edge will be placed more than once in the chart; edges may occur multiple times in the agenda, though.

The last, and probably most important thing to note is that the algorithm will always terminate. We can see this as follows. Each cycle of the transitive closure algorithm can do one of two things. It can either remove one edge from the agenda or add a new edge to the chart and possibly add edges to the agenda. Now there are only a finite number of edges possible, so the **otherwise** branch can be taken only a finite number of times. That means that only a finite number of edges can be added to the agenda, and they will eventually all be cleared out by the first action in the loop body. For an example see Section 7.3.6.

### 7.3.3 Completion

The set of inference rules for completion is the same in almost all chart parsing algorithms, and usually contains only one rule, called the “Fundamental Rule of Chart Parsing”; it says:

If there is an active edge (i, $A \rightarrow \alpha \bullet B \beta$, j) and an inactive edge (j, $B \rightarrow \gamma \bullet$, k), there must be an edge (i, $A \rightarrow \alpha B \bullet \beta$, k).

Like the Completer in the Earley parser this rule shifts the dot over the B in the item $A \rightarrow \alpha \bullet B \beta$ when we have a completed B.

### 7.3.4 Bottom-Up (Actually Left-Corner)

We are now in a position to specify a complete chart parsing algorithm. The simplest is probably the algorithm that is usually called "bottom-up chart parsing" but which is actually left-corner. (Pure bottom-up chart parsing is possible but unusual; see Problem 7.6.) It uses only one inference rule:

If the new edge E has the form (i, $A \rightarrow \alpha \bullet$, j) (is inactive), add an edge (i, $P \rightarrow A \bullet \beta$, j) for each rule $P \rightarrow A \beta$ in the grammar.

In other words, upon discovering an A try all rules that have an A as their left corner. For initialization, we leave the chart empty and for all input tokens $t_{k}$ put $\left(k, T_{k} \rightarrow\right. \left.t_{k} \bullet, k+1\right)$ in the agenda. If the grammar has $\varepsilon-rules$, we put edges (k, $P \rightarrow \bullet$, k) for each rule $P \rightarrow \varepsilon$ in the agenda, for all $1 \leq k \leq n+1.$ The parser is now ready to run.
7.3.5 The Agenda
The moment we try to run the parser we find that it is underspecified. Which edge should we extract from the agenda first? The simple answer is that it does not matter;

### 7.3.5 The Agenda

The moment we try to run the parser we find that it is underspecified. Which edge should we extract from the agenda first? The simple answer is that it does not matter;
we can extract them in any order and the parser will work. But doing so will recognize the input in a very haphazard way, and if we want a more controlled parsing, we need a more controlled agenda regime.
Suppose that we use the left-corner inference rule; that there are no $\varepsilon-rules$ in the grammar; that the agenda acts as a stack; and that the input edges are stacked in reverse order, so the edge for the leftmost token, $\mathbf{\left(1, T_{1} \rightarrow t_{1} \bullet, 2\right)}$  ends up on top. First the edge for $T_{1}$ is retrieved. There are no edges in the chart yet, so the Fundamental Rule does nothing; but the new edge is an inactive edge, so the left-corner rule finds grammar rules that have $T_{1}$ for its left corner. In a grammar without $\varepsilon-rules$ there must be at least one such rule; otherwise there would be no way to produce a sentence starting with $T_{1}$. An edge $\left(1, A_{i} \rightarrow T_{1} \bullet \alpha_{i}, 2\right)$ is made for each such rule and pushed on the agenda stack. The edge for T_{1} is put in the chart. See Figure 7.28.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070216874.png)
In the next cycle the topmost edge of the agenda is retrieved; we will assume it is $\left(1, A \rightarrow T_{1} \bullet \alpha, 2\right)$. Since it is not completed, it is put in the chart, and no new edges are generated. Now the edge for $t_{2}$ is on top and is retrieved. It can either activate the Fundamental Rule and combine with the edge for A into a new edge for, say, B, which now spans 2 tokens, or create left-corner edges for itself by the left-corner inference rule, or both. If we assume the edge for B is completed, the left-corner rule will create at least one new edge for it, perhaps for C, starting at position 1 . So slowly a left spine is being constructed, as in a left-corner parser.
If there are $\varepsilon-rules$, then, at initialization, edges for them must be put in the agenda stack before the edges for each token, and at the end.
There are many possibilities for the agenda regime. We have seen the stack regime in operation above; it leads to depth-first search, and if the edges for the input tokens are stacked in order, a left-to-right parser results. A queue regime is also possible, resulting in breadth-first search: first all inferences from all tokens are drawn; next all inferences from these inferences are drawn, etc. We already see that this allows left-corner parsing to be used with depth-first search (the usual order) and breadth-first search (very unusual).
Another possibility is to assign a priority to each edge and run the agenda as a priority queue. Not all words in a sentence are equally important, and sometimes it is wise to start our parsing with the most significant word, which in many languages is the finite - i.e. the conjugated - verb. As a result, the first edge created may already determine the structure of the sentence. Additional inference rules are then required to steer the parsing process so as to obtain edges for the components of this first edge. This is highly useful in head-corner parsing, for which see Section 10.3. A head-corner parser using this technique is described by Proudian and Pollard [198].

### 7.3.6 Top-Down

A top-down parser tries to predict the next production rule by inspecting the prediction stack. This stack consists of the remainders of right-hand sides that have already been recognized. In our chart parser, if the edge retrieved from the agenda is of the form (i, $A \rightarrow \alpha \bullet B \beta$, j) (it is an active edge), the prediction it holds is $B \beta$. This leads to the traditional top-down inference rule:

If the new edge E has the form (i, $A \rightarrow \alpha \bullet B \beta$, j) (is active), add an edge (j, $B \rightarrow \bullet \gamma$, j) for each rule $B \rightarrow \gamma$ in the grammar.

In a practical parser this rule will have additional conditions based on look-ahead, but the parser will also work without them. The parser can be initialized by putting an edge for the start symbol running from 1 to 1 and edges for the input tokens on the agenda stack, in that order.
We know that we have to be careful about left recursion in top-down parsers (Section 6.3.2), but it is easy to see that our transitive closure algorithm avoids the problem, as already claimed in Section 7.3.2. Suppose we have a leftrecursive non-terminal with the rules $\mathrm{L} \rightarrow \mathrm{La}, \mathrm{L} \rightarrow \mathrm{Lb}$, and $\mathrm{L} \rightarrow \mathrm{c}$, and an edge (i, $\mathrm{P} \rightarrow \ldots \bullet \mathrm{L} \ldots$, j) comes up. This causes edges for all three rules for $\mathrm{L}$ to be pushed in some order, say (j, $\mathbf{L} \rightarrow \bullet \mathbf{L a}$, j),(j, $\mathbf{L} \rightarrow \bullet \bullet c$, j),(j, $\mathbf{L} \rightarrow \bullet \mathbf{L b}$, j); see Figure 7.29(a). In this figure we have shown only the items, to fit it on the page; the start

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070217718.png)

and stop positions are j and j in all cases. Next the edge (j, $\mathrm{~L} \rightarrow \bullet \mathrm{Lb}$, j) is popped, and causes all three rules for $\mathrm{L}$ to be pushed again; that done, it is put in the chart (b). Now a (j, $\boldsymbol{\mathrm{~L} \rightarrow \bullet \mathrm{Lb}}$, j) gets popped again, but is discarded since it is already in the chart (c). Next an edge (j, $\boldsymbol{\mathrm{~L} \rightarrow \bullet c}$, j) is popped, and handed over to the inference mechanism. It may introduce other edges but these are not concerned with the nonterminal $\mathbf{L}$; eventually the edge itself is put in the chart (d). The edge (j, $\boldsymbol{\mathbf{L} \rightarrow \bullet \mathbf{L a}}$, j) is popped, and is replaced by another set of three edges for $\mathrm{L}$; the edge itself again goes to the chart(e). Finally the remaining items are popped one by one, and since all of them are already in the chart, they are all discarded, so the displayed part of the agenda disappears (f).

### 7.3.7 Conclusion

Chart parsing is a very versatile framework for creating and tailoring general contextfree parsers, and the present description can only hint at the possibilities. For example, a chart parser will happily accept more than one interpretation of the same token in the input, which is very convenient for natural language parsing. An ambiguous word like "saw" at position k can be entered as two edges, ( $k,\boldsymbol{Noun \rightarrow ' saw' \bullet}, k+ 1$) and ( k,$\boldsymbol{VerbPastTense \rightarrow ' saw' \bullet}$, $k+1$). Chart parsing shares with Earley parsing its $O\left(n^{3}\right)$ time requirements.
Chart parsing was invented by Kay in the early 1970s [16]. Many sophisticated inference rules have been published, for example by Kilbury [24] and Kay [25]. For a comparison of these see Wirén [27]. The agenda mechanism was introduced by Kay [25]. The literature references in (Web)Section 18.1.2 contain many other examples. There are several Prolog implementations of chart parsing on the Internet.
Probably the most extensive application of transitive closure and inference rules to parsing is by Sikkel [158].
Figure 7.30 shows the recognition table of Figure 4.16 in chart format.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070218754.png)

## 7.4 Conclusion

General bottom-up parsing methods are powerful because they start from the raw ma- terial, the input string, and recognize everything that can be recognized; they leave no stone unturned. Just because of this thoroughness they easily fall prey to exponential explosion. To remedy this, a top-down component is added, resulting in the Earley algorithm. The tabular implementation of this algorithm, chart parsing, allows fine control over the top-down, bottom-up, and prediction content of the parser.

# Problems

**Problem 7.1**: Redesign the depth-first and breadth-first parsing algorithms of Sections 7.1.1 and 7.1.2 so they yield parse-forest grammars (Section 3.7.4) rather than sequences of parse trees.

**Problem 7.2**: The naive parsing algorithms of Sections 7.1.1 and 7.1.2 do not work for grammars with ε-rules, but intuitively this defect is easily remedied: just recognize ε at all positions and backtrack if the recognition does not lead anywhere. Explain why this plan does not work.

**Problem 7.3**: Suppose all terminal symbols in a given grammar are different. Can that property be exploited in parser design?

**Problem 7.4**: The Earley sets from position 1 to a position k contain all predic- tion stacks possible at k. We can see this as follows. An item $\boldsymbol{A → α•β@m}$ contains the beginning of a prediction, **β**. The next segment of the prediction can be found as the δ in the item $\boldsymbol{X → γ•Aδ}$ in the item set at position m. Such an item must exist, but there may be more than one, in which case the prediction forks, and we get a predic- tion tree. a) Construct this prediction tree for position 3 in the parsing in Figure 7.11. b) Describe the complete algorithm for constructing the prediction tree at a position k.

**Problem 7.5**: The transitive items in position i in the improved Earley parser from Section 7.2.5 can be computed right away when itemseti is constructed, or the computation can be postponed until a Completer comes along for the first time. Comment on the difference.

**Problem 7.6**: Pure bottom-up chart parsing uses no additional rules besides the Fundamental Rule. Find an initialization that will make this parser work.

**Problem 7.7**: Show that the left-corner chart parser in this chapter will not loop on grammars with loops in them.
