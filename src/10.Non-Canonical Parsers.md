# Chapter 10 Non-Canonical Parsers

Top-down parsers make their predictions in pre-order, in which the parent nodes are identified _before_ any of their children, and which imitates leftmost derivations (see Section 6.1); bottom-up parsers perform their reductions in post-order, in which the parent nodes are identified _after_ all of their children have been identified, and which imitates rightmost derivation (see the introduction in Chapter 7). These two orders of producing and visiting trees are called "canonical", and so are the parsing techniques that follow them.

Non-canonical parsing methods take liberties with these traditional orders, and sometimes postpone the decisions that would be required to create parse trees in pure pre- or post-order. This allows them to use a larger set of grammars, but on the other hand these methods create fragments of parse trees, which have to be combined at later moments.

Like their canonical counterparts, non-canonical methods can be classified as top-down (Section 10.1) and bottom-up (Section 10.2) methods, based on whether they primarily use pre-order or post-order. There are deterministic and general non-canonical methods. The deterministic methods allow parsing in linear-time; as with LL and LR methods, they can be generalized by applying a limited breadth-first search. Altogether the non-canonical methods form a large and diverse field that has by no means been completely explored yet.

Figure 10.1 shows the relation between non-canonical parsing and the corresponding non-canonical production process, as Figure 3.9 did for canonical parsing. In this case just the node for $\mathbf{q}$ has been identified. Again the dotted line represents the sentential form.

The most important property of deterministic non-canonical parsing is that it allows a larger class of grammars to be used without modification while retaining linear time requirements. Since it has simultaneously aspects of top-down and bottom-up parsing it can also provide further insight in parsing; see, for example, Demers [103], who describes a parsing technique on a gliding scale between LL(1) and SLR(1).

On the down side there is an increased complexity and difficulty, both for the implementer and the user. Postponing decisions does not come free, so non-canonical parsing algorithms are more complex and require more ingenuity than their canonicalcounterparts; this makes them less attractive for implementers. And where LL and LR parser generators can leave the construction of the parse tree to the user, the non-canonical methods identify parse tree nodes in an often unintuitive order, making it next to impossible for users to construct a parse tree on their own. Consequently we find non-canonical methods primarily in systems that offer the user a finished parse tree.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112250139.png)

Creating a node in a parse tree is actually a two-step process: first the node is created and then it is identified, i.e., labeled with a non-terminal. In almost all parsers these two steps coincide, but that does not have to be. Some non-canonical parsers create the nodes, together with their identifications, in non-canonical order; others create the nodes in canonical order, but identify them later, in some different order. Examples of the latter are $PLL(1)$ and Partitioned LR. There should probably be different names for these approaches but there are not, to our knowledge. Note that operator-precedence creates the nodes in canonical order but identifies them later, or not at all, and therefore borders on the non-canonical.

## 10.1 Top-Down Non-Canonical Parsing

Top-down non-canonical parsers postpone parse tree construction decisions, but not as far as canonical bottom-up parsers. As a result they are less powerful but often allow earlier decisions than LR parsers. This is important when early semantics actions are desired.

We will discuss here three deterministic top-down non-canonical methods: left-corner parsing, cancellation parsing, and Partitioned LL. The first two allow top-down parsing with left-recursive grammars, while the third allows grammars for languages that would require unbounded look-ahead in traditional top-down parsing.

### 10.1.1 Left-Corner Parsing

As we have seen in Section 6.3.2, a standard top-down parser cannot deal with left-recursive grammars, since it has no way of knowing how many rounds of left recursion it should predict. Suppose we postpone that decision and concentrate on predicting a suitable subtree; once we have found that, we may be able to decide whether another round of left recursion is needed. This leads to a technique called "left-corner parsing".

#### 10.1.1.1 Left Spines

Consider the grammar for simple arithmetic expressions in Figure 10.2 copied from Figure 9.2, and the input string **n+nxn#**, where # is the usual end marker.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112251700.png)

We start with the prediction **S#** and the first token of the input, **n**. A traditional LL parser would want to produce the complete left spine of the parse tree before the **n** is matched. It would use something like the following reasoning: the pair (**S**,**n**) predicts an **E**; the pair (**E**,**n**) either predicts **E+T**, which brings us back to **E**, or a **T**; the pair (**T**,**n**) predicts **TxF**, which brings us back to **T** or an **F**; and the **F** finally predicts the **n**. Only then can matching take place. But the LL parser cannot do so deterministically, since it cannot decide how many **E+Ts** and **TxF**s it should predict, as shown in Figure 10.3(_a_).

A _left-corner parser_ postpones these decisions, finds that the left-corner prediction **F$\rightarrow$n** is a decision that can be made with certainty, and is satisfied with that. The predicted **F** is then parsed recursively (see below). By the same reasoning, the resulting **F** can only derive from a prediction **T$\rightarrow$F**, which allows the **F** to be matched. This allows us to look behind the **T**, where we see a +. This + tells us that the parse tree node starting with the **T** cannot have been **TxF**, but must have derived from **E$\rightarrow$T**. So we predict **E$\rightarrow$T**, match the **T** and look behind the **E**, where the + tells us to predict **E$\rightarrow$E+T**. The predicted + and the + from the input are now matched, and the left-corner parser starts parsing **n$\times$n** with **T** as the prediction.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112252012.png)

Several points must be made here. The first is that in some weird way we have been making predictions from hindsight; we will return to this point further on. The second is that the above process fixed the left spine of the parse tree to some extent, but not completely; see Figure 10.3(_b_). The reason is of course that the parser cannot know yet if more nodes **E+T** will be required; they would be if the input were, for example, **n+nxn+n+n**. The third is that the **F** (and the **T** and the **E**) must be parsed recursively, since they may match large portions of the input string. If the input had been (**n+nxn+n**) +n**, the whole sub-expression between parentheses would have to be absorbed by the parsing of the **F**, to allow us to look behind it. And the fourth, and probably most important point is that the above sketch is not yet a usable algorithm and needs more detail.

#### 10.1.1.2 The Production Chain Automaton

Figure 10.3(_a_) provides more information than we have given it credit for: it shows clearly that the left spine is described by a regular expression: $\mathtt{SE(E+T)}^{*}\mathtt{T(T\times F)}^{*}\mathtt{Fn}$. This regular expression corresponds to a finite-state automaton, which is depicted in Figure 10.4(_a_), and which also shows the second alternative for $\mathtt{F}$, ($\mathtt{E}$). The nodes are the predicted non-terminals that participate in the left-corner process, and the arrows are labeled with the rules involved in the predictions. The automaton describes all leftmost production chains from $\mathtt{S}$; such an automaton is called a _production chain automaton_ for $\mathtt{S}$ (Lomet [102]).

In the sketch of the algorithm in the previous section we were interested in the token that became visible behind the first non-terminal in a prediction; they form the look-ahead sets of the predictions and are also shown in the picture of the automaton. The non-terminal $\mathbf{S}$ starts of with a look-ahead set of $[\#]$, and the production $\mathbf{S} \rightarrow \mathbf{E}$ passes it on to the $\mathbf{E}$. So along the $S-to- \mathbf{E}$ arc the look-ahead set is $[\#]$. The production rule $\mathbf{E} \rightarrow \mathbf{E}+\mathbf{T}$ adds a + to this set, so when the production rule $\mathbf{E} \rightarrow \mathbf{T}$ is taken, the look-ahead set has grown to $[\#+]$. In the same way $T \rightarrow T \times F$ adds a $\times$, so when the production rule $T \rightarrow F$ is taken, the look-ahead set is $[\#+x]$. The final predictions
The automaton as depicted in Figure 10.4($a$) generates the predictions from **S** to the first terminal symbol non-deterministically, since there is nothing to guide the automaton into determinacy. But in our sketch of the algorithm we used the look-ahead tokens discovered after the first non-terminal of a rule to find our way _backwards_ through the automaton. This means that if we reverse the arrows in the production automaton, we can let ourselves be guided by the look-ahead tokens to find the next (actually the previous!) prediction. When the reversed automaton of each non-terminal in a grammar $G$ is deterministic, we have a deterministic parser, and $G$ is of type _LC(1)_. $\mathbf{F} \rightarrow \mathrm{n}$ and $\mathbf{F} \rightarrow(\mathbf{E})$ do not have look-ahead sets, since these rules start with the terminal symbols and are identified by these.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112257612.png)

The automaton as depicted in Figure 10.4(a) generates the predictions from $\mathbf{S}$ to the first terminal symbol non-deterministically, since there is nothing to guide the automaton into determinacy. But in our sketch of the algorithm we used the look-ahead tokens discovered after the first non-terminal of a rule to find our way backwards through the automaton. This means that if we reverse the arrows in the production automaton, we can let ourselves be guided by the look-ahead tokens to find the next (actually the previous!) prediction. When the reversed automaton of each non-terminal in a grammar G is deterministic, we have a deterministic parser, and G is of type LC(1)

We see in Figure 10.4(b) that the reverse production chain automaton for our grammar is indeed deterministic. The working of the parser on $\mathbf{n}+\mathbf{n} \times \mathbf{n}$ is now more easily followed. Starting at the bottom, the initial $\mathbf{n}$ of the input gives us the prediction $\mathbf{F} \rightarrow \mathrm{n}$ and absorbing the $\mathbf{F}$ reveals the + after it. This + leads us to the prediction $T \rightarrow F$, where the path splits in a path labeled $[\#+]$ and one labeled $[x]$. Since the input symbol is a + , we follow the first path and predict a parse tree node $\mathbf{E} \rightarrow \mathbf{T}$. At the automaton node $\mathbf{E}$ the path splits again, into one labeled $[\#]$ and one labeled $[+]$. Since the input symbol is still the + , we take the second path and predict a parse tree node $\mathbf{E} \rightarrow \mathbf{E}+\mathbf{T}$. Now the + can be matched, and the parser recursively recognizes the $\mathrm{n} \times \mathrm{n}$ with prediction $\mathrm{T}$, as described below. When that is done, we are back at the $\mathbf{E}$ node in the FSA, but now the input symbol is $\#$. So we predict $\mathbf{S} \rightarrow \mathbf{E}$, create a parse tree node $\mathbf{S}$, match the $\mathbf{E}$, and the parsing is finished.

If the input had been the erroneous string $\mathbf{n}$ ), the automaton would have got stuck at the F since the input symbol ) is not in the look-ahead set of the prediction $\mathbf{T} \rightarrow \mathbf{F}$, and the parser would have reported an error.
If the input had been the erroneous string **n**)**, the automaton would have got stuck at the **F** since the input symbol **)** is not in the look-ahead set of the prediction **T**, and the parser would have reported an error.

To parse the remaining $\mathrm{n} \times \mathrm{n}$ recursively with prediction T, a production chain automaton for $\mathbf{T}$ is created, using the same technique as above. Since it starts with a look-ahead set of $[\#+]$ just as the $\mathbf{T}$ in Figure 10.4(b), this automaton is identical to the lower part of that figure, so no new automaton is needed. But that is not always the case, as the following example shows.

Suppose the input is (n). Then we enter the automaton for $\mathbf{S}$ at the ( at the bottom, and predict $F \rightarrow(E)$. The open parenthesis is matched, and we now want to parse $\mathbf{n} ) \#$ with the prediction $\mathbf{E}$. But this $\mathbf{E}$ is followed by a ), unlike the one in Figure 10.4(b), which is followed by a $\#$. There are two ways to deal with this.

The first is to indeed create a new automaton for $\mathbf{E}[)]$; this automaton is similar to the automaton in Figure 10.4(b) from the $\mathbf{E}$ node down, except that the token $\#$ in the look-ahead sets is replaced by ). More in general, we create a new automaton for each combination of non-terminal and look-ahead set. This approach produces a full-LC(1) parser, comparable to the full-LL(1) parser explained in Section 8.2.3.

The second is to add the closing parenthesis to the look-ahead set of the $\mathbf{E}$ node in Figure 10.4(b) and update the automaton. If we do this for all non-terminals in the grammar, there will be only one node in the production chain automaton for each non-terminal and the look-ahead sets become equal to the FOLLOW sets. This course of action leads to a strong- L C(1) parser, comparable to a strong-LL(1) parser. Like the latter, it has smaller tables and weaker error detection properties than its fullLC(1) counterpart. For example, the erroneous input n ) will fool it into predicting $\mathbf{F} \rightarrow \mathrm{n}, \mathrm{T} \rightarrow \mathrm{F}$ and $\mathbf{E} \rightarrow \mathrm{T}$, and only then will the automaton get stuck on an input symbol ) and a look-ahead set $[\#]$.
#### 10.1.1.3 Combining the Chain Automaton and the Parse Stack

The actions of a left-corner parser can be implemented conveniently as a top-down parser, using a stack and a parse table. The table is indexed by the first token of the prediction if it is a non-terminal, and the first token of the rest of the input, the lookahead. We start with the prediction S\#; see Figure 10.5. We have seen above that the non-terminal \mathbf{S} and the look-ahead \mathbf{n} lead to the prediction \mathbf{F}_{1} \rightarrow \mathbf{n}, so we stack the \mathbf{n} and add the \mathbf{F}_{1} to the analysis (we have appended the subscript { }_{1} to identify the first rule for F). But that cannot be all: this way we lose the part of the prediction between the \mathbf{F} and the \mathbf{S} in the automaton of Figure 10.4(b). So we also stack a new-made symbol \mathbf{S} \backslash \mathbf{F}, to serve as a source for predictions for the left spine from \mathbf{S} to \mathbf{F}. As a stack symbol, \mathbf{S} \backslash \mathbf{F} matches the rest of \mathbf{S} after we have matched an \mathbf{F}; in other words, any string produced by \mathbf{F} \mathbf{S} \backslash \mathbf{F} is a terminal production of \mathbf{S}.

Next we match the \mathbf{n}, adding it to the analysis. Now we need a prediction for \mathbf{S} \backslash \mathbf{F}, with look-ahead + , and the automaton tells us to predict \mathbf{T}_{2} \rightarrow \mathbf{F}. We add the \mathbf{T}_{2} to the analysis, but we have parsed the F already, so we do not stack it; it is in fact the left operand of the \mathbf{T}_{2} in the analysis. We do, however, stack a symbol \mathbf{S} \backslash \mathbf{T}, to cover the rest of the left spine. The symbol \mathbf{S} \backslash \mathbf{T} designates the position \mathbf{T} in the automaton for \mathbf{S}, the look-ahead is still + , so the automaton wants us to predict \mathbf{E}_{2} \rightarrow \mathbf{T}. Like before, we have already parsed the entire right-hand side of the prediction, so we only stack the symbol \mathbf{S} \backslash \mathbf{E}, which brings us to the fifth frame in Figure 10.5 . Now

Now we have again a "normal" non-terminal on top of the prediction stack: $\mathtt{T}$. In principle we would now need a new automaton for $\mathtt{T}$, but since $\mathtt{T}$ occurs in the automaton for $\mathtt{S}$, that occurrence will serve. The recognition of the $\mathtt{n}$ as a $\mathtt{F}_{1}$ and the $\mathtt{F}$ as a $\mathtt{T}_{2}$ mimic the sequence of events above, but now something happens that requires our attention: after having recognized a $\mathtt{T}$ while looking for a $\mathtt{T}$, we cannot just pack up and be satisfied with it, but we have to acknowledge the possibility that we have to stay in the automaton for $\mathtt{T}$ for another round, that there may be more of the automaton tells us to predict \mathbf{E}_{1} \rightarrow \mathbf{E}+\mathbf{T}, of which we have already recognized the left corner \mathbf{E}. So we stack the +\mathbf{T}, and of course our reminder, again \mathbf{S} \backslash \mathbf{E}, after which the + is matched.

Now we have again a "normal" non-terminal on top of the prediction stack: T. In principle we would now need a new automaton for \mathbf{T}, but since \mathbf{T} occurs in the automaton for \mathbf{S}, that occurrence will serve. The recognition of the \mathbf{n} as a \mathbf{F}_{1} and the \mathbf{F} as a \mathbf{T}_{2} mimic the sequence of events above, but now something happens that requires our attention: after having recognized a \mathbf{T} while looking for a \mathbf{T}, we cannot just pack up and be satisfied with it, but we have to acknowledge the possibility that we have to stay in the automaton for \mathbf{T} for another round, that there may be more of \mathbf{T} after this \mathbf{T}. We do this by stacking the symbol for "the rest of \mathbf{T} after \mathbf{T} ", \mathbf{T} \backslash \mathbf{T}. This symbol allows two predictions: leave the automaton, and stay in it.

We see immediately how necessary it was to stack the \mathbf{T} \backslash \mathbf{T} symbol, since we d o have to stay in the automaton for \mathbf{T}, to parse the subsequent \times \mathbf{n} (frames 10 through 15). Only in frame 15 can we decide that we have seen the whole T we were looking for, due to the look-ahead \#, and leave the \mathbf{T} automaton.

Something similar happens in frame 17, where we stop at the top of the automaton for \mathbf{S} on the symbol \mathbf{S} \backslash \mathbf{S}, considering whether to continue looking for more \mathbf{S} or to stop here. But since there is no way to stay in the automaton in the state \mathbf{S} \backslash \mathbf{S}, the transition to frame 18 is automatic, and the parsing is finished.

We can collect our decisions in a parse table. The complete table is given in Figure 10.6; since we did not construct separate automata for \mathbf{E}, \mathbf{T}, and \mathbf{F} for different look-aheads, it represents a strong-LC(1) parser for the grammar of Figure 10.2.

![Fig.10.6.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112303732.svg)

#### 10.1.1.4 Obtaining A Parse Tree

The last frame of Figure 10.5 shows the left-corner analysis of $n+n \times n$ to be $\mathbf{F}_{1} \mathbf{n} \mathbf{T}_{2} \mathbf{E}_{2} \mathbf{E}_{1}+\mathbf{F}_{1} \mathbf{n T _ { 2 }} \mathbf{T}_{1} \times \mathbf{F}_{1} \mathbf{n S _ { 1 }}$ but that is less helpful than one would hope. The reason is of course that non-canonical methods identify the nodes in the parse tree in an unusual order, in this case in infix order. As explained in Section 3.1.3, infix order requires parentheses to be unambiguous, but the presented analysis does not include them (see Problem 10.3). So the analysis has to be analysed further to yield a parse tree.

To properly parenthesize the analysis, we need to identify the left and right children of each node.

- A grammar rule whose right-hand side starts with a terminal or is empty is recognized before all of its children. For example, a node for the second rule for F, $\mathbf{F} \rightarrow(\mathbf{E})$, has zero left children and 3 right children; we will represent the node as $F\_{2}[0+3]$.
- A grammar rule whose right-hand side starts with a non-terminal is recognized after its first child and before the rest of its children. For example, a node for the first rule for $T$, $T \rightarrow T \times F$, has one left child and 2 right children; we will represent the node as $\mathbf{T}_{1}[1+2]$.

 Using these rules we can parenthesize the left-corner analysis of $n+n \times n$ as shown in Figure 10.7.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112306534.png)
Now having to “analyse the analysis” may raise eyebrows, but actually the prob- lem is caused by the linear format of the analysis shown in diagrams like Figure 10.5. Left-corner parser generators of course never linearize the parse tree in this way but rather construct it on the fly, using built-in code that gathers the children of nodes as itemized above, so the problem does not materialize in practice.

#### 10.1.1.5 From LC(1) to LL(1)

The recognizing part of the left-corner parser presented above is indistinguishable from that of an LL(1) parser. Its stack, parse table, and mode of operation are iden- tical to those of an LL(1) parser; only the construction of the parse table from the grammar and the construction of the analysis differ. This suggests that there is an LL(1) grammar that gives the same language as the LC(1) grammar, and in fact there is.

An LL(1) grammar for the language produced by an LC(1) grammar can always be obtained by the following technique, for which we need the notion of "left-spine child". A non-terminal B is a left-spine child of A (written $B \angle A$ ) if there is a grammar rule $A \rightarrow B \alpha$ for some possibly empty $\alpha$ or if there is a rule $A \rightarrow C \beta$ and $B \angle C$. So B is a left-spine child of A if B can occur on a left spine starting at A. We shall also need to define that A is a left-spine child of itself: $A \angle A$. More in particular, in our example we have $\mathbf{S} \angle \mathbf{S} \angle \mathbf{E} \angle \mathbf{E} \angle \mathbf{T} \angle \mathbf{T} \angle \mathbf{F} \angle \mathbf{F}$. The $\angle$ is also called the left-corner relation.

To construct the LL(1) grammar, we start from the root non-terminal of the LC(1) grammar, $\mathbf{S}$ in our example, and we are going to need one or more rules for it in the LL(1) grammar. To find rules for a non-terminal A in the LL(1) grammar, we look for rules in the LC(1) grammar with a left-hand side B that is a left-spine child of A and whose right-hand side starts with a terminal or is empty. Such rules are of the form $B \rightarrow \beta$, where $\beta$ starts with a terminal or is $\varepsilon$. This $\beta$ can be a left-corner for A, after which we still have to parse the part of A after B, that is, $A \backslash B$. So for each such $B \rightarrow \beta$ we add a rule $A \rightarrow \beta A \backslash B$ to the $LL(1)$ grammar. For our start symbol $\mathbf{s}$ there are two such rules, $\mathbf{F} \rightarrow \mathrm{n}$ and $\mathbf{F} \rightarrow(\mathbf{E})$, which give us two rules for the LL(1) grammar: $S \rightarrow n S \backslash F$ and $S \rightarrow(E) S \backslash F$.

So now we need one or more rules for $\mathbf{S} \backslash \mathbf{F}$ in the LL(1) grammar. We obtain these in a similar but slightly different way. To find rules for a non-terminal $A \backslash B$ in the LL(1) grammar, we look for rules in the LC(1) grammar with a left-hand side C that is a left-spine child of A and whose right-hand side starts with the non-terminal B. Such rules are of the form $C \rightarrow B \gamma$, where $\gamma$ may be empty. Since we have already parsed the B prefix of A, and C is a left-corner child of A, we can try to continue by parsing $\gamma$, and if we succeed, we will have parsed a prefix of A produced by C, which leaves $A \backslash C$ to be parsed. So for each such $C \rightarrow B \gamma$ we add a rule $A \backslash B \rightarrow \gamma A \backslash C$ to the LL(1) grammar. For $\mathbf{S} \backslash \mathbf{F}$ there is only one such rule, $\mathbf{T} \rightarrow \mathbf{F}$, which causes us to add $\mathbf{S} \backslash \mathbf{F} \rightarrow \mathbf{S} \backslash \mathbf{T}$ to the LL(1) grammar.

For $\mathbf{S} \backslash \mathbf{T}$, however, there are two such rules in the LC(1) grammar, $\mathbf{T} \rightarrow \mathbf{T} \times \mathbf{F}$ and $\mathbf{E} \rightarrow \mathbf{T}$, resulting in two rules in the LL(1) grammar, $\mathbf{S} \backslash \mathbf{T} \rightarrow \times \mathbf{F} \mathbf{S} \backslash \mathbf{T}$ and $\mathbf{S} \backslash \mathbf{T} \rightarrow \mathbf{S} \backslash \mathbf{E}$. A similar step lets us create two rules for $\mathbf{S} \backslash \mathbf{E}: \mathbf{S} \backslash \mathbf{E} \rightarrow+\mathbf{F} \quad \mathbf{S} \backslash \mathbf{E}$ and $\mathbf{S} \backslash \mathbf{E} \rightarrow \mathbf{S} \backslash \mathbf{S}$. The latter, deriving from $\mathbf{S}_{\mathbf{S}} \rightarrow \mathbf{E}$, requires a rule for $\mathbf{S} \backslash \mathbf{S}$. Rules of the form $A \backslash A \rightarrow \varepsilon$ can always be created for any A when required, so we add a rule $\mathbf{S} \backslash \mathbf{S} \rightarrow \varepsilon$ to the LL(1) grammar.

The above new rules have introduced the non-terminals $\mathbf{E}, \mathbf{T},$ and $\mathbf{F}$ into the LL(1) grammar, and rules for these must also be created, using the same patterns. Figure 10.8 shows the final result. We see that in addition to the trivial rule $\mathbf{E} \backslash \mathbf{E} \rightarrow \varepsilon$ there is another rule for $\mathbf{E} \backslash \mathbf{E}$, reflecting the fact that $\mathbf{E}$ is directly left-recursive. The same applies to $\mathbf{T} \backslash \mathbf{T}$.

The above transformation can be performed on any $\mathrm{CF}$ grammar. If the result is an LL(1) grammar the original grammar was LC(1). If we offer the grammar to a strong-LL(1) parser generator, we obtain a strong-LC(1) parser; using a full-LL(1) parser generator yields a full-LC(1) parser.

Deterministic left-corner parsing was first described extensively in 1970 by Rosenkrantz and Lewis, II [101]. A non-deterministic version was already used implicitly by Irons in 1961 [2]. It seems a good candidate for a model of human natural language parsing (see, for example Chester [377], Abney and Johnson [383], and Resnik [384]).

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112309814.png)



### 10.1.2 Deterministic Cancellation Parsing

The non-deterministic version of cancellation parsing described in Section 6.8 can, as usual, be made deterministic by equipping it with look-ahead information.

As with left-corner parsing, the look-ahead set of a non-terminal $A$ consists of two components: the FIRST sets of the non-left-recursive alternatives of $A$, and the set of tokens that can follow $A$. As with LL(1) and LC(1) techniques, there are two possibilities to compute the follow part of the look-ahead set: it can be computed separately for each occurrence of $A$, or it can be replaced by the FOLLOW set of $A$. But in cancellation parsing there is another influence on the look-ahead sets: the set of alternatives of $A$, since that set is not constant but depends on $A$'s cancellation set; so the FIRST sets also fluctuate. There are again two possibilities here. We can compute these "first" look-ahead sets separately for each combination of $A$ and its possible cancellation sets, or we can compute it for $A$ in general, disregarding the influence of the cancellation set.

So in total there are four combinations. Nederhof [105] calls the deterministic parsers resulting from separate computation of the look-ahead sets for each occurrence of each combination of a non-terminal with one of its possible cancellation sets _C(k)_ parsers; using FOLLOW sets but keeping the differences in cancellation sets gives _strong-C(k)_ parsers; also disregarding the cancellation sets gives the _severe-C(k)_ parsers. The fourth combination, ignoring the cancellation sets but distinguishing different occurrences of the same non-terminal, is not described in the paper because it is identical to full-LL.

Since the construction of deterministic cancellation parsers is quite complicated; since such parsers are less powerful than left-corner parsers; and since non deterministic cancellation parsers are much more useful than their deterministic ver- sions, we will not discuss their construction here further.



#### 10.1.3 Partitioned LL

LL(1) parsing requires us to choose between the alternatives of a non-terminal right at the start. _Partitioned LL(1)_ (or _PLL(1)_) tries to postpone this decision as long as possible, but requires that the decision will be taken before or at the end of the alternatives. This assures that the deterministic nature of the parser is preserved. Partitioned LL($k$) parsing was designed by Friede [196, 195].

#### 10.1.3.1 Postponing the Predictions by Using Partitions

We will demonstrate the technique using the grammar from Figure 10.9,
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112318538.png)

which produces the language $\mathbf{a}^{n}\mathbf{b}^{n}\cup\mathbf{a}^{n}\mathbf{c}^{n}$. Sample strings are $\mathbf{ab}$, $\mathbf{a}\mathbf{a}\mathbf{c}$, and $\mathbf{a}\mathbf{a}\mathbf{a}\mathbf{b}\mathbf{b}$; note that "mixed" strings like $\mathbf{a}\mathbf{a}\mathbf{a}\mathbf{b}\mathbf{c}$ are forbidden.

Trying to get a top-down parser for this grammar is quite interesting since the language it generates is the standard example of a language for which there cannot be a deterministic top-down parser. The reason for the inherent impossibility of such a parser for this language is that a top-down parser requires us to make a prediction for a $\mathbf{b}$ or a $\mathbf{c}$ in the second half of the sentence for each $\mathbf{a}$ we read. But the point where we can decide which of them to predict can be arbitrarily far away; and just predicting $\boldsymbol{[bc]}$ will not do, since that would allow "mixed" sentences like the one above.

To postpone the decision between two alternatives of a non-terminal $A$, we first look for a common prefix. Suppose the first alternative is $A\rightarrow\alpha\beta$ and the second is $A\rightarrow\alpha\gamma$. We can then parse the common prefix $\alpha$ first although we will have to predict both $\beta$ and $\gamma$; we will see below how we can implement this. The moment we reach the point where the two alternatives start to differ, we try an LL(1)-like test to find out if we can see which one applies. To do this, we compute the sets FIRST($\beta$ FOLLOW($A$)) and FIRST($\gamma$ FOLLOW($A$)). If these are disjoint, we can base our decision on them, as we did in LL(1) parsing; we can then also discard one of the predictions.

If this were all, Partitioned LL(1) would just be LL(1) with automatic left-factoring (Section 8.2.5.2). But Partitioned LL(1) goes one step further. Suppose the LL(1) test fails and it so happens that $\beta$ and $\gamma$ both start with a non-terminal, say $P$ and $Q$; so $\beta=P\beta^{\prime}$ and $\gamma=Q\gamma^{\prime}$. Partitioned LL(1) then puts $P$ and $Q$ together in a "partition", which is actually a set: $\{P,Q\}$, and tries to postpone and still achieve the decision by parsing with the partition $\{P,Q\}$ as prediction. An essential requirement for this to work is of course that parsing with a partition tells in the end which member of the partition was found. We can then proceed with the proper choice of $\beta^{\prime}$ and $\gamma^{\prime}$, just as we could with $\beta$ or $\gamma$ after a successful LL(1) test.

At first sight this does not seem like a very bright plan, since rather than having to distinguish between the alternatives of $P$ and $Q$ separately, we now have to distinguish between the union of them, which will certainly not be easier. Also, when $P$ or $Q$ happen to be $A$, we get the same problem back that we had with $A$ in the first place. But that is not true! If we now try to distinguish between the alternatives of $A$ and run into $P$ in one alternative and $Q$ in the other, we can simply continue with the partition $\{P,Q\}$ -- provided the rest of the problems with the parser for $\{P,Q\}$ can be solved.

#### 10.1.3.2 A Top-Down Implementation

When we try to distinguish between the two alternatives of $\mathtt{S}$ in Figure 10.9, we find that they have no common prefix. Next we try the LL(1) test on $\mathtt{A}$ and $\mathtt{B}$, but since both start with an $\mathtt{a}$, it fails. So we combine $\mathtt{A}$ and $\mathtt{B}$ into $\mathtt{A\_or\_B}$. The recognizing routine for $\mathtt{A\_or\_B}$ is shown in Figure 10.10. It requires us to handle the following set of alternatives simultaneously:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112319814.png)
where the result of the recognition is given after the colon.

We see that they have a common prefix $\mathtt{a}$, for which we construct recognizing code (**token('a');**). The alternatives are now reduced to
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112320427.png)
But $\mathtt{A}$ and $\mathtt{B}$ have been replaced by $\mathtt{A\_or\_B}$ followed by tests whether a $\mathtt{A}$ or $\mathtt{B}$ resulted. This gives the following alternatives to deal with:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112320305.png)
where the test for the result is indicated by a question mark. Now the LL(1) test succeeds: FIRST($\mathtt{A\_or\_B}$) is $\mathtt{a}$, which sets it off from the two other alternatives, which start with $\mathtt{b}$ and $\mathtt{c}$, respectively. All this results in the parser of Figures 10.10 and 10.11, where we have added print statements to produce the parse tree. A sample run with input **aaaccc** yields the output 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112321171.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112321700.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112321776.png)

The recursive descent routines in a canonical LL parser just return **true** or **false**, indicating whether or not a terminal production of the predicted non-terminal was found. We see that this set of return values is extended with the identity of the non-terminal in the PLL code, and it is this small extension that makes the parser more powerful.

#### 10.1.3.3 PLL(0) or SPLL(1)?

The original definition of PLL($k$) (Friede [196]) splits the PLL($k$) test into two parts: two right-hand sides $\alpha\beta$ and $\alpha\gamma$ of rules for $A$ are distinguishable if after skipping the common prefix $\alpha$ at least one of the following conditions holds.

* Both $\beta$ and $\gamma$ start with a terminal symbol and those symbols are different.
* FIRST${}_{\mathrm{k}}$($\beta$FOLLOW${}_{\mathrm{k}}$($A$)) and FIRST${}_{\mathrm{k}}$($\gamma$FOLLOW${}_{\mathrm{k}}$($A$)) have nothing in common, where FIRST${}_{\mathrm{k}}$ and FOLLOW${}_{\mathrm{k}}$ are the FIRST and FOLLOW sets of length $k$.

This split allows the definition of PLL(0), PLL($k$) with $k=0$: the second test will always fail, but the first one remains meaningful, and saves the technique. In fact, the SLL(1) grammar from Figure 8.4 is PLL(0) under this definition -- but if we replace the rule for B by $\mathrm{B} \rightarrow \mathrm{ab} \mid \mathrm{a} a \mathrm{Bb}$


## 10.2 Bottom-Up Non-Canonical Parsing

Non-canonical parsers derive their increased power from postponing some of the decisions that canonical parsers have to take. For bottom-up parsers, this immediately leads to two questions.

The first is that bottom-up parsers already postpone the recognition of a subtree (handle) to the last possible moment, after all the terminals of the handle have been read, possibly plus a number of look-ahead tokens. So what more is there to postpone? The answer is that non-canonical bottom-up methods abandon rather than postpone the hunt for the (leftmost) handle, and start looking for a subtree further on in the input. Whether this subtree can again be called a handle is a matter of definition. We will still call it a handle, although many authors reserve that term for the leftmost fully recognized subtree, and use words like "phrase" for the non-leftmost ones.

The second question is: Why is it profitable to reduce a non-leftmost handle? Af- ter all, when a non-leftmost handle has been found and the corresponding reduction performed, the leftmost handle will still have to be found or we will never get a parse tree. Here the answer is that reducing segments further on in the input improves the look-ahead. A single token in the look-ahead may not give sufficient information to decide whether and how to reduce, but knowing that it is part of a non-terminal A or B might, and if that is the case, the grammar is amenable to non-canonical bottomup parsing. This shows that look-aheads are essential to non-canonical bottom-up parsing, and that we will need to allow non-terminals in the look-ahead.

The non-canonical bottom-up parsing methods differ in the way they resume their search for a handle. We will show here three methods, total precedence, NSLR(1), and $\operatorname{LR}(k, \infty)$; the bibliography in (Web)Section 18.2.2 shows several examples of other techniques.

Farré and Fortes Gálvez [209] describe a non-canonical $DR (k)$ parser; unlike the other parsers in this chapter it can require $O\left(n^{2}\right)$ time to parse its input.

For the - non-canonical - BC and BPC methods see Section 9.3.1.


#### 10.2.1 Total Precedence

Knuth (2007) was the first to hint at the possibility of non-canonical bottom-up parsing, but the first practical proposal came from Colmerauer (1991), who modified precedence parsing to recognize non-leftmost handles.

We shall use Colmerauer's grammar $G_{2}$ for the explanation:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310112349292.png)

This grammar produces the language $[a b]^{n} a b^{n}$, a number of as or bs, next an a, and then an equal number of bs. An example is ababb. It is tricky to make a leftto-right parser for this language, since all as before the last a come from the rule $\mathbf{S} \rightarrow \mathbf{a S B}$, but the last a comes from $\mathbf{S} \rightarrow \mathbf{a}$, and we cannot know what is the last $\mathbf{a}$ until we have seen the end of the input, after $n \ \boldsymbol{b}s$.

The grammar is not simple precedence (it is not LR(1) either, as is easily shown by considering the $\operatorname{LR}(1)$ state after the input string $\boldsymbol{aa}$). Its simple-precedence table can be computed using the procedure sketched in Section 9.2.4; the result is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120017761.png)

This table has two $\lessdot /  \gtrdot$ conflicts, $\mathbf{a} \lessdot / \gg \mathrm{b}$ and $\mathrm{b} \lessdot / \gtrdot \mathrm{b}$. The first should be resolved as $a\gtrdot b$ when the a is the last a but as $a\lessdot b$ when it is not; and the second as $b\lessdot b$ when it occurs before the last a and a s $b\gtrdot b$ after the last a. These are fundamental conflicts, which cannot be resolved by traditional conflict resolvers (Section 9.9) or by resorting to other precedence methods.

In a _total precedence_ parser we want to read on past the conflict and try to find another handle further on that will shed light on the present problem. The $\gtrdot$ relations in the conflicting entries prevents us from doing so, so we remove these; it is clear that that will have repercussion elsewhere, but for the moment it allows us to continue. Since now all combinations of **a** and **b** have the relation $\lessdot$, we will shift all **a**s and **b**s until we reach the end of the string, where we are stopped by the $\gtrdot$ in **b$\gtrdot$#:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120027725.png)

This leads us to reduce the handle $\lessdot$**b$\gtrdot$** to **B**, and this reduction turns out to be a great help.

Now that the subtree **B** has been recognized, it can be considered a newly defined terminal symbol, just as we did in cancellation parsing in Section 6.8. Going back to the simple-precedence table construction procedure sketched in Section 9.2.4, we see that the juxtaposition of **S** and **B** in the right-hand sides of **S$\rightarrow$aSB** and **S$\rightarrow$bSB** requires a $\gtrdot$ relation between all symbols in $\mbox{\rm LAST}_{\rm ALL}(\mbox{\bf S})$ and **B**, if **B** is a terminal. Since $\mbox{\rm LAST}_{\rm ALL}(\mbox{\bf S})$={**a,b,B**}, we get **a$\gtrdot$B**, **b$\gtrdot$B**, and **B$\gtrdot$B**, which gives us the following total precedence table:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120028027.png)

With this new table the parsing of the string $\bf{ababbb}$ is straightforward:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120028500.png)
The above total precedence table was constructed by ad hoc reasoning and hand- waving, but we need an algorithm to implement this technique on a computer. It turns out that a grammar can have zero, one or several total precedence tables, and the problem is how to find one if it exists. Colmerauer [191] gives a set of equations a total precedence table for a given grammar has to obey, and provides several meth- ods to solve these equations, including one that can reasonably be done by hand. But the procedures are lengthy and we refer the interested reader to Colmerauer’s paper.


### 10.2.2 NSLR(1)
Postponing decisions in a total precedence parser was easy: just ignore the problem, read on and come back later to repair the damage. It is not that easy in an LR parser; the reason is that an LR parser bets so heavily on the first handle that it finds it diffi- cult to switch to an alternative set of hypotheses when the first set leads to problems. A good example is supplied by the structure of declarations in some (Pascal-like) programming languages:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120029633.png)
which can be described by the grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120029054.png)
The reason we want exactly this grammar is that it allows us to attach semantics to the rules  $\bf intvar \rightarrow variable_name$ and $\bf realvar \rightarrow variable\_name$ that identifies the variable name with its proper type. But the Pascal-like syntax does not supply that information until the end of the declarations, which is why canonical LR techniques are not enough.

Since the long names in the above grammar are unwieldy in items and some of the tokens serve only to improve program readability, we shall use the abbreviated and abstracted grammar from Figure 10.12. The above declarations then correspond to $\bf vvvi$ and $\bf vvvr$.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120031418.png)

We will now show how to construct a new set of hypotheses for an SLR(1) parser when we have to abandon the original search for the first handle (Tai [197]). The initial state 1 of the SLR(1) parser for this grammar is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120032114.png)
and the moment we shift over the first $\mathbf{v}$ we run into a reduce/reduce conflict in state 2 when we add the FOLLOW sets of $\mathbf{v}$ and $\mathbf{W}$ as look-aheads to the reduce items, in accordance with the recipe for SLR(1) parsers (Section 9.8):
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120032839.png)
The rest of the SLR(1) automaton is free of conflicts, but this reduce/reduce conflict is bad enough, since it reflects our inability to do the proper reduction of $\mathbf{v}$ until we have seen either the $\mathbf{i}$ or the $\mathbf{r}$. But that token can be arbitrarily far away.

#### 10.2.2.1 Creating New Look-Aheads

Clearly the present look-aheads are inadequate, so two questions arise: what look-ahead symbols do we use instead, and how do we obtain them. The look-aheads in SLR parsing derive from FOLLOW sets, which normally contain terminals only, since whatever comes after the end of the handle is the untouched input. In _non-canonical SLR(1)_ (_NSLR(1)_) we try to obtain a 1-symbol (terminal or non-terminal) look-ahead by fully reducing a segment of the input that follows the item immediately. To determine what symbols qualify for this task, we need to know what fully reduced symbols can follow a given non-terminal $A$. This is easier than it sounds, since fully reduced symbols are exactly the symbols as they appear in the grammar. The set of fully reduced symbols that can follow a given non-terminal $A$ is called _$\mathit{FOLLOW}_{\text{LM}}$($A$)_, since it is the same set of symbols which can follow $A$ in sentential forms during leftmost production; hence the subscript LM.

The FOLLOW${}_{\text{LM}}$ set can be obtained by running a variant of the FOLLOW set construction algorithm of page 245, in which the second step is replaced by (and simplified to!)
* We process all right-hand sides, including the $S$# one. Whenever a right-hand side contains a non-terminal, as in $A\rightarrow\cdots BX\cdots$, where $X$ is a terminal or a non-terminal, we add $X$ to FOLLOW${}_{\text{LM}}$($B$). In addition, if $X\cdots$ derives $\epsilon$, we add all symbols from FOLLOW${}_{\text{LM}}$($A$) to FOLLOW${}_{\text{LM}}$($B$).

This fills $FOLLOW{}_{\text{LM}}$($A$) with all unexpanded (= fully reduced) symbols that can follow $A$. 

For $\mathbf{v}$ this yields {I,$\mathbf{i}$}, where the $\mathbf{I}$ comes directly from $\mathrm{I} \rightarrow \mathrm{VI}$, and the $\mathbf{i}$ comes from $\mathbf{I\mathbf{\rightarrow}}\mathbf{V}$ and
$\mathbf{S\mathbf{\rightarrow}}\mathbf{I}$i through the $\epsilon$ clause in the algorithm. Likewise, $FOLLOW{}_{\text{LM}}$($\mathbf{W}$) = {R,$\mathbf{r}$}. Note that FOLLOW${}_{\text{LM}}$ is in general neither a subset nor a superset of $FOLLOW$.

Now that we have determined the new look-aheads we can turn to the problem of obtaining them from the rest of the input. Actually the new non-terminal look-aheads can be seen as new hypotheses for finding the handle; only now the handle will be found (if possible) in the first segment of the rest of the input. So we add the items for $\mathbf{\bullet}$I and $\mathbf{\bullet}$R to state 2, plus all the prediction items that are brought in by these:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120036287.png)
If the state still has a conflict even when using FOLLOW${}_{\text{LM}}$ rather than FOLLOW, the grammar is not suitable for this technique (but see the next section).

The new items may cause transitions to new, non-canonical states that were not present in the original SLR(1) parser. These states are used by the parser when it hunts for a non-first handle. Of course these non-canonical states can again have conflicts, and if they cannot be solved by the same technique, the grammar is again not NSLR(1).

#### 10.2.2.2 Finding Minimum Look-Ahead Sets

The above state is not yet a proper NSLR(1) state but rather a LSLR(1) state, for _L_eftmost SLR(1), since it is based on FOLLOW${}_{\text{LM}}$. The $LSLR$ technique will work for the grammar of Figure 10.12, but it can be shown that the requirement for "fully reduced" items is overly strong. Quite often a less reduced look-ahead will do, and by using such a look-ahead we can occasionally avoid a non-canonical state which would have had a conflict.

The minimum set of look-ahead symbols can be found as follows (Tai [197]). We first determine the first symbols each look-ahead non-terminal $X$ of a reduce item $A\rightarrow\cdots[\cdots X\cdots]$ goes through on its way to being fully reduced. These are easily found, since they are the symbols right after the dot in the prediction items resulting from $X$. For I they are {V,v} and for R we get {W,v}. We tentatively add these to the reduce look-aheads, resulting in
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120037367.png)
We see that we have now created a reduce/reduce conflict, but that does not surprise us since we knew already that we had to reduce the **v** to something, so we remove the **v**. The whole state 2 now becomes
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120037339.png)
but this still has shift/reduce conflicts. Remarkably, these are easily removed: when we have a choice between using, say, V to resolve the conflict between the first two items and shifting over $\nabla$ to find another non-terminal, $\mathbf{I}$, which will then just later serve to resolve the same conflict, we of course choose to reduce and not to shift. So we can remove the shift items that cause shift/reduce conflicts (but only those that were added to resolve the original conflict)! Note that this differs from the traditional preference for a shift on a shift/reduce conflict presented in Section 9.9. The item set has become a lot smaller now:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120037056.png)
And since $\mathbf{I}$ and $\mathbf{R}$ no longer appear on the left-hand side of any item, they will not pop up as look-aheads and can be removed:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120037063.png)
This is the final form of the NSLR(1) state 2.

Tai [197] proves that applying this procedure to a conflict-free LSLR(1) state cannot cause the resulting NSLR(1) state to have conflicts, but the proof is lengthy. In other words, there are no LSLR(1) grammars that are not also NSLR(1). There exist, however, grammars that are NSLR(1) but not LSLR(1); this is caused by states in the LSLR(1) parser that are absent from the NSLR(1) parser. For examples see Tai's paper.

The complete NSLR(1) automaton is shown in Figure 10.13. The other reduce states (5, 6, 7, 10, 11, 12) have not been subjected to the SLR-to-NSLR transformation, since they are already adequate SLR(1) states.

#### 10.2.2.3 A Complete NSLR(1) Parsing Example

The input string $\mathbf{vvi}$ is now parsed as follows:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120038530.png)

Here a new look-ahead $\mathbf{\Psi}$ is obtained in state $②$, which causes a further reduce; one way of understanding this is by pushing the $\mathbf{V}$ back into the input stream:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120038233.png)
and push back again, followed by two shifts:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120038809.png)

![Fig.10.13.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120039717.svg)


The look-ahead i in state ③ asks for a reduce by $I \rightarrow V$, then by $I \rightarrow VI$ in state ⑤, and then on to the start symbol $\bf S$:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120039783.png)
It is interesting to note that states 6 and 11 cannot be reached...

Salomon and Cormack [200] give an explicit algorithm for $NSLR(1)$, and apply it to complicated problems in programming language parsing.

### 10.2.3 LR (k, \infty)

As we have seen in Sections 9.6 and 9.8, the essential difference between SLR(1) parsing and LR(1) parsing is that an SLR(1) parser uses the FOLLOW set of a nonterminal A as the look-ahead set of an item $A \rightarrow \cdots$, whereas an LR(1) parser con-structs the look-ahead set precisely, based on the look-ahead(s) of the item or items $A \rightarrow \cdots$ derives from.

When in a non-canonical SLR(1) parser the necessity arises to create new items because we abandoned a reduce item $A \rightarrow \alpha \bullet$, we can do so relatively easily by expanding the non-terminals in $\operatorname{FOLLOW}_{\mathrm{LM}}(A)$ and then do some clean-up. And when we then are forced to abandon these new items, we can again turn to the $FOLLOW _{\mathrm{LM}}(A)$ set to obtain new non-terminals and from them new items. This is because the look-ahead of an item in an SLR(1) parser does not depend on its origin, but only on A.

In a non-canonical LR parser we have to collect much more information about the set of tokens that can follow a given item I in a given item set (state). First its construction should follow the principles of LR parsing, which means that it should derive from the look-ahead information of the items I derives from. And second, the information should cover the entire rest of the input, since we do not know how often and until what point we need to postpone our decisions. Non-canonical LR parsing was first proposed by Knuth [52], but it was Szymanski [194] who gave an algorithm for its construction. The algorithm yields an $\operatorname{LR}(k, \infty)$ parser, where k is the length of the look-ahead and $\infty$ (infinity) is roughly the number of times a decision can be postponed (see Section 10.2.3.4 for more on this subject).

The regular right context grammar explained in Section 9.12.2 suggests itself as a good representation of the look-ahead information required by a non-canonical LR parser, but it is not good enough. The regular grammar describes all possible right contexts of an item I that can occur, over all paths along which a state can be reached, but in an actual parsing we know that path precisely. It is easy to see that in the grammar $\mathbf{S} \rightarrow(\mathbf{S}) \mid$ a the regular right context of the item $\mathbf{S} \rightarrow \mathbf{a} \bullet is )^{*}$, but when in a parsing we have seen the first part of the input ( ( ( ( a we know that the exact right context is ) ) ) ). And it is this kind of exact right context that we want to use as a look-ahead in $\operatorname{LR}(k, \infty)$ parsing; it is a subset of the regular right context grammar and has to be constructed during parsing.

So for the moment we have two problems: how to derive the $\operatorname{LR}(k, \infty)$ right contexts and how to use them during parsing.


#### 10.2.3.1 An LR(1,${}^{\infty}$) Parsing Example

We will again use the abstract grammar for declarations in Figure 10.12 on page 360 we used in explaining NSLR(1) parsing and stick to one-token look-ahead, so $k=1$. Since we want to see exactly what happens to the look-aheads, we will build up the states very careful. The kernel items of the initial state, $1_{k}$ of the LR($1,\infty$) parser are
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120042235.png)
Expanding the non-terminals after the dot yields the expanded initial state, $1_{e},$
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120043361.png)
which differs from the initial state of the NSLR(1) parser (page 360) only in that the full right context is kept with each item. For example, the right context of I$\rightarrow\bullet$VI is i# because the item derives from $\bf S \rightarrow \bullet I i \#$.

Suppose the input is vvi. As all LR parsers, the LR(1,$\infty$) parser starts with an empty stack and the input concatenated with the end marker as "rest of input" (Figure 9.12, page 279). We will write this configuration as <<vvi#, where $\bullet$ is the gap. The look-ahead in this configuration is a **v**. Rather than examining each item to see how it reacts to this look-ahead, we first simplify the state by removing all items that do not have **v** as their dot look-ahead and then see what the rest says. The filtered state $1_{f}$ is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120046736.png)
All items agree on the action: shift, which yields state $2_k$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120047950.png)

and the configuration changes to $\bf v\bullet vi \#$. The state suggests two different reduce operations, so we need look-ahead, which we obtain by expanding the dot look-aheads (I and R). In canonical LR parsing they are replaced by their FIRST sets (FIRST(Ii#) and FIRST(Rr#), respectively) but here we want their FIRST sets plus their expansions since one of these may be the basis for a non-canonical reduce operation further on. This causes dotted items in the look-ahead parts of other items, a strange but useful construction. Szymanski does not give them a name, but we will call them "dotted look-aheads"; and we will call the non-look-ahead item the "active item".
When we expand the $\mathrm{I}$ in $\mathrm{V} \rightarrow \mathrm{v} \bullet \mathrm{I} i\#$ to $\mathrm{VI}$, we obtain the item
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120048094.png)
where V$\rightarrow$$\bullet$v$\bullet$ is the active item, $[\mathrm{I} \rightarrow \bullet \mathrm{VI}]$ is a dotted look-ahead representing the first part of the right context, and i# is the rest of that context. This indicates that the complete right context is VIi#, with the understanding that when the VI gets recognized, it must be reduced to I. Szymanski uses the notation  $\bf ]\left._{5} \mathrm{VI}\right]_{3} \mathrm{i} \#$  for this item where the subscripted bracket $]_{n}$ means: “when you get here you can reduce by rule number n” This is more compact and more efficient algorithm-wise but less informative. Note that for $k>1$ there can be more than one dotted look-ahead in an item.
Applying this expansion to all items in state $2_{k}$ we obtain state $2_{e}$ :
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120052565.png)


Now we can filter out the items that are compatible with the look-ahead $\bf v$, yielding state $2_{f}$ :
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120052004.png)
We see that there is still no agreement among the items, so we give up on the hypothesized reduces $\mathrm{v} \rightarrow \mathrm{v} \bullet$ and $\mathrm{w} \rightarrow \mathrm{v} \bullet$, and promote the dotted look-aheads to active items:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120134575.png)
Now we shift; this shift is certain to succeed, since we have just made sure all items had a v as a the dot look-ahead. The result is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120134892.png)
which brings us back to state $2_k$ and changes the configuration to $\bf vv•i\#$. It may seem natural that we come back to state $2_{k}$ here, because we have read just another $\mathrm{v}$, but it isn't. If the rule $I \rightarrow \mathrm{VI}$ had been $I \rightarrow \mathrm{VIx}$, the first item in state $2_{k}$ had been $\bf \mathrm{v} \rightarrow \mathrm{v} \bullet Ixi\#$ and that of the above state $\bf \mathrm{v} \rightarrow \mathrm{v} \bullet Ixxi\#$. This shows the profound effect of keeping the exact entire right context.

Expanding state $2_{k}$ yields again state $2_{e}$, but now the look-ahead is i ! Filtering it with this look-ahead yields the state
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120136154.png)
and now a unanimous decision can be taken. We reduce $\mathbf{v}$ to $\mathbf{v}$ and the configuration becomes $\bf \mathbf{v} \bullet \mathbf{v i} \#$. After the first $\mathbf{v}$ the parser was in the state $2_{e}$, and this state must now be filtered with look-ahead v. This yields another reduce state:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120137291.png)

We reduce the first $\mathrm{v}$ to $\mathrm{v}$, with the new configuration $\bf \bullet \mathrm{vvi} \#$, which provides a look-ahead $\mathrm{v}$, with which we filter state $1_{e}$, etc. The rest of the parsing proceeds similarly.


#### 10.2.3.2 The LR($k$,$\infty$) Algorithm

The basic loop of a non-canonical LR($k$,$\infty$) parser is different and more complicated than that of a canonical LR parser:

* Consider the item set $p_{k}$ on the top of the stack.
* Expand the look-aheads in each of the items if they are non-terminals; they then yield dotted look-aheads. This results in a state $p_{e}$.
* Filter from $p_{e}$ the items that have the actual look-ahead as their dot look-ahead. This results in a state $p_{f}$.
* See if the items in $p_{f}$ lead to a decision. Five decisions are possible: reduce; accept; reject the input; reject the grammar as not LR($k$,$\infty$); and reject the grammar as ambiguous. They are covered in detail below.
* If the items in $p_{f}$ do not lead to a decision, shift, as described below. The shifted token and the new item set resulting from the shift are stacked.

We shall now examine the five possible decisions in more detail.

* Since it is the purpose of LR parsers to produce one single parse tree, each reduction we do must be correct, so we reduce only if all items in $p_{f}$ appoint the same reduction.
* We accept the input when it has been reduced to the start symbol. The look-ahead together with the end marker make sure that this can only happen at the end of the input.
* We reject the input as erroneous when there are no items left in the top state. When that happens there are no possibilities left for the right context, so no further input can ever complete the parse tree.
* If we have abandoned the active item it is possible that there is no dotted look-ahead left to eventually turn into an active item. Now we created dotted look-aheads hoping that after a while one of them would be recognized and reduced to a single non-terminal, which would be used as a look-ahead to resolve an earlier LR conflict -- but if there is no dotted look-ahead that will never happen, and the LR($k$,$\infty$) method is not strong enough to handle the grammar.
* It is possible that when we reach the end of the input we still have more than one item left, which means that we still have not been able to make a decision, and still more than one completion of the parse tree is possible. So the input is ambiguous, the grammar is ambiguous, and the grammar is not LR($k,\infty$).

Normally in an LR parser, when we shift we move the dot one place to the right in the (active) item, and when the dot happens to be at the end of the item we would not shift but rather reduce or have a conflict. Here we may need to shift even if the active item is a reduce item, and we have seen above how we do that: we abandon the active reduce item. If there is a dotted look-ahead at the front of the right context now, it is promoted to active item with the dot at the beginning. And if there is not, the item will continue for a while without active item, until a dotted look-ahead is finally shifted to the front. This can only happen for $k>1$.

LR($k,\infty$) parsing is much more powerful than LR($k$) parsing, but this power comes at a price. It is undecidable whether a grammar is LR($k,\infty$), and we have seen above that even during parsing we can find that the grammar is not LR($k,\infty$) or is even ambiguous. So we can successfully parse millions of strings with a grammar and only then find out that it was not LR($k,\infty$) and the parser was unsound. Also, the method has several serious implementation problems (see next section), but then again, it _is_ the strongest linear-time parsing technique for unambiguous grammars known.

#### 10.2.3.3 Problems with and Fixes for the LR($k,\infty$) Parser

We just claimed that LR($k,\infty$) parsers have linear time requirements, but the signs are not favorable. Suppose we have a grammar $\bf S\rightarrow aSb|aSc|\varepsilon$  and an input $\bf aaaa\cdots$. Then we meet the following kernel item sets:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120140116.png)
etc., so we see that the size of the item set grows exponentially. And when we try the algorithm on the left-recursive grammar of Figure 9.14, even the initial state is infinitely large because it contains infinite sequences like
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120140676.png)
The cause of these problems is that right contexts are finite-state (Type 3) languages and the above algorithm constructs finite-choice (Type 4) expressions for them. The ever-growing states $2_{k}$, $3_{k}$,... above actually contain only two items regardless of the number of **a**s read; for example, state $4_{k}$ is actually
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120141826.png)
and the infinite sequence can be condensed into a single regular expression:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120141871.png)
So the expansion step in the LR($k$,$\infty$) algorithm must be extended with grammar-to-expression transformations like those in Figure 5.19. Unfortunately the details of this step have not been published, as far as we know.

With these transformations the item sets no longer grow infinitely or exponentially, but they still grow, linearly. After having processed seven $\bf{a}s$ from the input the two-item state is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120142580.png)This can_not_ be condensed to
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120143940.png)

because only exactly seven **b**s or **c** are acceptable, one from the active item and six from the look-aheads. Since the look-ahead sets are copied from item to item, a linear growth in look-ahead size translates into a quadratic time requirement. Fortunately there is a simple way to fix this problem. New look-aheads are created only when a non-terminal is expanded; during this expansion an item $A\rightarrow\alpha\bullet B\beta$$\gamma$ causes an item $B\rightarrow\bullet\delta$$\beta\gamma$ to be created, so the look-ahead changes from $\gamma$ to $\beta\gamma$: the addition is always at the front of the old look-ahead. So we can implement the item $B\rightarrow\bullet\delta$$\beta\gamma$ as $B\rightarrow\bullet\delta$$\beta\mathcal{P}$ where $\mathcal{P}$ is a pointer to $\gamma$, the look-ahead of the parent item. This reduces the copying of an item to constant costs, and the overall parse time requirements to linear in the length of the input. (The dotted look-aheads complicate the algorithm somewhat but can be handled in basically the same way.)

LR($k$,$\infty$) parsing is the most powerful linear-time parsing algorithm known today. It can handle many more grammars than LR($k$) but it cannot handle all unambiguous grammars; an example of an unambiguous non-LR($k$,$\infty$) grammar is $S\to aSa|\epsilon$. It is undecidable whether a grammar is LR($k$,$\infty$), and the parser discovers its own insufficiency only while parsing a string that hits one of its weak spots. LR($k$,$\infty$) is also called _NLR($k$)_, for Non-canonical LR($k$).

#### 10.2.3.4 LR($k$,$t$)

LR($k$,$\infty$) parsing may be the most powerful linear-time parsing algorithm known today, but it has one problem: decidability. Not only does the undecidability surround it with a blanket of uncertainty, it also prevents the creation of a table-driven version. Section 10.5 has shown us how much less convenient and efficient the interpretive parser in Figure 17 is compared to a table-driven one based on the deterministic automaton of Figure 18, and we would like to "upgrade" our LR($k$,$\infty$) parser in a similar way. One reason why we cannot is that the LR($k$,$\infty$) parser has an infinite number of states, as the examples in the previous sections show. If it had a finite number of states, we could construct them all, and thus achieve decidability and a table-driven version at the same time.

So it becomes interesting to see why there are infinitely many states. There are only a finite number of dotted items, and a much larger but still finite number of combinations of them, but it is the unbounded length of the right contexts that causes the number of states to be infinite. This raises the question why we need unbounded length right contexts, especially if we use a finite look-ahead of $k$ tokens only. The answer is that the segment of the right context after the first $k$ tokens serves one important purpose: to create dotted look-aheads which turn into active items when the original active item is abandoned. So intuitively it should help if we restricted the number of times the active item in a given item can be abandoned to say $t$; this leads to _LR($k$,$t$)_ parsing. (This notation also explains the name LR($k$,$\infty$).)

To understand that this works we go back to the explicit, finite choice implementation of right contexts, where a right context is just a string of terminals and from a shift over $\mathrm{p}$ in a grammar which contains the rules $\mathrm{P} \rightarrow \mathrm{p}$, $\mathrm{R} \rightarrow \mathbf{r}, \mathbf{S} \rightarrow \mathbf{s}$, and $\mathbf{T} \rightarrow \mathrm{t}$, among many others, and we follow this item through the shift and expand actions performed on it. We will assume that at each decision point there are other, conflicting, items in the same state which force us to abandon the active item; this assumption causes the maximum utilization of the right context.

Since $t=2$, we can abandon the active item only twice, and to keep track of this we record the number of abandons with the item; see Figure 19. In step 2 of the table we expand the look-ahead R to a dotted look-ahead, which turns into an active item in step 3, due to giving up on $P\rightarrow p \bullet$. Also the counter rises to 1. Similar actions bring us to step 6 where the counter has risen to 2, and no further "abandon and shift" is possible.

Exhausting the number of abandons allowed means two things. The first is that when the exhausted item occurs during parsing and we still cannot make a decision, the grammar is not $\operatorname{LR}(1,2)$ and the parsing fails. The second is more important for our purposes: we see that the trailing uv\# never played a role, so we can remove them from the original item $\bf \mathrm{P} \rightarrow \mathrm{p} \bullet RSTuv\#$, truncating it to $\mathrm{P} \rightarrow \mathrm{p} \bullet \mathrm{RST}$.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120145446.png)
In this way we can for each item find out how much of its right context is needed to allow at most $t$ abandons. This keeps the right contexts limited in length and keeps the number of possible LR($k$,$t$) states finite, so we can construct a table-driven parser. Also, we do not have to wait until parse time to find conflicts; they reveal themselves during table generation, as with LR parsers: we have achieved decidability! Szymanski [194] gives details.

Just as the power of LR($k$,$\infty$) came at a price, decidability, the decidability of LR($k$,$t$) comes at a price: power. Although LR($k$,$t$) can handle many more grammars than LR($k$), it cannot handle more languages.

#### 10.2.3.5 Discussion

LR($k$,$\infty$) parsing is the strongest linear-time parsing algorithm known today, both with respect to grammars and to languages. Suitability of a given grammar cannot be checked in advance, so the parser may reject the grammar while parsing. The full algorithm is quite complicated and carries a heavy performance penalty, as states, look-aheads and right contexts are constructed on the fly.

LR($k$,$t$) parsing is the strongest decidable linear-time parsing algorithm known today, with respect to grammars. It handles many more grammars than LR($k$), but can handle deterministic languages only. Its table-driven implementation is as efficient as LALR(1) parsing, but the table construction algorithm is very complex and the tables can be large.

Hutton [202] describes non-canonical LALR($k$) (NLALR($k$)) and (NLALR($k$,$t$)), also called LALR($k$,$t$). It turns out that it is undecidable if a grammar is NLALR($k$), but it is decidable if a grammar is NLALR($k$,$t$), just as with LR($k$,$\infty$) and LR($k$,$t$).

As Szymanski [194] and Farre and Fortes Galvez [207] point out, non-canonical LR parsing does LR parsing with context-free look-ahead. It could with some justification be called LR-context-free, in analogy to LR-regular.

### 10.2.4 Partitioned LR

When an LL(1) parser is confronted with two alternatives, both starting with the same token, as in
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120146893.png)
it has a FIRST-FIRST conflict; but an LR parser merrily shifts the **a** and accepts both alternatives, leading to a state  $\{\mathrm{P} \rightarrow \mathrm{a} \bullet, \mathrm{Q} \rightarrow \mathrm{a} \bullet\}$ (which is why there is no such thing as a shift-shift conflict). When an LR parser is confronted with two possible reduction rules, as in the state $\{\mathrm{P} \rightarrow \mathrm{a} \bullet, \mathrm{Q} \rightarrow \mathrm{a} \bullet\}$, it has a reduce/reduce conflict; but a _Partitioned LR_ parser merrily reduces the **a** to both non-terminals, resulting in a set ${P,Q}$. This is of course only possible when all right-hand sides in the reduction have the same length.

#### 10.2.4.1 Sets of Non-Terminals as LR Stack Entries

In a Partitioned LR parser, the LR stack can contain sets of non-terminals in addition to the usual non-terminals. The uncertainty that that implies can often be resolved by later reductions, as the following example shows. Suppose the top few elements of the stack are
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120146755.png)

where the non-terminal sets are linked to partial parse trees as the non-terminals did in Figure 12; so $\bf \{U,V,W\}$ points to a tree that can represent a terminal production of a $\bf U$, a $\bf V$ or a $\bf W$, and similarly for the other sets. Now suppose the top state $\textbf{s}_{6}$, possibly with help from some look-ahead, indicates that the parser should reduce using the rules A$\rightarrow$QSU and B$\rightarrow$RSV. Then the top three non-terminal sets get scooped up from the stack and linked to a single node for $\bf \{A,B\}$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120148271.png)
We see that the set $\{\mathrm{U}, \mathrm{V}, \mathrm{W}\}$ has been narrowed down to $\{\mathrm{U}, \mathrm{V}\}$ in the process, and that the second member of the right-hand side has been fixed to S, since the W and the $\mathbf{T}$ are not compatible with the right-hand sides of the reduction rules $\mathrm{A} \rightarrow Q \mathrm{QSU}$ and $\mathrm{B} \rightarrow \mathrm{RSV}$.


We also note that, unlike canonical LR states, Partitioned LR states can contain reduce items with unrelated right-hand sides. In a canonical LR state each right-hand side must be a suffix of another right-hand side or vice versa, for example F$\rightarrow$AbC$\bullet$ and G$\rightarrow$bcC$\bullet$, since both must match the top of the stack $\cdots$AbC$\bullet$. Actually the same is true in Partitioned LR parsers, but since the stack contains sets of non-terminals, the right-hand sides of rules in a state have much more leeway, and indeed the reduce items A$\rightarrow$QSU$\bullet$ and B$\rightarrow$RSV$\bullet$ both match the top of the stack above. If they did not they would not have survived the shifts over {Q,R}, {S,T} and {U,V,W}.

Now suppose state $\mathbf{s}_{7}$ tells us to reduce with the rule C$\rightarrow$PB. The reduction refines the {A,B} to a single B. This information then propagates into the tree for {A,B}, fixing {Q,R} to R and {U,V} to V, resolving all uncertainties:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120150540.png)
We see that a reduce action in a Partitioned LR parser entails updating the parse tree in addition to the usual task of creating a new parse tree node. Actually one can distinguish these two tasks even in a canonical LR parser: first the node is created with its children, and then the node is labeled with the proper non-terminal. Similarly, we have seen in Section 9.2.2 that operator-precedence parsers construct skeleton parse trees: nodes are just constructed; they never get labeled. Node construction and node labeling are two fairly independent actions; only in canonical LR parsing do they occur simultaneously.

#### 10.2.4.2 A Partitioned LR Parsing Example

It is relatively easy to construct a Partitioned LR handle-finding automaton, and we even have the choice between LR(0), SLR(1), etc. for the look-ahead. We first construct the canonical LR automaton, of the desired kind. When it has no conflicts, we are of course done and do not need to resort to non-canonical techniques. When it has shift/reduce conflicts or reduce/reduce conflicts with rules of unequal lengths, the Partitioned LR technique cannot help us (but see Problem 10.17). But when there is a state with a reduce/reduce conflict with rules of equal lengths, we mark the state for reduce with all these rules for the appropriate look-aheads. This creates a set of non-terminals $A=\{A_{1},A_{2},\ldots,A_{n}\}$ as a potential stack entry. We examine each state that contains at least one item $B\rightarrow\cdots\bullet A_{k}\cdots$ and see how it reacts to shifting over the set $A$. This may create new states, and so on, but the process will eventually terminate. If this removes all conflicts, the grammar is Partitioned LR of the desired kind.

Figure 10.15 shows part of the Partitioned SLR(1) automaton for the grammar of Figure 10.12, and we will use it to demonstrate the parsing of the input **vvvi**.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120150185.png)
The initial state 1 is that of an SLR(1) parser, as it was in Figure 10.13. Shifting over the **vv** brings us to state 2, which under a look-ahead of **vv** asks for a reduce to {**V**,**W**}:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120150106.png)
Next, we shift over the {**V**,**W**} and the second **vv**:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120151759.png)
and history repeats itself. But after the third $\mathbf{v}$ is shifted, state 2 finds a look-ahead $\mathbf{i}$ and can now authorize a reduce to $\mathbf{v}$:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120151884.png)

Shifting over the V brings us to state *, which reduces the V to an I, which results in state * on the top of the stack. This causes the reduction of {V,W}I to I, which requires fixing the {V,W} to V. From there the road to the end state is clear:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120153882.png)
There are striking similarities but also considerable differences between this Partitioned LR example and the NSLR(1) parsing example on page 363. In Partitioned LR non-terminals do not figure as look-ahead, and the shift and reduce actions are more similar to those of an LR parser than of an NSLR parser. On the other hand an NSLR parser does not need to update the parse tree.

#### 10.2.4.3 Discussion

A restricted parser based on the above principles was described by Madhavan et al. [206]. The described parser is used as a structuring tool in compiler design in a technique called Graham-Glanville code generation.[^1] It requires grammars to use only two types of rules, $A\to B_{1}\cdots B_{n}t$ and $A\to B$, with the restriction that if the terminal $t$ occurs in more than one rule, all these rules must have the same value for $n$. This requirement ensures that the grammar is Partitioned LR(0), but makes the parser impossible to use in a more general setting. No other publication on Partitioned LR is known to us.

[^1]: In Graham–Glanville code generation a bottom-up parser is used to structure the stream of intermediate machine instructions originating from the intermediate code generator in a compiler into final machine instructions, which are specified to the parser as grammar rules.

One practical advantage of Partitioned LR is that it delivers a partially resolved parse tree, which can then be disambiguated on the fly or off-line by grammatical or external means. This is exploited by Madhavan et al. by incorporating a cost function in the parser; this cost function cooperates with the parser to find the optimal structuring of the input as to costs. For details see Madhavan et al. [206].

It is easy to see that the class of Partitioned LR grammars and that of NSLR grammars are incommensurable. NSLR can handle grammars with reduce-reduce conflicts with rules of unequal length (for example, the grammar of Figure 10.12 with $\bf W\rightarrow v$ replaced by $\bf W\rightarrow vv$) which Partitioned LR cannot. Partitioned LR can handle some ambiguous grammars (for example, the grammar of Figure 10.12 with $S\rightarrow Rr$ replaced by $S\rightarrow Ri$), which NSLR cannot. In fact, the ability to handle ambiguous grammars is one of the strong points of Partitioned LR, since it allows the efficient construction of ambiguous parse trees, which can then be disambiguated on external criteria.

It is clear that Partitioned LR parsing needs more research.

## 10.3 General Non-Canonical Parsing

In a sentence like "The neighbors invited us to a barbecue party", the word that carries the most syntactic and semantic information is "invited". It tells us many things: the action is in the past; it is a transitive verb so we should be looking for two noun phrases, one for the subject and one for the object; and if we have a good data base entry for "to invite" we know that the subject and object are very likely human, and that there might be a preposition phrase starting with "to". With this knowledge we can identify the noun phrase "the neighbors" as the subject, "us" as the object, both duly human, and "to a barbecue party" as the preposition phrase. In the noun phrase "the neighbors", the most significant word is "neighbors"; in "to a barbecue party" it is "party"; etc. And we already see a parse tree emerging.

When we want to develop this idea into a parsing technique, we meet two problems: how do we tell the computer what is the most important component of a phrase; and how does the computer find that component in the input. The answers are "head grammars" and "head-corner parsing," respectively. A _head grammar_ is a CF grammar in which one member in each right-hand side is marked as the "head" of that right-hand side. The top level of a very simple head grammar of English could look like this:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120155216.png)

Here NP stands for "noun phrase", VP stands for "verb phrase", and PP for "preposition phrase". We use a bar over a symbol to indicate that it is the head; if there is only one symbol in a right hand side it is the head automatically. Head grammars were first discussed by Proudian and Pollard [198], and made popular by Kay [199].

Several algorithms have been published to exploit the head information and to lead the parser to the proper heads of the phrases. These are called _head-corner parsers_, for reasons to be explained below. They include modified chart parsers (for example Proudian and Pollard [198], or Kay [199]) or modified Earley parsers (for example Satta and Stock [201], or Nederhof and Satta [203]).

An intuitively appealing version of a head-corner chart parser is given by Sikkel and op den Akker [204]. We start from the start symbol $S$. In each of its right-hand sides we expand the symbol marked "head", unless it is a terminal, and at the same time construct the corresponding partial parse tree. We then continue expanding non-terminals marked "head" in these partial parse trees, producing more and more partial parse trees, until in each of them we reach a terminal marked "head." This process yields a set of spines similar to the ones produced in left-corner parsing in Section 10.1.1 and Figure 10.3. Whereas left spines are constructed by systematically expanding the _leftmost symbol_ until we meet a terminal, head spines are constructed by systematically expanding the _head symbol_ until we meet a terminal. Like the left spines in Figure 10.4, head spines can contain cycles. In fact, if the head in every right-hand side in the grammar is the leftmost symbol, head-corner parsing turns into left-corner parsing. This is how head-corner parsing got its name, in spite of the fact that no corner is involved.

This preparatory step, which is independent of the input, yields a large number of head spines, each connecting $S$ to some terminal $t_{A}$ through a rule $S\to\alpha\bar{A}\beta$, where $t_{A}$ is eventually produced by $A$ through a chain of head non-terminals. For each $t_{A}$ found in the input at position $p$, a partial parse tree $P$ is now constructed from $S$ to $t_{A}$. For such a parse tree $P$ to be correct, $\alpha$ has to produce the segment $1\ldots p-1$ of the input and $\beta$ the segment $p+1\ldots n$.

Now suppose $\alpha$ is actually $BC$. We then construct all head spines for $B$ and $C$, and for all head spine terminals $t_{B}$ produced by $B$ and $t_{C}$ produced by $C$ that occur in that order in the input segment $1\ldots p-1$ we connect their spines to $P$. Next we do the same for $\beta$ and the segment $p+1\ldots n$. If the required spines cannot be found $P$ is discarded. We continue this process recursively on both sides, until all input tokens are accounted for. We have then constructed all possible parse trees. Sikkel and op den Akker [204] use chart parsing arcs to do the administration, and give many details in their paper.

The above explanation actually missed the point of head-corner parsing, which is that semantic considerations can be introduced to great profit at an early stage. When we have a head spine $S\to\cdots\bar{A}\cdots$, $A\to\cdots\bar{F}\cdots$, $F\to\cdots\bar{G}\cdots$, $G\to\cdots\bar{t_{A}}\cdots$, we can take all the semantic information attached to $t_{A}$ -- its "attributes" -- propagate them up the spine and use them to restrict possible head spines to be attached to the dots on the left and the right of the spine. Suppose, for example, that $t_{A}$ is a verb form identifying a feminine plural subject, and suppose the dots to the left of $F$ in the rule for $A$ include a possible subject, then only head spines ending in a terminal which identifies a feminine plural form need to be considered for that position. This tends to quickly reduce the search space.

An in-depth description of a head-corner parser implemented in Prolog is given by van Noord [205].

We see that head-corner parsing identifies the nodes in the parse tree in a characteristic non-standard way. That and its close relationship to left-corner parsing has led the authors to classify it as a general (non-deterministic) non-canonical method and to cover it in this chapter; but we agree that the taxonomy is strained here.

## 10.4 Conclusion

Non-canonical parsing is based on postponing some decisions needed in canonical parsing, but postponement is something that is open to interpretation, of which there are many. Almost all decisions can be postponed in more than one way; a whole range of parsing techniques result, and the field is by no means exhausted.

The advantage of non-canonical parsing techniques is their power; LR($k$,$\infty$) is the most powerful linear parsing technique we have. Their main disadvantage is their sometimes inordinate complexity.

## 10.5 Problems

**Problem 10.1**: What conclusion can be drawn when a production chain automaton (like the one in Section 10.1.1.2($a$)) happens to be deterministic?

**Problem 10.2**: Analyse the movements of the strong-LC(1) parser from Figure 10.6 on the incorrect input string **n)**. What nodes did it predict before detecting the error?

**Problem 10.3**: Why is it impossible for a left-corner parser to produce the analysis shown in Figure 10.7 immediately, including the proper parentheses?

**Problem 10.4**: Construct the full-LC(1) parse table corresponding to the strong-LC(1) one in Figure 10.6.

**Problem 10.5**: Rosenkrantz and Lewis, II [101] and Soisalon-Soininen and Ukkonen [104] use slightly different and incompatible definitions of LC($k$). Map the differences.

**Problem 10.6**: _Project:_ Implement an LC(1) parser for the example in Section 10.1.1 using an LL(1) parser generator and add code to produce a correct parse tree.

**Problem 10.7**: _Project:_ The treatment of left-corner parsing is marred by an asymmetry between rules with right-hand sides that start with a terminal and those that start with a non-terminal. This nuisance can, in principle, be remedied by introducing a non-terminal $\mathcal{E}$ which produces only $\epsilon$, and putting an $\mathcal{E}$ in front of all right-hand sides that start with a terminal symbol. Rethink the examples and algorithms of Section 10.1.1 for a thus modified grammar.

**Problem 10.8**: By definition non-canonical parsers take their parsing decisions in non-standard order. The parsers from Figures 10.17 and 10.10 print the rules involved in the parse tree in standard postfix order, just as any LR parser would. Is there a contradiction?

**Problem 10.9**: Arguably the simplest non-canonical bottom-up parser is one in which any substring in the sentential form that matches a right-hand side in the grammar is a handle. Determine conditions for which this parser works. See also Problem 9.1.

**Problem 10.10**: On page 361 we wrote that FOLLOW${}_{\text{LM}}$ is in general neither a subset nor a superset of FOLLOW; explain.

**Problem 10.11**: _Project:_ As we saw on page 364, the NSLR table generation process can produce unreachable states. Design an algorithm to find and remove them, or an algorithm that does not create them in the first place.

**Problem 10.12**: Since the FOLLOW${}_{\text{LM}}$ set can contain symbols that cannot actually follow a given item in a given set, it is possible that the NSLR(1) parser introduces an item for a look-ahead non-terminal, which, when it is finally recognized in an incorrect input string, does not allow being shifted over, giving rise to an error message of the type "Syntactic entity $X$ cannot appear here". Construct a grammar and input string that shows this behavior.

**Problem 10.13**: When we abandon an active item in an LR($k$,$\infty$) parser and find that there is no dotted look-ahead to be promoted to active item, all is not lost. We could: 1. scan the look-ahead to find the first non-terminal, which will be at position $k_{1}>k$, and try parsing again, this time as LR($k_{1}$,$\infty$); 2. hope that the rest of the input matches one of the other items in the state, so we can at least parse _this_ input. Develop and evaluate both ideas.

**Problem 10.14**: Suppose we reach the end of the input in an LR($k$,$\infty$) parser and we still have more than one item left. Design an algorithm that constructs the multiple parse trees from the state of the parser at that moment.

**Problem 10.15**: _Project:_ Design and implement a complete, linear-time, LR($k$,$\infty$) parser.

**Problem 10.16**: Complete the Partitioned LR automaton of Figure 10.15.

**Problem 10.17**: Suppose there is a shift/reduce conflict in a Partitioned LR parser. Design a way to adapt the grammar to Partitioned LR parsing.

**Problem 10.18**: _Project:_ As with the NSLR table generation process, the Partitioned LR table generation process as described in Section 10.2.4.2 can produce unreachable states. Design an algorithm to find and remove them, or an algorithm that does not create them in the first place.

**Problem 10.19**: _Project:_ Design and implement a complete Partitioned LR parser.

**Problem 10.20**: _Project:_ Since head-corner parsers require the nodes of the parse tree to be constructed in non-canonical order, it seems an ideal candidate for non-canonical parsing. Design a non-canonical LR parsing algorithm that postpones the reduction until it can reduce the head of a rule.