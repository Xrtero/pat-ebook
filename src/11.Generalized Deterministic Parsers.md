# Chapter 11. Generalized Deterministic Parsers

Generalized deterministic parsers are general breadth-first context-free parsers that gain efficiency by exploiting the methods and tables used by deterministic parsers, even if these tables have conflicts (inadequate states) in them. Viewed alternatively, generalized deterministic parsers are deterministic parsers extended with a breadth-first search mechanism so they will be able to operate with tables with some multiple, conflicting entries. The latter view is usually more to the point.

Before going into the algorithms, we have to spend a few words on the question what exactly constitutes a "generalized deterministic parser". Usually the term is taken to mean "a parser obtained by strengthening an almost-deterministic parser by doing limited breadth-first search", and initially the technique was applied only to parsers with LR tables with just a few inadequate states. Later research has shown that "generalized parsing" can also be used profitably with tables with large amounts of inadequate states, and even without tables (actually with trivial tables; see next paragraph). We will therefore cover under this heading any breadth-first CF parser with some, even the weakest, table support.

Trivial parse tables are interesting in themselves, since they are a low extreme all other tables can be measured against: they form the bottom element if one wishes to order parse tables in a lattice. The _trivial bottom-up table_ has one state, which says: both shift the next input token onto the top of the stack and reduce with all rules whose right-hand sides match the top of the stack. The _trivial top-down table_ has one state, which says: either match the next input token to the top of the stack or predict with all rules whose left-hand sides match the top of the stack. Since states are used to make a distinction and since just one state cannot make a distinction, one can also leave it out.

As said above, the first generalized parsing algorithms that were designed were based on almost-LR parse tables, and much more is known about generalized LR parsing than about the other variants. We shall therefore treat generalized LR parsing first, and then those based on other tables.

Merrill [171] has shown that it is also possible to do generalized LR parsing by strengthening an almost-deterministic LR parser by doing _depth-first_ search.

## 11.1 Generalized LR Parsing

Section 9.4 has shown how we can make very powerful and efficient parsers for LR grammars. But although most languages of practical interest are LR (deterministic) most grammars of practical interest are not. And if we try to design an LR grammar for one of these LR languages we find that such a grammar is hard to construct, or does not provide the proper structuring needed for the semantics, or -- usually -- both. This limits the practical usefulness of LR parsing.

On the bright side, most practically useful grammars are _almost_ LR, which means that their LR automata have only a few inadequate states. So we become interested in ways to exploit such almost-LR automata, and the answer lies in reintroducing a little bit of breadth-first search (Section 7.1.2). Research in this direction has resulted in "generalized LR parsing".

Generalized LR parsing (_GLR parsing_) can be characterized as left-to-right bottom-up breadth-first parsing in which the breadth-first search is limited by information from an LR handle-recognizing automaton; the LR automaton is allowed to have inadequate states (conflicts) in it.

GLR parsing was first described by Lang [159] in 1974 but unfortunately the publication was not noticed by the world. The idea was rediscovered in 1984 by Tomita [160, 161], who wrote a 200-page book about it [162]. This time the world took notice and the technique became known as _Tomita parsing_. Over the years it was increasingly found that this naming was not ideal, and the technique is now almost universally referred to as "GLR parsing".

### 11.1.1 The Basic GLR Parsing Algorithm

The GLR method does breadth-first search exactly over those parsing decisions that are not solved by the LR automaton (which can be LR$①$, LALR$①$, SLR$①$, LR(0), precedence or even simpler), while at the same time keeping the partial parse trees in a form akin to the common representation of Section 7.1.3. More precisely, whenever an inadequate state is encountered on the top of the stack, the following steps are taken:

1. For each possible reduce in the state, a copy of the stack is made and the reduce is applied to it. This removes part of the right end of the stack and replaces it with a non-terminal; using this non-terminal as a move in the automaton, we find a new state to put on the top of the stack. If this state again allows reductions, this copy step is repeated until all reduces have been treated, resulting in equally many stack copies.
2. Stacks that have a rightmost state that does not allow a shift on the next input token are discarded (since they resulted from incorrect guesses). Copies of the next input token are shifted onto the remaining stacks.

There are a number of things to be noted here. First, if the automaton uses look-ahead, this is of course taken into account in deciding which reduces are possible in step 1; ignoring this information would not be incorrect but would cause morestacks to be copied and subsequently discarded. Second, the process in step 1 may not terminate. If a grammar contains non-terminals that can produce themselves, for example $A \stackrel{*}{\rightarrow} A$ (loops), $A$ will continuously be reduced to $A$. And grammars with hidden left recursion turn out to cause infinite numbers of $\varepsilon$-reductions. These two problems will be dealt with in Section 11.1.3. Third, if all stacks are discarded in step 2 the input was in error, at that specific point. Fourth, if the table is weak, especially if it is not produced by an LR process, it may suggest reductions with rules whose right-hand sides are not present on the stack; such reductions are then to be ignored.

### 11.1.2 Necessary Optimizations

The above steps form the basic mechanism of the GLR parser. Since simple stack duplication may cause a proliferation of stacks and is apt to duplicate much more data than necessary, two optimizations are used in the practical form of the parser: combining equal stack suffixes and combining equal stack prefixes. We shall demonstrate all three techniques using the grammar of Figure 11.1 as an example.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120157665.png)
The grammar is a variant of that of Figure 3.1 and is moderately ambiguous. Its LR(0) automaton is shown in Figure 11.2; it has one inadequate state, ⑤. Since the grammar is ambiguous, there is no point in using a stronger LR method. For more (and larger!) examples see Tomita [162] and several of the publications in (Web)Section 18.2.1.

#### 11.1.2.1 Stack Duplication

Refer to Figure 11.3, in which we assume the input **d+d+d$**.

![Fig.11.3.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120158979.svg)

The automaton starts in state ① (frame $a$). The steps shift ($b$), reduce, shift, shift ($c$) and reduce ($d$) are problem-free and bring us to state ⑤. The last state, however, is inadequate, allowing a reduce and a shift. True to the breadth-first search method and in accordance with step 1 above, the stack is now duplicated and the top of one of the copies is reduced ($eI$) while the other one is left available for a subsequent shift ($e2$). Note that no further reduction is possible and that both stacks now have a different top state. Both states allow a shift and then another ($fI,f2$) and then a reduce ($g1$, $g2$). Now both stacks carry an inadequate state on top and need to be duplicated, after which operation one of the copies undergoes a reduction ($h1.1$, $h1.2$, $h2.1$, $h2.2$). It now turns out that the stack in $h2.1$ again features an inadequate state $ after the reduction; it will again have to be duplicated and have one copy reduced. This gives the stack in $h2.1a$. Now all possible reductions have been done and it is time for a shift again. Only state ③ allows a shift on $\bf \$$, so the other stacks are discarded and we are left with $i1.1$ and $i2.1a$. Both require a reduction, yielding $j1.1$ and $j2.1a$, which are accepting states. The parser stops and has found two parsings.

In order to save space and to avoid cluttering up the pictures, we have not shown the partial parse trees that resulted from the various reductions that have taken place.

If we had done so, we would have found the two $\bf{S}s$ in _j.1.1_ and _j.2.1a_ holding the parse trees of Figure 11.4.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120202967.png)

#### 11.1.2.2 Combining Equal Stack Suffixes

Examining Figure 11.3$f1$ and $f2$, we see that once both stacks have the same state on top, further actions on both stacks will be identical, and the idea suggests itself to combine the two stacks to avoid duplicate work. This approach is depicted in Figure 11.5($f$) (Figure 11.5($a$) to ($e$) are identical to those of Figure 11.3 and are not shown). Note that we have not only combined the two ②s on top, but also the two $\tt 3s$ one token back; this is possible because they are both connected by the same token, $\bf d$. In fact, we are combining equal stack suffixes.

Combining equal stack suffixes is, however, not entirely without problems, as becomes evident as soon as we need to do a reduce that spans the merge point. This happens in (g), which also features an inadequate state. Now a number of things happen. First, since the state is inadequate, the whole set of combined stacks connected to it are duplicated. One copy (g ") is intended for the shift in step 2, but there is no shift from $⑤$ over $\$$, so the whole copy is discarded. The other (g ') is subjected to the reduce. This reduce, however, spans the merge point (state $④$ and extends up both stacks, comprising a different leftmost $\mathbf{E}$ in the two branches. To perform it properly, the stack combination that gave rise to (f) is undone, resulting in $\left(g^{\prime} .1\right)$ and $\left(g^{\prime} .2\right)$. The reduces are then applied to both stacks, resulting in (h l) and (h 2). The reduce in (h2) again puts the inadequate state $⑤$ on top, which necessitates another copy operation, to (h2.1) for the reduce, and to (h2.2) for the shift. The reduce on (h 2.1) results in the stack $①E③$, which has a $③$ on top. But we already have a stack with a $③$ on top: (hl), so we must combine their tops. We then see that the combined top is connected by two Es to two $①$s, so we can combine these two, which combines the whole stacks: the result of the reduction of (h2.1) is (h1). This shifts to (i) and from there the road is clear. This is the parser described in Tomita's first publication [160].

Although in this example the stack combinations are undone almost as fast as they are performed, stack combination greatly contributes to the parser's efficiency in the general case. It is essential in preventing exponential growth.

![Fig.11.5.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120206205.svg)
In the process of combining stack suffixes we have lost the power to construct the parse trees on the fly: we have combined $\bf{E}s$ that represent different parse trees. This might actually be a good thing. After all, in $GLR$ parsing many stacks will be discarded, and the work spent in constructing the parse trees in them is wasted. And the parse trees can be retrieved afterwards, in the same way as explained for the Earley parser (Section 7.2.1.2). More semantic information may be available then, which will reduce the number of parse trees. It is of course also possible to continue assembling parse tree information, but then fewer opportunities to combine equal suffixes will be available.

#### 11.1.2.3 Combining Equal Stack Prefixes

When step 1 above calls for the stack to be copied, there is actually no need to copy the entire stack; just copying the top states suffices. When we duplicate the stack of Figure 11.3($d$), we have one forked stack for ($e1$) and ($e2$):
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120207646.png)
Now the reduce is applied to one top state $\$$ and only so much of the stack is copied as is subject to the reduce:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120207339.png)
In our example almost the entire stack gets copied, but if the stack is somewhat larger, considerable savings can result. This is the parser described in Tomita's second publication [161].

The title of this section is in fact incorrect: in practice no equal stack prefixes are combined, they are never created in the first place. The pseudo-need for combination arises from our wish to explain first the simpler but impractical form of the algorithm in Section 11.1.1. A better name for the technique would be "common stack prefix preservation".

#### 11.1.2.4 The Graph-Structured Stack

The stack in the GLR algorithm and its optimizations has the form of a "dag", a directed acyclic graph. It is called a _graph-structured stack_, often abbreviated to _GSS_. Since stacks are processed from the top downwards in most algorithms, the edges of nodes in them point to their predecessors, although a bidirectional graph representation is occasionally useful. The GSS corresponding to the sample parsing in the previous sections is shown in Figure 11.6.
![Fig.11.6.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120208272.svg)

### 11.1.3 Hidden Left Recursion and Loops

Two grammar features threaten the correctness of the GLR parser: hidden left recursion and loops. Both features can cause infinite reduce loops and thus cause step 1 of the GLR algorithm not to terminate. That grammar loops cause reduce loops is easy to see: with rules $A\to B$ and $B\to A$ and having found a $B$, we can reduce the $B$ to $A$ using the first rule and then reduce the $A$ to $B$ by the second, getting our original $B$ back. Note that the stack does not grow in this process.

The problem with hidden left recursion is less obvious, all the more since visible left recursion is OK. Suppose the grammar has a rule $A\to\alpha A\beta$, where $\alpha$ produces $\epsilon$. Then some state $s$ contains the item $A\to\alpha\epsilon A\beta$ (item $i_{1}$), and so it also contains $A\to\bullet\alpha A\beta$ (item $i_{2}$). When we start parsing in state $s$. an $\alpha$ will be recognized, which moves the dot over it in item $i_{2}$, which makes it equal to item $i_{1}$, and we are back where we were. If $\alpha$ was recognized as matching $\epsilon$, we are still in the same place in the input and the parser is again poised to recognize an $\alpha$. Nothing has changed except that the stack now shows a recognized $\alpha$. So a loop results. We note two things: with hidden left recursion the stack grows; and visible left recursion is no problem because no reduction is involved: item $i_{2}$ is the same as item $i_{1}$ in state $s$.

By a stroke of good luck the optimizations from Sections 11.1.2.2 and 11.1.2.3 help a great deal in solving both problems. To show how this works we use the grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120208247.png)
It produces the language $\epsilon^{n}\texttt{bc}^{n}$. This is of course $\texttt{bc}^{n}$, since the leading $\texttt{es}$ are invisible, which is exactly the point. Its LR(0) automaton is in Figure 17; states 1 and 2 contain conflicts.
![Fig.11.7.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120209469.svg)
We now see clearly the root of the trouble: the transition diagram has a cycle the edges of which consist only of nullable non-terminals (As in our case), and traveling those edges can bring us back to the same state without consuming input. Or, in other words, the automaton cannot know how many As to stack to match the coming sequence of $\texttt{es}$.

We use the GLR parser based on this automaton to parse the input $\texttt{bccc}$; the resulting GSS is in Figure 18. Most of the interesting things happen already before the $\mathbf{b}$ is shifted, in position 1; the four columns $2\ldots 5$ only serve to show that any
![Fig.11.8.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120209205.svg)
number of cs can be accepted. The GLR parser starts in state $①$, which calls for a reduction, $A \rightarrow \varepsilon$, leading to a ② in the same position. State ② also calls for a reduction, $A \rightarrow \varepsilon$, again leading to a $②$. But we already have a $②$ in this position, so we combine them. Note that this creates a cycle in the GSS, a feature it did not possess before; in fact, the stack can be represented by a regular expression: $① A (②A) ^*$ . There is another stack segment starting with $A$ from $②$ but it ends in $①$ so we cannot combine it with our new stack segment.

The next round of step 1 of the GLR algorithm yields another stack segment from $②$ over A to $②$, so now it combines with the previous one, and no modification to the GSS is made. This means that if we go on doing reduces, nothing will change any more and step 1 will just loop. So we modify step 1 by replacing the condition "until all reduces have been treated" by "until no reduce modifies the GSS any more". This introduces a transitive closure component.

Step 2 shifts over the b to position 2 and introduces the stack segments $①$ b$③$ and $②$ b$③$; our stack is now represented by $①b③|①A ②A)^*b$③$.$ State $③$ is a reduce state, yielding an $\mathbf{S}$, which gets shifted. The one starting at $①$ leads to $⑥$; the one starting at $②$ to $④$. A shift over the first $\mathbf{c}$ gives a $⑤$ in position 3, which is again a reduce state which wants to reduce ASC to $\mathbf{A}$. Working back from the $⑤$, we find two paths ASc back, one leading to $①$ and the other to $②$, the latter going through the A loop once. Of course both states allow a shift on the $\mathbf{S}$ from the reduction, the first yielding a $⑥$ and the second a $④$, both in position 3. The $④$ allows the next c, $c_{3}$ to be shifted, and the resulting $⑤$ causes the two-path reduction to be done again. After shifting the last $\mathbf{c}$ and doing the reductions, an accepting state $⑥$ remains, so the input is recognized. We see that the $(②A)^{*}$ in position 1 can produce any desired number of As needed by c further on in the input. This is an application of the left context (Section 9.12.1).

Nozohoor-Farshi [167] notes that the cyclic structures resulting from $\varepsilon$-reductions in hidden left recursion can be precomputed and need not be computed repeatedly at parse time.

A different way of handling hidden left recursion and loops is to employ the $\varepsilon$-LR automaton of Nederhof and Sarbo [94] (Section 9.5.4) as the underlying automaton. Since each stack entry in this parser corresponds to a non-empty segment of the input, a reduce loop in step 1 of the GLR algorithm cannot occur. As said in that section, it complicates the reduce action itself, though.

### 11.1.4 Extensions and Improvements

The basic GLR parser has exponential time requirements, but the standard GLR parser, which includes equal stack prefix and suffix combination, is $O(n^{k+1})$, where $k$ is the length of the longest right-hand side in the grammar. To limit this to $O(n^{3})$, Kipps [165] stores distance information in the GSS so all the starting points of a reduce of length $k$ can be found quickly. The same effect is achieved by Scott et al. [182] by using an automaton that does reductions of length 2 only. Remarkably, Alonso Pardo et al. [175] derive an $O(n^{3})$ GLR parser from an Earley parser.

Most other speed improvements are based on improving the underlying LR parser. Nullable non-terminals at the beginning of a rule cause problems when they hide left recursion; those at the end cause inefficiencies because they postpone simplifying reductions. Algorithms to incorporate both kinds of $\varepsilon$-moves in the automaton are reported by Scott et al. [177], Aycock et al. [178], Johnstone and Scott [180]. The resulting parser, called a _RNGLR_ parser, for "Right-Nullable GLR", is very efficient on a wide variety of grammar; for an extensive description see Scott and Johnstone [188].

The number of stack operations -- the most expensive operations in a GLR parser -- can be reduced considerably by using the stack for recursion only; all other stack operations can be incorporated in the LR automaton, as described in Section 9.10.2. Algorithms to this effect are discussed by Aycock and Horspool [176], Johnstone and Scott [181] and Scott and Johnstone [183, 186].

The applicability of this optimization is hindered by the very large size of the tables that result for grammars for every-day programming languages like C++, but it turns out that table size can sometimes be reduced spectacularly by allowing a little bit more stack activity than needed. Trade-offs are can mapped by Johnstone and Scott [185].

Johnstone et al. [187] gives an overview of the various speed-up techniques.

There has been some speculation about what strength of LR table to use: LR(0), SLR(1), LALR(1) or LR(1); most authors and practitioners settle for LR(0), because it is the easiest table to construct. In a low-availability paper, Lankhorst [166] reports experiments which show that LALR(1) is best, but LR(0) is only 5-10% worse. LR(1) especially does much worse; its large number of states causes the GSS to be very frayed, causing much overhead. More specialized LR automata are considered by Johnstone et al. [184], with RNGLR a likely winner.

An incremental GLR parser is described by Vilares Ferro and Dion [331]. In [179] Fortes Galvez et al. show the construction of a generalized DR parser (Section 9.13.1). It required rethinking the DR parsing process.

Piastra and Bolognesi [168] outline a way to efficiently perform semantic actions during GLR parsing.

## 11.2 Generalized LL Parsing

For a long time it was thought that generalized LL parsing could not exist: an LL parser in which the conflicts were caused by left recursion could never work since it would always loop, regardless of how much breadth-first search one would do. However, in 1993 van Deudekom and Kooiman [170] reported a top-down generalized LL parser, in a report on the implementation of non-correcting error recovery according to Richter [313].

The algorithm we describe here is a simplification of theirs, the reason being that their parser is integrated with the creation of a suffix grammar and is adapted to the idiosyncrasies of _LLgen_, the LL$①$ parser generator in which it is embedded. Those aspects are discussed in annotation [320].

### 11.2.1 Simple Generalized LL Parsing

The generalized LL parser is very loosely based on Greibach's 1964 "directed production analyser" [7], which is actually a breadth-first predictive parser:

* As long as the top of the prediction stack is a non-terminal, we predict a right-hand side for it, based on the LL parse table; this is the prediction phase. Then we match the input symbol to the top of the stack; this is the matching phase.
* If the LL(1) table presents more than one right-hand side for a non-terminal $A$, the prediction stack forks, resulting in more than one top and making the node for $A$ the parent of more than one substack. We apply the above to all tops.
* This leads to a prediction tree rather than a prediction stack; technically it is a reversed tree, "reversed" because the parent pointers point from the leaves to the root. The leaves together are kept in a "top list".
* If we are about to make a prediction for a non-terminal $A$ which has already been expanded before in the same branch of the prediction tree in the same prediction session, we replace the prediction by back pointer, forming a loop in the tree.
* This leads to a prediction tree with loops; note that this is still less general than an arbitrary graph.

Greibach's parser as described in [7] works only for grammars in Greibach Normal Form (Section 6.2), thus avoiding the use of an LL(1) table and problems with left recursion. The admission of left recursion and its treatment as suggested above is relatively simple, but it turns out that its combination with $\epsilon$-rules causes a host of problems.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120217573.png)

In the absence of left recursion and $\epsilon$-rules, things are straightforward. Given the grammar given here) predicts $\mathbf{S}$$\rightarrow$$\mathbf{AB}$ for ($\mathbf{S}$,$\mathbf{a}$) (Figure 11.9($b$)), and both $\mathbf{A}$$\rightarrow$$\mathbf{a}$ and $\mathbf{A}$$\rightarrow$$\mathbf{X}$ for ($\mathbf{A}$,$\mathbf{a}$) ($c$); next $\mathbf{X}$ is expanded ($d$, where the top list is marked by the dotted box). The input $\mathbf{a}$ matches both tops, after which only the prediction $\mathbf{B}$ remains ($e$); it predicts $\mathbf{B}$$\rightarrow$$\mathbf{b}$, after which the input $\mathbf{b}$ is matched. If we keep the grammar rules used, we obtain two parse trees.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120218652.png)
There are two ways to perform the predictions: depth-first and breadth-first. Suppose we are about to predict expansions for a non-terminal $A$, and the LL(1) table indicates the alternatives $A_{1}\,\dots\,A_{n}$. In depth-first prediction we first stack $A_{1}$ and if its top is another non-terminal $B$, we predict and expand $B$ recursively, until all new branches have terminal symbols on top, which we then enter into the top list, or until we are stopped by left recursion. We then expand $A_{2}$ in the same way as the second prong of the fork, etc. In breadth-first prediction we stack the alternative $A_{1}\,\dots\,A_{n}$, one next to another in fork fashion, and append the top of each stack to the top list. Each of these tops will then get the rest of their treatment when their turn comes in the processing of the top list.

It does not make much difference which method we use, since eventually the same actions are performed, and a very similar data structure results. The action are, however, performed in a different order, and it is good to keep that in mind for the rest of the algorithm. Van Deudekom and Kooiman use depth-first prediction; breadth-first prediction makes its easier to handle infinite ambiguity (Section 11.2.3).

### 11.2.2 Generalized LL Parsing with Left-Recursion

When we introduce left recursion this simple data structure begins to shows its shortcomings. Given the grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120218877.png)
and the input $\mathbf{abb}$, we get the predictions $\mathbf{a}\mathbf{B}$ and $\mathbf{S}\mathbf{B}$, sharing the $\mathbf{B}$ (Figure 11.10(_a_)). But when we want to expand the left-recursive $\mathbf{S}$ by making a loop in the prediction tree as suggested above, two questions arise: how do we know that this is the left-recursive expansion and not the initial expansion, and where is the node to which we should connect the loop? Both questions are answered by the same device: we keep a non-terminal node after its expansion, properly marked as "expanded", as shown in Figure 11.10(_b_) where square brackets around a non-terminal indicate that it has already been expanded.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120218240.png)
We have to modify our parsing algorithm to accommodate the expanded nodes, as follows. When such an expanded node for a non-terminal $A$ appears in the top list during parsing, it means that a terminal production of $A$ has been recognized. The expanded node is skipped, and the node or nodes pointed to by its outgoing pointers are included in the top list in its place.

With this data structure we can check the top $\mathbf{S}$ for being a left-recursive occurrence by following the parent pointers down the stack to see if $\mathbf{S}$ has been expanded before in this prediction phase; and the expanded node can act as the starting point of the loop (_c_). The test requires us to know if a non-terminal was expanded during the present prediction phase; we can achieve this, for example, by keeping a set of all non-terminals expanded in the present prediction phase, or by numbering the prediction phases increasingly and recording the prediction phase number in each expanded node.

Note the direction of the back pointer: like all other pointers in the reversed tree it runs from the predicted node "back" to the predicting node. Note also that parent

Figure 11.10: Prediction trees without expanded nodes (_a_), with expanded nodes (_b_), and with a loop (_c_)pointers and back pointers are treated differently: during parsing we follow both, but during left-recursion checking we only follow the parent pointer.

It is clear that the nodes in the loop in Figure 11.10(_c_) may be used more than once. This requires us to be more careful with them during prediction: rather than just marking the node "expanded", we need to make a copy of it, and mark the copy; we then base our prediction on that copy.

We can see this mechanism in action when we continue the parsing of **abb**. We start from Figure 11.11(_a_), which is a copy of Figure 11.10(_c_) except that the original nodes for **S** and **A** are still visible. 
![Fig.11.11.svg](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120219079.svg)

Note that these are not in the top list; they play no direct role in the algorithm any more, but it is good to remember they are there. The first matching phase matches the **a**, which makes the expanded node **[A]** the only member of the next top list. The **[A]** node has already been expanded (an **A$\rightarrow$a** was recognized), we follow its pointer, and bring node **B** into the top list (_b_). Non-terminal **B** and look-ahead **b** command the prediction **B$\rightarrow$b**, so a marked copy is made of the node and the prediction **b** is based on it and its first component, the **b**, is now in the top list (_c_).

The **b** is matched, the expanded **[B]** is skipped, and we arrive at the node **[S]**. This node, unlike the nodes we have seen up to now, has more than one outgoing pointer. The parent pointer points to the bottom of the stack, indicating that the input could be finished here (indeed **ab** is produced by the grammar), and the back pointer leads to **[A]**. We follow both, so the bottom of the stack and the **[A]** node together form the next top list ($d$). The LL$①$ parsing table immediately rejects the bottom of the stack, and the **[A]** is skipped because it has already been expanded (a **A$\rightarrow$S** has been recognized). Now node B is the only member of the new top list; this brings us back to Figure 11.11($b$), and the system is ready to match the next **b**.

This technique can handle the ultimate in left recursion, a loop in the grammar, but only with some difficulty. Prediction for the grammar **S$\rightarrow$S**, **S$\rightarrow$a** and input **a** ties the parser in an infinite loop if we do not take measures.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120220047.png)
The initial step predicts both rules for **S**; the first one is left-recursive and causes a loop in the prediction tree. The result is shown in Figure 11.12($a$). Matching the **a** causes the **[S]** node to become the only member of the top list; its processing follows parent and back pointers, and appends the bottom of the stack and the node **[S]** itself again to the top list ($b$). The node **[S]** is then processed again, etc., resulting in a parser loop, as shown in Figure 11.12($c$).

This is not actually a fault of the algorithm. The algorithm tries faithfully to find all possible parsings, but the grammar is infinitely ambiguous, and the algorithm is just building the data structure for a infinite number of parsings. The problem can be solved crudely by not appending a node to the top list if it is already there. This does, however, ruin the property that the parser finds all possible parsings; the next section gives a much sharper criterion for the suppression of top nodes, which also solves the grammar loop problem.

### 11.2.3 Generalized LL Parsing with $\varepsilon$-Rules

The introduction of $\varepsilon$-rules to the parser described above causes hardly any problems as long as no left-recursion is involved. When the LL(1) parse table indicates an $\varepsilon$-rule $A\rightarrow\varepsilon$ as the prediction for an $A$ on the top of the stack, its empty right-hand side is stacked on the expanded $[A]$ node, which causes this node to be appended to the top list. When its turn comes, it is skipped and its parent and back pointer nodes are appended to the top list. This is exactly what should happen with a nullable non-terminal.

The situation is more complicated for a left-recursive nullable non-terminal. Suppose $A$ is such a non-terminal. As above, a node $[A]$ is appended to the top list when the system discovers that the node for $A$ produces $\varepsilon$. Later it is then skipped and its parent and back pointer nodes are appended to the top list; but the problem is that its list of back pointers may be incomplete. After all that list is still under construction, and it may happen that there is a left-recursion loop that is discovered after the nullable $[A]$ node has been processed. This can make us miss parsings.

There are several ways to solve this problem. Van Deudekom and Kooiman derive nullability information from the LL(1) parser generator, and if a left-recursion back pointer is constructed for a nullable node $[A]$ pointing to a node $B$, that node is appended to the top list right away. This makes sure that no back pointers are missed. Also, when an expanded nullable node created in the present prediction phase comes up for prediction its back pointers are _not_ followed to avoid spurious parsings. (Note that if such a node has back pointers it must be left-recursive.) For a second way of solving this see Problem 11.3.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120220391.png)

A much more severe problem arises when there is a loop in the grammar involving a nullable non-terminal. Complicated cases like form convoluted, ever-growing prediction graphs, full of predicted productions of length 0. 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310120221577.png)

Van Deudekom and Kooiman [170] do not address this problem; we shall sketch a solution here, under the assumption that prediction is performed breadth-first.

The grammar is infinitely ambiguous in many ways, which causes the algorithm to produce infinitely many parsings. These infinitely many parsings originate from the second, third, etc., appearance of a node in the top list. As said before, bluntly refusing to take more than one copy of a node in the top list damages the ability to produce the correct number of parsings in non-pathological cases. So we need a better criterion for deciding to admit an expanded node $N$ to the top list.

To this end we follow the parent and back pointers from $N$ and put them in a set $\varepsilon_{N}$, the set of nodes reachable from $N$ by $\varepsilon$-transitions. So far it is the set of nodes that will be added to the top list when node $N$ will be processed. Now for each nullable or expanded node $M$ in the set we put the parent and back pointers of $M$ in $\varepsilon_{N}$ and we continue doing so until no more nodes are to be added (this is another example of a transitive closure algorithm).

The set $\varepsilon_{N}$ thus obtained is the set of nodes that would be predicted by $N$ without predicting an intervening token, if $N$ is appended to the top list. Nodes that cause a prediction which includes at least one token or non-nullable non-terminal cannot give rise to prediction loops, since they cannot lead to the recognition of a non-terminal expanded in the present prediction phase; so the set $\varepsilon_{N}$ contains the nodes that are not safe.

Two tests can be made on this set. If the original node $N$ is not in $\varepsilon_{N}$, node $N$ can safely be appended to the top list; there is no prediction loop involving $N$ and thus no parser loop needs to be feared from $N$. If node $N$_is_ in $\varepsilon_{N}$, there is a prediction loop from $N$ to itself; appending $N$ is unsafe, but not appending it may make us lose parsings. With this information the decision is easy: if $\epsilon_{N}$ contains a node $M$ for the same non-terminal as node $N$, and $M$ is already in the top list, then we can afford not to append node $N$ to the top list. The reason is that the predictions for $M$ are the same as for $N$, and appending $N$ would just produce them a second time. If there is no such node $M$, $N$ must be appended to the top list; next time it will produce a similar $\epsilon_{N}$ set with different nodes, but now $N$ is in the top list and $N$ will not be appended a second time, thus quenching the loop.

A number of remarks can be made about this algorithm. The first is that it is immaterial whether the node $M$ is in the top list before or after the node being processed. If it is before, it has already been expanded; if it is after, it will be expanded; in both cases parsings due to $M$ will not be lost. The second is that the algorithm works with breadth-first prediction only. The top list in depth-first prediction contains terminals and symbols revealed by matching rather than the in-between results of predictions, and an additional data structure is needed to make the algorithm work with depth-first prediction.

The third is that we do not need to actually construct the set $\epsilon_{N}$. While scanning the prediction graph by following nullable and expanded nodes, we can record the answers to our two questions: do we meet node $N$, and do we find a node $M$ with the properties described above. Unless both answers are affirmative, we append the node for $N$ to the top list.

And last but not least, it is a bit worrying that the algorithm was designed by repairing the shortcomings of a simple approach; also, no correctness proof of the algorithm has been published to our knowledge. It is probably fair to say that this is the least researched and least well-founded algorithm in the book.

Van Deudekom and Kooiman [170] describe various optimizations in their report, including combining identical nodes in the top list. This turns their data structure into what could be called "a directed acyclic graph with restricted cycles" and they describe a specialized garbage collector for it.

### 11.2.4 Generalized Cancellation and LC Parsing

Similar data structures are used by Nederhof to implement generalized cancellation parsing [105] and left-corner parsing [172].

Cancellation parsing (Section 6.8) can handle direct left recursion, but it cannot handle indirect (hidden) left recursion, so loops in the prediction tree will still occur.

In addition to allowing all CF grammars, generalized cancellation parsing has the advantage over deterministic cancellation parsing that it can work with a simplified parse table that derives directly from the grammar. This allows a very simple and light-weight implementation in Prolog, for which see [105].

The LC version uses the left-corner relation $\angle$ (Section 10.1.1.5) rather than the actual LC parse table. It lends itself well for serious optimization, including such things as precomputation of $\epsilon$-generating parse forests. It is all described in [172].

### 11.3 Conclusion

The generalized non-deterministic parsing algorithms extend the powerful deterministic LR and LL techniques to general CF grammars. For grammars with limited non-determinism, which includes most practical grammars, they are very efficient and vastly outperform CYK and Earley parsers, usually achieving almost linear time dependency.

Their basic tool is the forked stack, a forked reduction stack for GLR and a forked prediction stack for GLL. Simple optimizations convert these forked stacks to directed acyclic graphs, or DAGs. Infinitely ambiguous grammars cause these DAGs to degenerate into full-fledged graphs and come with a host of parsing problems (Sections 11.1.3 and 11.2.3). Given the limited usefulness of infinite ambiguity, rejecting such infinitely ambiguous input grammars is a serious option.

To our knowledge, nothing has been published on generalized non-canonical parsing.

## Problems

**Problem 11.1**: Since the LR automaton of Figure 11.2 is already inadequate anyway, we can just as well leave out the terminator $\$$; then state 6 disappears and state 3 becomes inadequate too. How does this affect the parsing in Figure 11.3?

**Problem 11.2**: Modify the GLR algorithm so it produces a parse forest grammar rather than a parse forest. Does the resulting grammar need cleaning?

**Problem 11.3**: Design a way to allow nullable left-recursive non-terminals in a GLL parser without requiring nullability information from the LL parser generator (Section 11.2.3).

**Problem 11.4**: _Research project_: Give a formal proof of the GLL parsing algorithm of van Deudekom and Kooiman (Section 11.2) or a provable version of it.

**Problem 11.5**: _Project_: Investigate how generalized parsing can be done with less usual bottom-up methods: LR-regular, non-canonical. How about using a non-deterministic handle recognizer (as, for example, in Figure 9.15)?