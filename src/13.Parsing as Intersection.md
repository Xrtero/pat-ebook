# Chapter 13 Parsing as Intersection

In 1961 Bar-Hillel, Perles and Shamir [219] proved that "the intersection of a context-free language with a regular language is again a context-free language". On the face of it, this means that when we take the set of strings that constitute a given CF language and remove from it all strings that do not occur in a given FS language, we get a set of strings for which a CF grammar exists. Actually, it means quite a bit more.

It would be quite conceivable that the intersection of CF and FS were stronger than CF. Consider the two CF languages $L_{1}=a^{n}b^{n}c^{m}$ and $L_{2}=a^{m}b^{n}c^{n}$, whose CF grammars are in Figure 1. When we take a string that occurs in both languages, it
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131003678.png)
will have the form $a^{p}a^{q}a^{r}$, where $p=q$ because of $L_{1}$ and $q=r$ because of $L_{2}$. So the intersection language consists of strings of the form $a^{n}b^{n}c^{n}$, and we know that that language is not context-free (Section 2.7.1). But Bar-Hillel et al.'s proof shows that that cannot happen with a CF language and a FS language. On the other hand one could well imagine that intersecting with a regular language would kill all CF power of description; again Bar-Hillel's proof shows that that is not the case.

Sometimes proofs of such theorems are non-constructive: one shows, for example, that if the theorem were not true one could solve a problem of which it has been proven that it is unsolvable, like full Type 0 parsing. Bar-Hillel et al.'s proof is much better than that: it is constructive, and demonstrates how to obtain the CF grammar of the intersection language, starting from the original CF grammar and the FS automaton that describes the regular language. And what is more, the process is quite simple and has polynomial (non-exponential) time requirements.

It was not realized until the 1990s that this has something to do with parsing. It is simple to see that an input string can be considered as a very simple, even linear, FSA with the positions between the tokens as the states. When, subsequently, parse forest grammars were recognized as a convenient way to represent the output of the parsing process (Billot and Lang [164]), the pieces began to fall together.

Parsing by intersection is based on three ideas:

* the input to the parsing process can be described by a finite-state automaton;
* the output of the parsing process can be described by a CF grammar;
* there is a practical process for obtaining the grammar for the intersection of a CF grammar with a finite-state automaton.

## 13.1 The Intersection Algorithm

Since even small examples will soon yield large data structures, we shall start with an almost microscopic grammar:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131003840.png)

but we will also do examples with larger grammars further on. The input will be $\mathbf{ab}$. As a finite-state automaton this defines 2 transitions and 3 states:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131003705.png)

where 1 is the initial state and 3 the accepting state.

The intersection algorithm is unexpected but surprisingly simple. We start by creating a tentative non-terminal $A\_n\_m$ for each non-terminal $A$ in the original grammar $G_{orig}$, with the meaning that $A\_n\_m$ produces everything that $A$ produces and at the same time is recognized by the FS automaton between the states $n$ and $m$. Next we derive rules from our grammar for all the $A\_n\_m$; how we do that is shown in the next section. This set of rules form the CF part of the intersection grammar; we will call it $I_{rules}$.

Two properties prevent $I_{rules}$ from being a complete grammar: it has no start symbol and the treatment of terminal symbols is incomplete. To start with the latter, the rules in $I_{rules}$ involve terminals of the form $t\_p\_q$, which describe those terminals $t$ that provide a transition between states $p$ and $q$. Now for each terminal $t$ in the FSA that indeed provides a transition between states $p$ and $q$, we construct a rule $t\_p\_q\to t$. And we declare all non-terminals $S\_n\_m$ to be start symbols of the intersection grammar, where $S$ is the start symbol of $G_{orig}$, $n$ is the initial state of the FSA and $m$ is an accepting state of the FSA. (Note that this may assign more than one start symbol to the intersection grammar.) Together with $I_{rules}$ these constitute the rough form of the intersection grammar, $I_{rough}$.

And finally we clean $I_{rough}$ using the algorithm from Section 2.9.5, which gives us the clean intersection grammar $I$. That's all. We will first see _that_ it works and then _why_ it works.

### 13.1.1 The Rule Sets $I_{rules}$, $I_{rough}$, and $I$

The tentative non-terminals from our demo grammar are
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131003016.png)
Suppose we want to create rules in $I_{rules}$ for S_1_3. This non-terminal spans the FSA between the states 1 and 3, so its right-hand must span these two states too. There are two rules for S: S_$\rightarrow$b and S_$\rightarrow$aS. The first immediately yields a rule: S_1_3_$\rightarrow$b_1_3; but the second one involves an unknown state $X$: S_1_3_$\rightarrow$a_1_$X$S_X_3. Here the right-hand side spans 1...3 provided both occurrences of $X$ are replaced by the same state, but that state is unknown. This is "solved" by brute force: we copy the rule for all states in the FSA:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131004541.png)
We do this for all tentative non-terminals. This results in the rule set $I_{rules}$.

As usual, we have to pay some attention to $\varepsilon$-rules and $\varepsilon$-transitions. Our present example contains neither, but if the original CF grammar has a rule of the form $A\rightarrow\varepsilon$, a rule $A\_p\_p\rightarrow\varepsilon$ should be added to the intersection grammar $I_{rules}$, for each state $p$. Such rules represent the idea that an $\varepsilon$-producing $A$ can occur in the parse tree without any consequence for the FSA. The intersection algorithm as published by Bar-Hillel et al. cannot handle $\varepsilon$-transitions in the FSA, but it is not very difficult to add this feature. See Problem 13.2.

Next we add the rules for the transitions in the FSA: a_1_2_$\rightarrow$a and b_2_3_$\rightarrow$b. To complete the intersection grammar, we appoint S_1_3 as the start symbol. Since the initial state of the FSA is 1 and the accepting state is 3, S_1_3 is going to produce whatever the FSA recognizes between its initial and accepting states.

All this amounts to 27 rules from S_$\rightarrow$aS, 9 rules from S_$\rightarrow$b, and 2 rules for the terminal symbols, for a total of 38 rules. They are collected in Figure 13.2, and together they form $I_{rough}$.

The last step consists of cleaning the intersection grammar. Removing the non-productive rules and unreachable non-terminals yields the clean intersection grammar $I$; it is shown in Figure 13.3. It is important to note that it is at the same time a parse-forest grammar, and, since the parsing is unambiguous, it is a parse-tree grammar. The corresponding parse tree is shown in Figure 13.4.

The parse forest above has been obtained in an almost shockingly simple way. The algorithm does not seem to need all the tricks and cleverness that we have met in previous algorithms. The parse forest grammar just somehow "develops" out of the original grammar, the way a photograph develops out of the latent image in photographic paper.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131004812.png)
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131004423.png)


### 13.1.2 The Languages of $I_{rules}$, $I_{rough}$, and $I$

To understand better how and why the algorithm works, we will consider the languages that are produced by the three successive versions of the intersection grammar, $I_{rules}$, $I_{rough}$, and $I$.

First we consider a grammar $G_{rules}$ that has the set $I_{rules}$ as its rules. As start symbols we accept all tentative non-terminals that derive from the original start symbol, $\texttt{S\_1\_1}$ through $\texttt{S\_3\_3}$, and accept all FSA transitions $\texttt{a\_p\_q}$ and $\texttt{b\_p\_q}$ as its terminal symbols. With these provisions, $G_{rules}$ produces all sequences of transitions $\texttt{[a\,|\,b]\_p\_q}$ such that the sequence of $\texttt{a}$s and $\texttt{b}$s conform to the grammar $G_{orig}$ and the $p$s and $q$s correctly link each transition to the next. A sample of the possible productions is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131004636.png)

Restricting the start symbol to the only one that correctly specifies the initial and accepting states of the FSA, $\texttt{S\_1\_3}$, restricts the language produced to those sequences of transitions that start with $\texttt{[a\,|\,b]\_1\_x}$ and end with $\texttt{[a\,|\,b]\_y\_3}$, for any state $x$ and $y$. This restricted set contains sequences like
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131005315.png)
When we now restrict the transitions to those that are permitted by the FSA, $\texttt{a\_1\_2}$ and $\texttt{b\_2\_3}$, only one sequence remains, $\texttt{a\_1\_2\_b\_2\_3}$, which corresponds to our input string.

When we look carefully at the grammar cleaning process used to obtain $I$ from $I_{rough}$, we see that it mirrors the above restriction process, except that it works on the grammar -- which is finite -- rather than on the produced language -- which is infinite. Removing non-productive rules corresponds to removing non-permissible transitions, and removing unreachable symbols corresponds to removing start symbols that do not conform to the initial and accepting states of the FSA.

###  13.1.3 An Example: Parsing Arithmetic Expressions

We will now turn to a larger example, the parsing problem used in Section 4.1. We want to parse the sentence (i+i)xi with the grammar of Figure 4.1, here repeated in Figure 13.5.

The input sentence corresponds to the FSA

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131005668.png)

which is a convenient short-hand for the seven rules
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131005515.png)

Since the rough intersection grammar $I_{rough}$ has 12487 rules, it is not possible to demonstrate the complete process here, and the reader will have to believe us when we say that the resulting cleaned grammar $I$ is shown in Figure 13.6.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131005298.png)
When we compare this grammar to the parse forest grammar obtained from the Unger parser in Figure 4.5, we see that the two are almost identical, and that the differences are minor. In Unger parsing a grammar symbol is marked with the start position and length of the segment it spans, whereas in intersection parsing it is marked with the states of the FSA. A slightly more characteristic difference is that the rules in the Unger parse forest grammar are produced in top-down order, whereas those in the intersection grammar appear in a seemingly arbitrary order, determined by the order of the rules in the original grammar and the details of the cleaning algorithm.


## 13.2 The Parsing of FSAs

Until now we have only used linear FSAs, thereby restricting ourselves to tasks that could be performed equally well or even better by traditional parsing. We will now consider intersection with more general FSAs: FSAs for arithmetic expressions with one or more unknown tokens in them, FSAs for substrings of arithmetic expressions, and the FSA for $\texttt{?}^{*}$, where? is any token in the grammar.

### 13.2.1 Unknown Tokens

We start with the regular expression (i?i)x!; it represents an input sentence of which the third token is unknown. In terms of the FSA this means that where we had the rule +_3_4_+ in Section 13.1.3, we now have the rules

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131005273.png)

Two new rules appear in the parse forest grammar, compared to Figure 13.6:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131006953.png)

These supply a new alternative for **Expr_2_5**, and show that the algorithm has recognized that (i**x!)x! is a terminal production of **Expr** too, but since the x is in a different place in the grammar, an additional step through **Term_2_5** is required. Note that we now have a parse _forest_ since there are two alternatives for **Expr_2_5**.

Something completely different happens when we try the regular expression $(\text{i ? ? ? i}) \times i$, since we end up with an empty grammar: the cleaning process removes even the start symbol. This is correct: no terminal production of **Expr** matches $(\text{i ? ? ? i}) \times i$

Again a different effect is obtained from the regular expression $(\text{i ? ? ? i}) \times i$. A fairly long (30 rules), fairly uninformative grammar results, which contains several non-terminals with multiple alternatives, and which produces the following six sentences:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131007720.png)

### 13.2.2 Substring Parsing by Intersection

Intersection parsing can also be used to do substring parsing. For example, to parse the substring +i), the FSA corresponding to $\texttt{?}^{*}\texttt{+i}\texttt{)?}^{*}$ is offered to the intersection process. This FSA is represented by the rules 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131008522.png)
where the question mark is an abbreviation for all terminals in the grammar.  
The intersection process results in the parse forest grammar of Figure 13.7. This
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131008444.png)

grammar produces all sentences produced by the original grammar in Figure 13.5 that contain a substring **+i**). With its 12 rules it is remarkably simple, certainly for a grammar that was produced automatically, for what at first sight would not seem to be a simple problem. It is even reasonably easy to see what it does: the non-terminals marked **_1_1** and **_4_4** produce expressions to the left and the right of the substring, and only those with "mixed" markings are concerned with fitting them around the substring.

This is even more visible in a manually simplified version of the same grammar, shown in Figure 13.8. The non-terminals marked $1$1 and $4$4 have been simplified into one copy of the original grammar for **Expr**; terminal productions of this grammar are not guaranteed to contain the substring; note that this **Expr** is not the start symbol. The non-terminals marked $1$4, on the other hand, are obliged to contain the substring; **Expr_1_4** and **Term_1_4** parcel out this obligation to their left child, their right child or their only child. **Factor_1_4** is the first rule that can actually form part of the substring by producing the }_3_4. The remaining obligation to span the states 1 and 3 is passed to **Expr_1_3**, from where it trickles down. Note that none of the above considerations are present in the intersection algorithm; the algorithm just expands and cleans.

An attempt to draw the corresponding parse forest is shown in Figure 13.9.
Several simplifications have been applied: the unmarked **Expr**, **Term**, and **Factor** have not been expanded; (marked) non-terminals that occur more than once have been expanded only once; and OR-nodes are not shown when there are no alternatives. The numbers in the other OR-nodes are the numbers of the alternatives in the rule for the parent in Figure 13.8.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131009207.png)


Another interesting regular expression is  $\bf ? (?^*$ which describes all sentences that have an open parenthesis as their second symbol. The intersection grammar is again simplicity itself: Figure 13.10. It is quite remarkable that the only symbol with marking **_1_2** is an open parenthesis: the intersection process has "discovered" that if the second symbol of an expression is an open parenthesis, the first symbol must be an open parenthesis too! Manual simplification yields the grammar of Figure 13.11; we have left out the original grammar for **Expr**.

We started this chapter with Bar-Hillel's theorem that "the intersection of a context-free language with a regular language is again a context-free language", but until now we have created and discussed just the CF grammars of those languages. The grammars in Figures 13.10 and 13.11 give us the opportunity to view them as grammars which produce "new" languages, rather than as parse forest grammars which represent parsings.The difference between the two is a matter of degree.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131011909.png)

At one extreme, a parse tree can be viewed as a grammar that produces exactly one sentence; at the other, a grammar as in Figure 13.5 can be viewed as a parse forest for all arithmetic expressions. We will see more evidence for this view in the next paragraph and in Section 13.2.3.

It is also interesting to see the result of intersecting with the FSA that accepts any sequence, the FSA for $?^*$. This FSA has one state, 1, and all tokens have transitions from 1 to 1 . The intersection is so simple in this case that it can be performed by hand. The outcome is in Figure 13.12, and, except for the markings $\_1\_1$, it is identical to
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131012820.png)
the original grammar. This is of course as it should be, but it is still satisfying to see that it is.

### 13.2.3 Filtering

Since both the input and the output of the intersection process are grammars, intersection can be used as a filter: the result of intersection with an FSA can again be intersected with another FSA. We shall use this technique to construct a grammar for the language $\mathbf{a}^{p}\mathbf{b}^{q}\mathbf{c}^{r}$, $p,q,r\geq 1$, by filtering a grammar for arbitrary sequences of $\mathbf{a}$s, $\mathbf{b}$s, and $\mathbf{c}$s through an FSA which disallows $\mathbf{b}$s before $\mathbf{a}$s and then through an FSA which disallows $\mathbf{c}$s before $\mathbf{b}$s.

We start from the obvious grammar for [**abc]${}^{*}$**
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131013298.png)
and filter it through $\bf [a c]^{*} a[b c]^{*}$. This regular expression describes a sequence of $\mathbf{a}$s and $\mathbf{c}$s with at least one $\mathbf{a}$ followed by a sequence of $\mathbf{b}$s and $\mathbf{c}$s; that is, it forces all $\mathbf{a}$s to come before all $\mathbf{b}$s, without affecting the $\mathbf{c}$s. The result of the intersection is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131013572.png)
where we have replaced the marked terminals $a \_1 \_1$ etc. by their unmarked counterparts, since we are interested in the resulting grammar rather than in the parsing of the FSA for $\bf [a c]^{*} a[b c]^{*}$.

Filtering this grammar through the regular expression $\bf [a b]^{*} b[a c]^{*}$, which requires at least one $\mathrm{b}$ and forces all $\mathrm{bs}$ to come before all $\bf{c}s$, yields the grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131016778.png)
Now the only thing left is to require at least one $\mathrm{c}$ and we do this by filtering through the regular expression $\bf [a b c]^{*} \mathrm{c}$. The resulting grammar is
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131017509.png)
We see again that the grammar does not become unduly large, but it does look forbidding, until we realize that it contains only four non-terminals, $S \_1\_2\_1\_2\_1\_2, S\_2\_2\_1\_2\_1\_2$, 
$\bf S\_2\_2\_2\_2\_1\_2$ and $\bf S\_2\_2\_2\_2\_2\_2$ them to $\mathbf{S}, \mathbf{T}, \mathrm{U}$, and $\mathrm{V}$, we obtain the grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131032862.png)
which is one of the simplest grammars for the language $\mathbf{a}^{p} \mathbf{b}^{q} \mathbf{c}^{r}$, p, q, $r \geq 1$.

Note that this grammar was derived without applying any human intelligence. The final rewrite was only cosmetic and served just to make the result more readable.
## 13.3 Time and Space Requirements

The reader will already have noticed that the grammars resulting from intersection parsing are of modest sizes; twenty to thirty rules for an input of say ten tokens. This is not really surprising, since the grammars represent parse trees, which for unambiguous input have sizes ranging from linear $(O(n))$ to $O(n \ln n)$ to $O\left(n^{2}\right)$ for the worst case.

But appearances are deceptive in this case, since the intersection process is one of expansion and clean-up, and the intermediate data structures are much larger. We have seen that the fully expanded grammar for the parsing of $(i+i) \times i$ with the grammar of Figure 13.5 contains 12487 rules. This number is easily verified: There are 8 states. Each rule with a right-hand side of length 1 $A \rightarrow B$ expands into $8 \times 8$ rules $A \_n \_m \rightarrow B \_n \_m$, for all 8 values of n and m; and there are 3 of these. Each rule with a right-hand side of length 3 $A \rightarrow B C D$ expands into $8 \times 8 \times 8 \times 8$ rules $A \_n \_p \rightarrow B \_n \_m C \_m \_o D \_o \_p$, for all 8 values of n, m, o and p; and there are again 3 of these. This gives $\left(3 \times 8^{2}=3 \times 64=192\right)+\left(3 \times 8^{4}=3 \times 4096=12288\right)=12480$ rules, plus 7 for the terminals makes 12487 . More in general, a rule with a right-hand side of length k expands into $n^{k+1}$ rules, where n is the number of states in the FSA. For a linear FSA with n states, representing a string of length n-1, and a grammar in $\mathrm{CNF}$, this reduces to $n^{3}$, exactly the space requirement of the traditional general $\mathrm{CF}$ parsing algorithms.

## 13.4 Reducing the Intermediate Size: Earley's Algorithm on FSAs

We have seen that the size of the intermediate grammar may be a problem. One immediate idea to address this problem is to suppress the generation of all rules that contain non-existent marked tokens: when we are about to generate a rule like $\bf Expr\_1\_1 \rightarrow Expr\_1\_1 + \_1\_1 \ Term\_1\_1$ and know there is no $+\_1\_1$, we refrain from doing so. This optimization reduces the number of rules in $I_{\mathit{rough}}$ from 12487 to 260 (of which 11 remain after clean-up).

Another approach is trying to be as frugal as we can in creating rules, as follows. To keep the size of the example manageable, we consider the grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131040098.png)
which produces the language $\mathbf{a}^{\prime\prime}\mathbf{b}^{\prime\prime}$, and which we are going to intersect with the FSA for $\mathbf{a}^{*}\mathbf{b}\mathbf{b}$, represented by the grammar rules
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131040554.png)
Here 1 is the initial state and 3 is the accepting state.

We certainly need rules that start with the start symbol of the grammar, in the initial state of the FSA: $\mathbf{S}\_1\_X\boldsymbol{\Rightarrow}\mathbf{a}\_1\_Y$$\mathbf{S}$$\mathbf{b}$ and $\mathbf{S}\_1\_1\boldsymbol{\Rightarrow}\varepsilon\_1\_1$. Both start in FSA state 1. The first rule ends in an as yet unknown state $X$. Its right-hand side starts with an $\mathbf{a}$ starting in FSA state 1 and ending in another as yet unknown state $Y$. There are also other unknown states connecting $\mathbf{a}$ to $\mathbf{S}$ and $\mathbf{S}$ to $\mathbf{b}$, so the full form of the rule is $\mathbf{S}\_1\_X\boldsymbol{\Rightarrow}\mathbf{a}\_1\_Y$$\mathbf{S}\_Y$$\mathbf{Z}$$\mathbf{b}\_Z\_X$; note that the end state of $\mathbf{b}$ is $X$, the end state of the left-hand side. We do not know yet what the states are, but we know that they are equal. The second rule also starts in state 1, which makes the $\varepsilon$ start in state 1, which of course ends in state 1, which in its turn makes the $\mathbf{S}$ end in state 1. For convenience the rules generated by the algorithm are brought together in Figure 13. It starts with the rules for the terminal symbols since these will be
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131041201.png)

To complete the intersection grammar we need to find instantiations of the uninstantiated variables in them. The first rule with an uninstantiated variable is rule 4. We can instantiate $X$ only once we have identified the $\mathbf{b}$ at the end, so we try to find a value for the $Y$. The only way to do this is to go to the rules that have already been generated and find there a rule with a left-hand side $\mathbf{a\_1\_}k$ where $k$ is instantiated; there is only one such rule, rule 1. (We are using upper case letters ($X$, $Y$, etc.) for uninstantiated variables and lower case letters ($k$, $p$, etc.) for variables that have values.) Instantiating the $Y$ to 1 yields rule 6, where the next position in need of instantiation has moved to the $\mathbf{s}$ in the right-hand side.

Now there are two ways to use rule 6. One is to consult again the already generated rules and find a rule with a left-hand side $\mathbf{S\_1\_}k$ where $k$ is instantiated. There is one such rule in Figure 13.13, rule 5; combining it with rules 6 yields rule 7, in which only $X$ is still unknown. But since $\mathbf{s}$ is a non-terminal, there is another way: to consult the original grammar and instantiate new rules for $\mathbf{S\_1\_}Z$. The two rules for $\mathbf{S}$ in the grammar yield rules that are identical to rules 4 and 5, so this step creates no new rules.

Rule 7 requires a $\mathbf{b\_1\_}k$ which is provided by rule 2. Here $X$ finally receives a value, 2, from the $\mathbf{b\_1\_2}$. This means that a new $\mathbf{S\_1\_}k$ with $k$ instantiated has been created, $\mathbf{S\_1\_2}$. This allows a new possibility to derive a rule from rule 6: rule 9. The missing $\overline{X}$ in $\mathbf{b\_2\_}X$ is provided by rule 3, instantiating $X$ to 3; this results in rule 10. So again a new $\mathbf{S\_1\_}k$ has been created, $\mathbf{S\_1\_3}$, which again yields a new rule from rule 6: rule 11. But since there is no rule that supplies a $\mathbf{b\_3\_}k$, this is a dead end, and since no more new rules can be derived from the existing ones and the grammar, the algorithm stops here.

As a final step we clean the grammar by removing all rules that contain uninstantiated variables, mark $\mathbf{S\_1\_3}$ as the start symbol and thus obtain the intersection grammar
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131041781.png)

This grammar indeed produces $\mathbf{aabb}$ with the proper parse tree.

The reader will have noticed (if it were only from the title of this section) that the algorithm we have shown above is a variant of the Earley algorithm described in Section 7.2. The relationship can be summed up as follows.

* A tentative rule of the form $A\_p\_X\rightarrow$$B\_p\_\cdots C\_q\_Y$$\cdots$ corresponds to an item $A\to B\cdots C\bullet\cdots$@$p$ in _itemset${}_{q}$_ in the traditional Earley parser. The correspondence is not exact, though. If the Earley parser has an item $A\to BC\bullet\cdots$@$p$ in _itemset${}_{q}$_ and there are two ways in which $BC$ can produce the input segment from $p$ to $q$ with different lengths for the $B$s and $C$s, the intersection Earley parser has two tentative rules, $A\_p\_X\rightarrow$$B\_p\_i\_C\_i\_q$$\cdots$ and $A\_p\_X\rightarrow$$B\_p\_j\_C\_j\_q$$\cdots$with $i\neq j$. This is because the intersection Earley parser constructs the parse forest on the fly and does not need a second scan (Section 7.2.1.2) to construct the parse forest.
* Creating new rules from a tentative rule $A\_p\_X\to B\_p\_\cdots V\_q\_Y\cdots$ by finding symbols of the form $V\_q\_r$ in $G_{rough}$, where $V$ is a terminal or a non-terminal, corresponds to the actions of the Scanner.
* Creating new rules from a tentative rule $A\_p\_X\to B\_p\_\cdots C\_q\_Y\cdots$ by finding rules of the form $C\rightarrow\cdots$ in $G_{orig}$ corresponds to the actions of the Predictor.
* Completing a tentative rule of the form $A\_p\_X\to B\_p\_\cdots E\_q\_r$ to $A\_p\_r\to B\_p\_\cdots E\_q\_r$ and then offering it to the Scanner corresponds to the action of the Completer.

This discussion of the intersection Earley parser is based on Section 5 of Albro's paper [222], which has the intersection and the Earley part, but not the parse forest grammar part.

One wonders if the LL(1) and LR(1) methods can also be extended to parse finite-state automata, but to our knowledge no such research has been reported. See Problem 13.7 for some thoughts.

## 13.5 Error Handling Using Intersection Parsing

We have seen (page 13.5) that if we intersect a grammar with an incorrect input sequence (or actually with its FSA) an empty grammar results. From the viewpoint of the algorithm nothing is wrong. The algorithm has dutifully computed the intersection and the intersection happened to be empty.

We have also seen that intersection parsing can be used to do substring parsing, and the idea suggests itself to use this capability to do error handling. This does not work either since unless we first identify a correct substring in the incorrect input (and how could we do that) the intersection algorithm will again produce an empty result.

When we follow the algorithm in what it does on its way to rejecting all rules of the intersection grammar, we see that it collects valuable information bottom-up, but that all that information is deleted because it is not reachable from the start symbol. This is easy to understand: error information can only be collected bottom-up, since the bottom-up process collects correct building blocks; top-down the answer to any incorrect situation is always "No". So the idea suggests itself to stop the cleaning phase _before_ unreachable rules are removed, and extract the error information from that stage. We will call the grammar at that stage $G_{BU}$ for "Bottom-Up".

As an example, we will try to parse the erroneous string **(i+i)+xi** using the grammar from Figure 13.5; this is the same string as we used in Section 13.1.3 with a spurious + inserted. The FSA corresponding to the input string is

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131041491.png)

and the resulting $G_{BU}$ is shown in Figure 13.14. Two useful bits of information can be derived immediately from this grammar. 
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131042353.png)

The first is that the grammar contains all terminals except $+\_6\_7$ and $x\_7\_8$, so these tokens are implicated in the error. The second is that $\bf Expr\_1\_6$, spanning 5 tokens, is the longest identified node in the parsing.

The non-terminal **Expr_1_6** describes the largest correct subtree over the input; it explains the segment (i+i). When we isolate it from the grammar, we can find the next largest, etc. The next one we find is headed by **Expr_8_9**; it explains the final i. No further subtrees can be found; the remaining non-terminal **Expr_4_5** does not lead anywhere.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310131043529.png)
The subtrees are shown in Figure 13.15. It is not immediately clear how this information can be converted into helpful error messages; one possibility is telling the user that the input can be reduced to **Expr** + x


Two things can be noticed here. The first is that a directional parser would have accepted the last + in **(i+i)+xi**, and declared the $\star$ to be the culprit. But from a non-directional point of view -- and intersection parsing is a non-directional method -- there is nothing that says that the $\star$ is more in error than the $\star$. See also Problem 13.8.

The second is that the above technique allows us to identify subsections of FSAs that match subsections of CF grammars. It is not clear what that means, or how or where that can be used.

## 13.6 Conclusion

Parsing as intersection immediately provides polynomial-time algorithms for tasks that would seem problematic otherwise, including substring parsing, managing ambiguity, and large-scale error recovery, all without imposing restrictions on the CF grammar used in the parsing. Although its basic component is old, it is a relatively new and little-studied subject; (Web)Section 18.2.4 holds only 6 references.

In spite of its title, two subjects have been introduced in this chapter: intersection parsing and parsing of finite-state automata. The combination is fortunate, but each is also valuable on its own accord. Intersection parsing allows new insights in and representations of ambiguous parsings, and may play a role in the unification of the present plethora of parsing algorithms. The ease with which the Earley algorithm was derived (Section 13.4) bodes well in this respect. The parsing of FSAs allows the analysis of damaged or incorrect input in a natural way; extension of LL, LR and generalized LR techniques to FSAs would be very useful. Not enough research has been done to fully appreciate the possibilities of Bar-Hillel's long-neglected algorithm.

The results in this chapter have been obtained in an unexpected and surprising way. Especially the determination by the algorithm that the first token in a simple arithmetic expression of which the second one is an open parenthesis (Section 13.2.2) must also be an open parenthesis is astonishing. One wonders where the cleverness and the intelligence is that seemed so necessary in all our previous parsing algorithms. Part of the answer seems to lie in the use of names of non-terminals as multi-way pointers, but it may be too early to find a satisfactory answer to this question.

## Problems

**Problem 13.1**: The use of logic variables in Section 13.1.1 suggests that the intersection of FSAs and CF grammars can be implemented conveniently in Prolog, which has logic variables as a built-in feature. Explore such an implementation.

**Problem 13.2**: Extend Bar-Hillel's intersection algorithm so it can handle $\varepsilon$-transitions in the FSA.

**Problem 13.3**: _Project_ To be honest, the three regular expressions used in Section 13.2.3 to create a CF grammar for the language $\mathbf{a}^{p}\mathbf{b}^{q}\mathbf{c}^{r}$ were chosen carefully to produce a nice grammar. Unfortunately, most other regular expressions one could use for this purpose, for example $\mathbf{[ac]}^{*}\mathbf{[bc]}^{*}$, produce less attractive results. Investigate this phenomenon.

**Problem 13.4**: Turn the sketch of the intersection Earley parser in Section 13.4 into a complete algorithm.

**Problem 13.5**: We could refuse to produce rule 11 in Figure 13.13 on the grounds that there is no $b\_3\_k$  This corresponds to prediction look-ahead in the Earley parser, as described in Section 7.2.4.1. Incorporate this optimization in the result of Problem 13.4.

Problem 13.6: Project: The Earley intersection parser creates (as frugally as possible) the rules from the top down, but one could also create them bottom-up. Start with all terminals of the form $t_{-} p_{-} q$, find all rules that contain t, and instantiate as many positions in the non-terminals as possible. On the basis of these halfconstructed rules, create more rules. etc. Turn this into an algorithm.

**Problem 13.7**: Research Project: (a) How can the LL(1) parsing method be adapted to parse finite-state automata rather than strings? Hint: The big question is of course what to do about the prediction stack. It seems possible to construct, for each state p of the FSA F to be parsed, an FSA $G_{p}$ which describes all the prediction stacks that any string produced by F could encounter in state p, either by specializing the general FSA for the prediction stack (Section 5.1.1) or by propagation and transitive closure. (b) Same question for LR(1). (c) Same question for Generalized LR.

**Problem 13.8**: Error detection traditionally requires the determination of the longest grammatically acceptable prefix of the input string. Consider an algorithm that determines that prefix by binary search, using the intersection algorithm as a test of acceptability. What is the complexity of this algorithm? Once the prefix has been removed, can a similar algorithm be used to determine further correct substrings?