# Chapter 14. Parallel Parsing

There are two main reasons for doing parallel programming: the problem has inherent parallelism, in which case the parallel programming comes naturally; and the problem is inherently sequential but we need the speed-up promised by parallel processing, in which case the parallel programming is often a struggle.

Parsing does not fall in either of these categories. It has no obvious or inherent parallelism and on present-day machines and using state-of-the-art techniques it is already very fast. A 250-line module in a computer program and a 25-word sentence in a natural language can both be parsed in a fraction of a second. In parallel parsing processing is performed on multiple processors, which either have a shared memory or are interconnected by means of a (high speed) network. Given the communication overhead in the most common parallel systems, little speed-up can be expected for the average parsing task.

## 14.1 The Reasons for Parallel Parsing

From a practical point of view, parallel parsing is interesting only for problems big enough to require considerably more time than a fraction of a second on a single processor. There are three ways in which a parsing problem can be this big: the input is very long (millions of tokens); the grammar is very large (millions of rules); or there are millions of inputs to be parsed. The last problem can be solved trivially by distributing the inputs over multiple processors, where each processor processes a different input and runs an ordinary, sequential, parser.

Examples of very long inputs requiring parsing are hard to find. All very long parsable sequences occurring in practice are likely to be regular: generating very long **CF** sequences would require a place to store the nesting information during sentence generation. The boundaries are not very clear, though, since a novel can be considered a very long parsable sequence of words. Upon a closer look, this sequence, however, is either regular - just a list of chapters which are lists of paragraphs which are lists of sentences, which in themselves are **CF - or** it is context-sensitive - a coherent sequence of words, punctuation, etc., with strong context dependencies, forexample keeping track of the color of the heroine's eyes. This suggests that parallel context-sensitive parsing might be useful, but no research on the subject is known.

The situation is different for parsing with very large grammars. These are found most often in linguistics. They are especially bothersome there since most linguistic applications require general CF parsing techniques, the speed of which depends on the grammar size. This dependency is quadratic ($O(|G|^{2})$, where $|G|$ is the size of the grammar), or, for more advanced algorithms, linear ($O(|G|)$). General CF parsing has a practical upper bound of $O(n^{3})$, where $n$ is the length of the sentence, but if the grammar is very large and the sentence to be parsed is short, grammar size may be much more significant then input length. Little explicit research on parsing with very large grammars is known, but many parallel parsing techniques can be exploited to allow very large grammars, as we shall see.

This brings us to two further reasons to study parallelism: scientific curiosity and theoretical investigations. It is well known that trying to parallelize an otherwise sequential program often leads to deeper insight, clever techniques, and sometimes to improved infrastructure. The theoretical investigations are concerned with the inherent complexity of parsing: Can parsing be done in linear time ($O(n)$)? (Yes, easily, using $O(n^{3})$ processors; see Section 14.4.2.1.) Can parsing be done in logarithmic time ($O(\ln n)$)? (Yes, with difficulty, using $O(n^{6})$ processors; see Sections 14.4.3.1 and 14.4.3.3.) Can parsing be done in double-logarithmic time ($O(\ln\ln n)$)? (We don't know.)

It is because of these considerations and questions that researchers have given parallel parsing a lot of attention, so much so that it is impossible to describe all this research in a single chapter. Rather, we will describe some of the directions this research has taken.

Three main methods of parallelizing the parsing process have been developed: multiple serial parsers, process-configuration parsers, and connectionist parsers. These correspond closely to three main streams in the use of multiple processors: parallel programming, in which most of the processors perform the same program with different data sets; distributed programming, in which most of the processors perform autonomous cooperating programs; and hardware parallelism, in which large numbers of processors consisting of special hardware perform simple actions.

In this chapter we will discuss an example of each of the parsing methods.

## 14.2 Multiple Serial Parsers

A multiple serial parsers is a parallel parser in which each processor runs a sequential parser on a part of the input, which is split up and divided among the processors. A typical example of a multiple serial parser is Fischer's algorithm [223]. Fischer developed a mechanism that can be applied to several sequential parsing techniques. We will discuss the LR(0) version here.

Each Fischer parser has a number of incomplete LR(0) stacks, each incomplete stack being the head (top segment) of a possible actual stack. $N$ such parsers are started at $N$ essentially arbitrary points in the input sequence (which must be known in its entirety); one parser starts at the beginning. The latter starts with one stack head, containing the initial state, so it certainly starts with a correct and complete LR(0) stack. The stack head sets of the other parsers are determined by the first token they look at. If a certain parser looks at a token $a$, it gets one stack head for each state in the LR(0) table that allows a shift on $a$, and that stack head contains that state. Each parser proceeds as follows: It considers each active state in the stack head set; some will indicate a shift, some a reduce and some will indicate an error. The stack heads indicating an error are discarded. Now, five cases are distinguished:

* All active states (the states on top of the stack heads) indicate a shift: the action is done for each stack head and the parser proceeds.
* All active states indicate a reduction for which the corresponding stack head is deep enough: the action is done for each stack head and the parser proceeds. Note that the stack head of the first parser will always be deep enough.
* All active states indicate a reduction, but none of the stack heads are deep enough to allow the reduction. In this case, the parser suspends itself, it has to wait for its left neighbor.
* There are no stack heads left; all stack heads were discarded because they indicated an error. In this case, there actually was an error.
* If none of the above applies, the parser splits the stack head set up in such a way that one of the above cases applies to each part. Next, a number of further parsers are started, so that there is a parser for each of these parts of the stack head set.

When a parser $P$ runs into a token that has already been processed by a subsequent parser $Q$, $P$ waits until $Q$ gets suspended (or finishes); it then combines the results. If $Q$ was split, $P$ must wait until all parsers that resulted from this split (and further splits) are either finished or suspended. We will discuss this combination of results in more detail in the example below. Parser $P$ may then be able to continue, or it may have to wait again or be suspended. The first parser will never get suspended (though it may have to wait) and will ultimately finish the job.

Now let us examine how this works with the example of Figure 9.18, with input $\mathbf{n-n- (n-n) \$} $. Let us assume that we cut the input in two pieces of equal length: $\mathbf{n-n-(}$ and  $\mathbf{n-n)\$} $ . Figure 14.1 presents the steps that the first parser takes. It starts

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071722559.png)

with one stack head containing the initial state, state ⓵. It ends up waiting for the second parser, in state ⓺.
To determine the stack heads of the second parser, we have to consider the first token: $\mathbf{n}$. There are three states that have a shift on $\mathbf{n}$ : states ⓵, ⓺, and ⓻, so our set of stack heads consists of these states. The first few steps of the second parser are presented below.

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071722326.png)

The first two steps above actually represent three stack heads but are combined into one line. This is possible as long as the only difference is the state on the bottom of the stacks. Now, state ② requires a reduction for which its stack head is deep enough, but state ⑧ requires a reduction for which it is not, so a split is required. The second part of the split is suspended immediately, so we will now examine what happens with the first part.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072050202.png)

So, now all splits are either suspended or finished (because of an error), and the suspended stack heads are:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072050255.png)

These results can now be combined with the result of the first Fischer parser. The state set at the top of the stack of the first Fischer parser (6) has no states in common with the bottom state of the second suspended parser (7). Therefore, this is a dead end, and is discarded. Combining the result of the first suspended parser with the result of the first Fischer parser, we get:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072051103.png)

As we can see from this example, the second part of the input results in a lot of work that later turns out to be without merit, but at least the second part could mostly be processed independently of the first part. However, unfortunate splits are possible. For instance, if an expression is split up right in front of a ), the parser processing the part that starts with a ) can only shift it and then suspend, leaving all the work to its left neighbor, and making the process virtually sequential. So, it is important to split the input in suitable places. There are two ways in which this may be achieved:

* We may split up at terminal symbols that have only a few states with a shift on that symbol. This keeps the number of stack heads small. This may not always be a good heuristic though, see our example of the ), for which only state (r) has a shift.
* We may also split up in such a way that it is unlikely that the parser will have to suspend itself because it meets a reduce for which its stack is insufficient. For instance, a split right in front of a ( might be a good heuristic, because the parser processing that part can then at least process its input up until the corresponding closing parenthesis. Parallel bracket matching techniques can be used to find suitable places to split up. See, for instance, Bar-On and Vishkin [225] or Srikant [230].

The time we have to wait for the answer consists of two components: the time required by the $N$ parsers, each working in parallel on $n/N$ of the input ($O(n/N)$) and the time required by the $N$ processes to communicate their findings ($O(N)$); together this is $O(n/N)+O(N)$. This shows that when $n$ is large with respect to $N$, adding processors helps, but also that after a certain number of processors the $O(N)$ terms will start to dominate and adding further processors will be detrimental. Since the grammar is incorporated in the LR(0) table the speed does not depend on the grammar, but the technique works for LR(0) languages only.

It is interesting to note that the behavior the start-up phase of each parser except the first one is very similar to that of the GLR suffix parser of Bates and Lavie [214] discussed in Section 12.3.2.1. In fact, the Fischer parser parses 1 prefix and $N-1$ substrings, and combines the result.

A (simpler) finite-state variant of Fischer's method can also be used to construct a parallel lexical analyser: each of the $N$ finite-state automata starts in all states, except the first one, which starts in the initial state. Only a few of these states will survive until the end of the chunk that is processed by each automaton. Only these survivors are available for combination into the complete list of resulting tokens.

## 14.3 Process-Configuration Parsers

A *process-configuration parser* assigns an agent, a process, to each task in the parser. They are also known as *agent parsers*. Agents are autonomous processes whose actions are triggered by messages: when an agent receives a message, this triggers some computation, after which the agent may send messages to other agents. In an LR parser, for instance, an agent could process a particular state in the LR automaton. State transitions are modeled as messages from one agent to another, so that a message contains enough information for the agent to continue. See, for instance, Hendrickson [235].

### 14.3.1 A Parallel Bottom-up GLR Parser

An interesting example of a process-configuration parser is presented by Sikkel and Lankhorst [233]. Their "PBT" (Parallel Bottom-up Tomita) parser allocates an agent to each position in the input, one in front of each input symbol, and one at the end. Each agent yields the constituents that start at its own position. The algorithm works with any CF grammar, including ones with $\epsilon$-rules (hence the agent at the end of the input). The left-to-right restriction of GLR is abandoned, which allows each agent to start parsing at its own word, in parallel. For convenience, an end marker (#) is added at the end of the input, so that the last agent has a symbol to work with as well.

We will use the grammar of Figure 7.8 and input **a-a+a#** as a running example, where we have added a rule **S' $\rightarrow$S#** to the grammar, and the end marker to the input. As parsing proceeds, an agent $P_{k}$ will send items that it has found to its left neighbor $P_{k-1}$. It will also send any items it receives from its right neighbor on to its left neighbor. These items all have the form $(i,X,j)$, where $X$ is either a terminal or a non-terminal, and which indicates that $X\smash{\stackrel{{*}}{{\rightarrow}}}a_{i}\cdots a_{j-1}$.

For the example, the agents are driven by the parse table of Figure 14.2. Its construction will be discussed later. This example table has a shift/reduce conflict in state 2, but conflicts are allowed. The agent tries all possibilities. All agents are started in state 1.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072053234.png)

Let us consider agent $P_{5}$, the one dealing with position 5 in the input, in detail. It sees the symbol **a**, so it sends the item (5,a,6) to its left neighbor, $P_{4}$. Then, it consults the parse table to determine what to do next. The parse table tells it to

Figure 14.2: The PBT parse table for the grammar of Figure 7.8

shift to state 6, which prescribes a reduce using rule **F$\rightarrow$a**. This means that an **F** is recognized, so the agent sends the item **(5,F,6)** to $P_{4}$. When applying a reduce, the parser keeps the old stack around, because it may still be needed. It uses stack duplication, with combined prefixes, as described in Section 11.1.2.3. After the reduce, the parser is back in state 1, and sees an **F**. The parse table instructs the parser to shift to state 5, which again prescribes a reduce, now using rule **E$\rightarrow$F**. So, an **B** is recognized, and the item **(5,E,6)** is sent to $P_{4}$. Another shift, another reduce, a new item **(5,S,6)** sent to $P_{4}$, and another shift bring $P_{5}$ into state 9. Somewhere along the way, agent $P_{6}$ has discovered the # and sent the item **(6,#,7)** to $P_{5}$. Agent $P_{5}$ now forwards this item to $P_{4}$, and also determines that the sentence starting at position 5 is accepted. The final stack of $P_{5}$ is depicted in Figure 14.3.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072054046.png)

The reader is invited to verify that $P_{4}$ derives the item **(4,Q,5)**. It also sends the items found by $P_{5}$ through to $P_{3}$. Now, let us look at $P_{3}$ in more detail. After its local processing of its own symbol, its stack looks very much like the one from Figure 14.3, with some positional differences. Next, it receives the **(4,Q,5)** item from $P_{4}$. $P_{3}$ has a stack at position 4, in state 2, in which a **Q** can be shifted. The resulting stack is shown in Figure 14.4.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072054517.png)

Next, it receives the item **(5,F,6)** and this **F** can now be shifted. There is a subtle issue here concerning the order of the messages: before passing on items that start at a position $k$, an agent must send all items that end right in front of this position and that pass through this agent. Shifting the **F** has brought $P_{3}$ into state 4, which prescribes a reduce using rule **E$\rightarrow$EQF**. So, $P_{3}$ has discovered the item **(3,E,6)**.

Continuing, it also discovers **(3,S,6)** and also discovers that the sentence starting at position 3 is accepted. The final stack of $P_{3}$ is shown in Figure 14.5.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072055643.png)

Like agent $P_{4}$, agent $P_{2}$ only derives the item **(2,Q,3)**, so we finally turn our attention to $P_{1}$. Initially, $P_{1}$ develops its stack quite similarly to $P_{3}$. However, where $P_{3}$ finishes, $P_{1}$ can continue. It has a stack head in state 2 after processing 3 symbols. When it receives the item **(4,Q,5)** from $P_{2}$, shifting the Q brings it into state 3, and when it then receives **(5,F,6)**, shifting the F brings it into state 4, which, as we know by now, prescribes a reduce using rule **E$\rightarrow$EQF**. So, $P_{1}$ has discovered the item **(1,E,6)**. It also discovers **(1,S,6)**, ends up in state 9, where **(6,#,7)** causes $P_{1}$ to accept the sentence. See Figure 14.6.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072055741.png)

Some observations should be made here:

* All states on the stack are active, not just the ones on top of a stack head. Each state can start a branch when an item arrives that can be shifted in this state.
* The order in which the items are sent is important. When an agent $P_{i}$ tries to fit an item $(j,X,k)$ on any one of the states on its stack, all items $(l,Y,m)$ with $i\leq l\leq m\leq j$ must have been processed, which is a bit of a problem when $l=m=j=k$. Nullable symbols have to be retried.
* Although the stack of each agent is in fact tree structured, with common prefixes, the complete conceptual picture of the stack is graph structured: every state transition labeled $(i,X,j)$ implicitly refers to agent $P_{i}$ which has the details for $(i,X,j)$.
* The number of items sent from one agent to another can be reduced considerably. For instance, if a symbol (either terminal or non-terminal) $X$ only occurs as a first symbol in the right-hand sides of the productions, and agent $P_{i}$ has discovered the item $(i,X,j)$, this is only useful information for agent $P_{i}$. Also, if $P_{i}$ discovers $(i+1,Y,j)$, then this information is only useful for other agents if the combination $a_{i}Y$ occurs in a right-hand side but not at the beginning, or a combination $AY$ occurs in a right-hand side and produces a string ending with $a_{i}$.

Generation of the parse table is particularly easy. It is similar to the LR(0) parse table generation, but there are some differences: since each agent is initially ready to recognize any non-terminal, the initial state contains all items $A{\rightarrow}\bullet\alpha$ for all grammar rules $A{\rightarrow}\alpha$ in the grammar. Next, for each symbol $X$ after a dot, a new state is defined with the items that have the dot after the $X$. The resulting automaton is deterministic by the way it is constructed (but it may contain shift/reduce conflicts). For our example, the automaton is shown in Figure 14.7.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072056910.png)

To obtain a parse-forest grammar, each agent must make a grammar rule for each reduction it finds. For instance, when agent $P_{i}$ finds that it can reduce $(i,Y,j)$ and $(j,Z,k)$ to $(i,X,k)$, it adds a rule $X\_{i}\_{k}{\rightarrow}Y\_{i}\_{j}Z\_{j}\_{k}$. It can be sure that if $Y$ is a non-terminal, it has rules for $Y\_{i}\_{j}$, because it has made a reduction to it at some time. Likewise, if $Z$ is a non-terminal, agent $P_{j}$ has made a reduction to $Z\_{j}\_{k}$ and thus has one or more grammar rules for it. At the end of the parsing process, we have a parse-forest grammar, but it is distributed over the agents. One way of dealing with this is to send the parse-forest grammar rules with the items. This has the disadvantage that items may now have to be sent several times (with different grammar rules).

Sikkel and Lankhorst [233] do not give a theoretical derivation of time and communication requirements of their algorithm. Rather, they did a practical performance evaluation that showed that for large sentences, their algorithm actually gave a speedup of about $O(\sqrt{n})$ when using $n$ processors with respect to a sequential Tomita parser.

### 14.3.2 Some Other Process-Configuration Parsers

Yonezawa and Ohsawa [229] describe an agent parser where there is one agent for each grammar rule. The agent for the rule $N\to ABC$ receives messages from all agents that manage rules for $A$, $B$ and $C$ and sends messages to all agents for rules of the form $P\to\alpha N\beta$. Each message is a parse tree for a chunk of the input, including its position in the input and its length. The agent waits for chunks of the right non-terminal and of the right position and length, combines them into a new parse tree and sends it to the interested agents. In the end, the agent processing the start symbol delivers all parse trees. The parser does not allow $\varepsilon$-rules or circularities.

Ra and Kim [236] describe what they call an Earley-based parser, which looks like an Earley parser without the top-down component, i.e., a bottom-up Earley parser, but which is actually a bottom-up left-corner parser (see Section 7.3.4 and Sikkel [158]). Their parser too does not allow $\varepsilon$-rules or circularities. It bears resemblance to the PBT parser of Section 14.3.1 in that it has an agent for each position in the input, and sends the completed items that it finds to its left neighbor. However, it does not use a parse table. Instead, each agent maintains a set of dotted items, which have the format $(i,A{\to}\alpha\bullet\beta,j)$; this means that we have recognized an $\alpha$ in positions $i$ up to (but not including) $j$ and are looking forward to recognizing a $\beta$ starting at position $j$. Each agent $P_{i}$ initializes its item set with all items $(i,A{\to}\bullet\alpha,i)$ for all grammar rules $A{\to}\alpha$. Next, each agent $P_{i}$ performs the following steps, repeatedly.

* A "scanner" step: if the agent has an item $(i,A{\to}\alpha\bullet a\beta,j)$ and $a=a_{j}$, it adds the item $(i,A{\to}\alpha a\bullet\beta,j+1)$.
* A "starter" step: for each item $(i,A{\to}\alpha\bullet,j)$ (with the dot at the end of the rule, which means that the rule has been recognized completely) and each grammar rule $B{\to}A\beta$, the item $(i,B{\to}A\bullet\beta,j)$ is added. This corresponds to the left-corner inference rule of Section 7.3.4. As $\beta$ can be $\varepsilon$, in which case a new completely recognized item has been found, this step has to be performed repeatedly until no new items are added.
* An "extender" step: if the agent has an item $(i,A{\to}\alpha\bullet B\beta,r)$ for some $r$ and a received message contains the item $(r,B,j)$, the agent adds the item $(i,A{\to}\alpha B\bullet\beta,j)$ to its item set.

Initially, the agents process their own itemset, until no new items can be added. Then, each received item may trigger new additions. Each new addition that represents a completed item is passed on to the left neighbor, as are all items that are received from the right neighbor. Conceptually, the process can be divided into stages: after stage $k$ (in which agent $P_{i}$ computes "its" items of length $k$), $P_{i+1}$ sends its own completed items of length $k$, the completed items from $P_{i+2}$ of length $k-1$, the completed items from $P_{i+3}$ of length $k-2$, et cetera. So, messages may become bigger with $k$, but the number of messages decreases: after the initialization, agent $P_{n}$ will never receive messages, agent $P_{n-1}$ will only receive one message, et cetera. In the end, when agent $P_{1}$ has processed $n-1$ messages, it has computed its items of length $n$. If this item set now contains the item $(1, S \rightarrow \alpha \bullet, n+1)$, where $S$ is the start symbol of the grammar, the sentence is recognized.

Ra and Kim [236] present a time requirement analysis, and conclude that the worst-case performance is $O(n^{3}/p)$ on $p$ processors. It should be noted, however, that there is also a dependency on the size of the grammar that is at least quadratic.

We can see that the above algorithms are all parallel ways of filling the items table of Figure 7.13.

## 14.4 Connectionist Parsers

A connectionist parser is a parser that runs on a connectionist network. In general, a *connectionist network* is a network of nodes connected by unidirectional lines which each carry a value, the level of activation, which is determined by the node the line emanates from. Each node continually examines the activity levels on its input lines and computes from them the activity level on its output line(s).

To allow for reasoning about time, we split time up into discrete steps, and assume that it takes one time step to compute the activity level on the output lines from the activity levels on the input lines. In other words, the activity level on the output lines at time $t$ is computed from the activity level on the input lines at time $t-1$.

### 14.4.1 Boolean Circuits

The simplest connectionist network is a *Boolean circuit*, which is a directed graph where the nodes of the graph correspond to the nodes in the connectionist network, and the (directed) edges of the graph correspond to the connections in the network. In a Boolean circuit, there are only two activation levels: "on" and "off", and there are only two types of nodes: "OR-nodes" and "AND-nodes". Each node has an arbitrary number of inputs and an arbitrary number of outputs (the outputs all carry the same value). An OR-node determines its output value as follows: if any of its input values is "on", its output value will be "on", otherwise it will be "off". An AND-node determines its output value as follows: if any of its input values is "off", its output value will be "off", otherwise it will be "on". We say that a node is "off" when its output value is "off", and it is "on" when its output value is "on". Usually, Boolean circuits may have NOT-nodes as well. A NOT-node is a node with a single input and a single output, where if the input is "on", the output is "off", and vice versa. However, we do not need NOT-nodes for the discussion below.

The circuit is started by setting the initial activation level for some of the nodes to "on". At each time step, the outputs of a node are computed as the result of the node computation with the input values of the previous time step. This means that if there are no changes in a single time step, the input values of the next time step are the same, and thus the output values are too, so the system is stabilized. This happens after a number of time steps, because once a node is "on", it will never be "off" again. When the circuit has no cycles, the number of time steps required to propagate the initial values through the whole graph is equal to the number of connections in the longest path from any initial node.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072059328.png)

Figure 14.8 presents a small example of a Boolean circuit, with an AND-node computing the result of $a$ AND $b$, and an OR-node with inputs from the AND-node and $c$. The result $d$ is "on" if either $c$ is "on" or both $a$ and $b$ are "on". The circuit takes two timesteps to stabilize. When a Boolean circuit has NOT-nodes, it is possible for such a circuit to never stabilize.

### 14.4.2 A CYK Recognizer on a Boolean Circuit

A CYK recognizer is particularly easy to implement with a Boolean circuit, as long as we limit the length of the sentence to some upperbound. Suppose we want to build a CYK recognizer for a grammar $G$ and input sentences with length at most $N$. As we did in Section 4.2.2, we assume that the grammar is in Chomsky Normal Form. [^1]

[^1]:The Boolean circuit implementation presented here is by Sikkel [158].

The idea is that we assign a node to every possible hypothesis "non-terminal $A$ derives substring $s_{i,l}$, starting at position $i$ and with length $l$". We will call this node $A_{i,l}$. If this hypothesis is realized, $A$ is a member of the set $R_{i,l}$, as discussed in Section 4.2.2. So, each set $R_{i,l}$ has a node for every non-terminal, and its output is "on" if the non-terminal is a member of the set, and "off" otherwise. Note that in total there are $O(n^{2}|V_{N}|)$ of these hypotheses (and nodes), where $|V_{N}|$ stands for the number of non-terminals.

Next, we will determine what kind of nodes these are, and what their inputs are. For a non-terminal $A$ to derive a substring $s_{i,l}$, there must be at least one rule $A\to BC$ that derives this substring.

For a rule $A\to BC$ to derive a substring $s_{i,l}$, the right-hand side $BC$ must derive this substring. This means that for some $k$, $B$ must derive $s_{i,k}$ and $C$ must derive $s_{i+k,l-k}$. So, if we have a node $B_{i,k}$ that represents the hypothesis that $B$ derives $s_{i,k}$ and we have a node $C_{i+k,l-k}$ that represents the hypothesis that $C$ derives $s_{i+k,l-k}$, the combined hypothesis can be represented by an AND-node $A\to BC_{i,k,l}$ with two inputs: the outputs of nodes $B_{i,k}$ and $C_{i+k,l-k}$. For every length $k$ between $1$ and $l-1$ we create such a node. Now, we see that node $A_{i,l}$ will be an OR-node with inputs from all nodes $A\to BC_{i,k,l}$.

This tells us how to deal with rules of type $A\to BC$, and describes the main engine, but we still have to initialize and start it. For that purpose, we have AND-nodes $a_{i}$ for all terminals $a$, for all positions $i$. These nodes have two inputs, one that is used to start the circuit (when it is turned "on" by a switch or so), and one that is "on" if, in fact, $s_{i,1}$ is $a$. We also have AND-nodes $A\to a_{i}$ for all production rules $A\to a$, for all positions $i$, which get their one input from the AND-node $a_{i}$. This node is turned on when the symbol in position $i$ is, in fact, $a$. Finally, we also have OR-nodes $A_{i,1}$ for all non-terminals $A$ and all positions $i$. These nodes represent the hypotheses that the non-terminal $A$ derives the substring $s_{i,1}$. Their inputs are the outputs of all nodes $A\to a_{i}$.

In the end, when the circuit is stabilized, we look at the node representing the hypothesis $S_{1,n}$, that is, the hypothesis that the start symbol $S$ derives the input sentence $s_{1,n}$. When this node is "on", the sentence is recognized, when it is "off", it is not. With the circuit constructed above we also can recognize sentences of length less than the maximum length $N$. Like we saw with a CYK parser, where a sentence $s_{1,k}$ is recognized when $S$ is a member of $R_{1,k}$, in the Boolean circuit, the node $S_{1,k}$ will be turned "on".

Time for an example! We will use grammar 14.9 as an example to build a Boolean circuit from. We will build a recognizer for this language for strings with length at most 3, because things get out of hand rather quickly.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072104170.png)

Figure 14.10 presents the Boolean circuit recognizer for the grammar of Figure 14.9. The circuit is started by flipping the switch at the bottom right hand side of the picture. The arrows leaving the picture at the left are the lines to check when the circuit is stabilized. When $S_{1,1}$ is "on", a sentence of length 1 is recognized, et cetera. The elements of the CYK recognition table $R_{i,j}$ are represented by the dotted boxes.

We will follow the time steps on recognizing the input sentence $\mathbf{acb}$, which is represented by setting the left inputs of nodes $a_{1}$, $c_{2}$, and $b_{3}$ to "on". Now, when we flip the switch, the right inputs of nodes $a_{1}$, $c_{2}$, and $b_{3}$ are also "on". Therefore, one timestep after flipping the switch, the inputs of $A\to a_{1}$, $S\to c_{2}$, and $B \rightarrow b_{3}$ are "on", and one timestep later the inputs of $A_{1,1},S_{2,1}$, and $B_{3,1}$ are "on". After the third timestep, the inputs of $S\to AS_{1,1,2}$ and $D\to AS_{1,1,2}$ are "on". This, in turn, will set the inputs of $D_{1,2}$ and $S_{1,2}$ "on" at the next timestep. So, the sentence $\mathbf{ac}$ is recognized, but we are not done yet. Both inputs of $S\to DB_{1,2,3}$ are now "on", so its output will be "on" at the next timestep, and $S_{1,3}$ will be "on" next, which means that the sentence is recognized.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072104017.png)

As can be seen from Figure 14.10, the circuit as constructed above contains many nodes that can never be activated. Its size can be reduced using *metaparsing*: we let the circuit try and recognize any input sentence of length $\leq N$ by turning on all nodes $a_{i}$. Any node not turned "on"eventually by this input can be discarded, because if it is not turned "on" now, it will not be turned on, ever. This results in the recognizer as presented in Figure 14.11.

Next we mark all nodes reachable by a *reversed scan*, starting with nodes $S_{1,k}$ and following all connections in the reversed direction. Any intermediate node not marked in this way can be removed, because there is no path from such a node to a node $S_{1,k}$, see Figure 14.12.

#### 14.4.2.1 Time and Node Requirements

Now let us examine the time requirements for such a Boolean circuit, i.e., how many time steps it takes for the circuit to stabilize. From the description above, it appears that there are no cycles in the resulting circuit, and getting from nodes associated with length $l$ to nodes associated with length $l+1$ takes two time steps; so getting from nodes associated with length $1$ to nodes associated with length $n$ takes $2n$ time steps. In other words, we need $O(n)$ time steps for the system to stabilize; note that this time is independent of the grammar size. This is a major improvement over the $O(n^{3})$ that we saw in Section 4.2.2. However, this improvement comes with considerable costs: the number of nodes needed.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072107959.png)

Obviously, the number of nodes needed depends on the maximum length of the input sentence. There are $n(n+1)/2$ sets $R_{i,l}$, and for each of these sets, we need:

* an OR-node for each non-terminal. The number of such nodes within one $R_{i,l}$ set thus depends linearly on the number of non-terminals $|V_{N}|$ in the grammar;
* AND-nodes $C\to AB_{i,k,l}$ for all rules $C\to AB$ in the grammar and for all $1\leq k<l$ (when $l>1$), or AND-nodes $A\to a_{i}$ for all grammar rules $A\to a$ when $l=1$. The number of such nodes depends not only linearly on the number of rules $|P|$ in the grammar, but also linearly on $l$, so ultimately also on the maximum length of the input sentence.

Thus, considering the number of non-terminals, the number of grammar rules, and the number of terminals constant, the number of nodes needed for a single set $R_{i,l}$ is $O(n|G|)$ where $|G|$ is the size of the grammar, so the total number of nodes needed for $R$ is $O(n^{3}|G|)$. In addition, we need $n|\Sigma|$ AND-nodes, where $|\Sigma|$ stands for the number of different terminals ($\Sigma$ is the set of terminal symbols). This number is $O(n|G|)$, so the total number of nodes needed is $O(n^{3}|G|)$ for parsing in time $O(n)$. We see that although the time requirement is independent of $|G|$, $|G|$ reappears in the node requirements. Unlike Fischer's parser, adding more processors to a connectionist parser to speed it up is not an option: $G$ and $n$ together determine exactly the number of processors the parser needs.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072107115.png)

Chang, Ibarra and Palis [228] show that a linear-time CYK parser can be implemented on $O(n^{2})$ processors, a two-dimensional array of finite-state machines.

It is interesting that the above connectionist parser can be seen as a process-configuration parser in which the connections are hard-wired.

#### 14.4.2.2 Creating a Parse Forest

One might think that the circuit as presented above, once it is stabilized, represents a parse forest, but in general this is not the case: outputs and nodes can be turned "on" but not participate in a valid parsing. This corresponds to the introduction of non-reachable rules when getting a parse-forest grammar from a CYK recognition table, as we have seen in Section 4.2.8. There, we had to remove non-reachable rules from the parse-forest grammar. In Boolean circuit terms, this means that we have to make sure that a node that is turned "on" contributes to the ultimate turning "on" of $S_{1,N}$.

On the other hand, all nodes that are left in the circuit after the reversed scan could represent nodes in a parse forest. The problem is that we need a top-down scan to determine which nodes should actually be turned "on". This requires the value of node $S_{1,N}$ as input, which means that we have to build additional circuitry for the parse forest. We basically need additional circuitry that determines which nodes actually are part of the parse forest, and which nodes are non-reachable for the input at hand. The additional circuitry will have a node $X^{\prime}$ for every node $X$ in the recognizing circuit. $X^{\prime}$ will be an OR-node if $X$ is, and an AND-node if $X$ is. $X^{\prime}$ will be turned "on" if, and only if, it is part of the parse forest. The difference with the recognizing circuitry lies in the wiring (the connections), so we will now concentrate on that.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072108079.png)

Starting with the top level, as we would do when marking the reachable rules in the parse-forest grammar, where we would mark $S_{-}1_{-}N$ reachable, the node $S^{\prime}_{1,N}$, an OR-node, gets a single input: the output of $S_{1,N}$. This can be considered a switch to turn on the parse-forest circuitry. Next, we turn our attention to all nodes $C\to AB_{i,j,k}$ which determine the inputs of node $C_{i,k}$. Node $C\to AB^{\prime}_{i,j,k}$ should be part of the parse forest if $C\to AB_{i,j,k}$ is "on" (obviously), and $C^{\prime}_{i,k}$ is "on" (which makes sure that it is actually part of the parse forest). So, $C\to AB^{\prime}_{i,j,k}$ is an AND-node with two inputs: $C\to AB_{i,j,k}$ and $C^{\prime}_{i,k}$. It has two outputs: one to node $A^{\prime}_{i,j}$ and one to $B^{\prime}_{i+j,k-j}$. This corresponds to marking $A\_i\_j$ and $B\_i+j\_k-j$ reachable when $C\_i\_k$ is and the rule is applicable for the input at hand (see Section 4.2.8).

In addition, for nodes $A^{\prime}_{i,1}$ we add an output to node $A\to a^{\prime}_{i}$, which is an AND-node with two inputs, for all rules $A\to a$. The second input to node $A\to a^{\prime}_{i}$ comes from the output of node $A\to a_{i}$.

The complete circuit for our example grammar and strings of length 3 is presented in Figure 13. To keep the figure readable, we have removed the nodes that do not contribute to the recognizing of strings of length 3 (again, using a reversed scan).

### 14.4.3 Rytter's Algorithm

Many problems that require linear or more time on a sequential processor can be solved in $O(\log\ n)$ time on a parallel machine, so the question arises if parallel parsing in $O(\log\ n)$ is possible. Rytter [226, 227] invented a clever recognition algorithm that can be executed in $O(\log\ n)$ time. We will first describe the building blocks of the algorithm, then describe the algorithm itself, and then show how to build a Boolean circuit for it. We will use the grammar of Figure 13 as a running example, with input sentence **aacb**. Virtually the same algorithm was invented by Brent and Goldschlager [224]. In fact, their paper was published earlier.

In this discussion, we will use the version of Brent and Goldschlager, but call the algorithm Rytter's algorithm, for that is the name it goes by.

Like the CYK algorithm, Rytter's algorithm requires the grammar to be in Chomsky Normal Form. Also like CYK, Rytter's algorithm maintains a set of hypotheses that have been realized. Like CYK, this set is initialized with all hypotheses of the form "non-terminal $A$ derives substring $s_{i,1}$", where $A\to s_{i,1}$ is a rule of the grammar, and $s=s_{1}\cdots s_{n}$ is the input sentence, and $s_{i,1}$ is the substring starting at position $i$, of length 1. We will denote CYK hypotheses with a triple $(A,i,j)$, meaning that the hypothesis is: "non-terminal $A$ derives substring $s_{i+1}\cdots s_{j}$". Note that we have switched from length to index, which is more convenient for the following description. We will call the set of realized CYK hypotheses $S_{CYK}$.

For our example, initially, $S_{CYK}$ consists of the following elements:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072110429.png)

Now, in the CYK algorithm, if there is a grammar rule $C\to AB$, and we have two realized CYK hypotheses "$(A,i,j)$" and "$(B,j,k)$", the CYK hypothesis "$(C,i,k)$" will be realized. In this way, the CYK algorithm maintains a set of realized CYK hypotheses. The Rytter algorithm, in addition, maintains a set of what we will call *Rytter_proposals*. The general form of a Rytter proposal consists of two CYK hypotheses, denoted as follows: $(A,i,j;B,o,p)$, and it is realizable if $A\smash{\stackrel{{\ast}}{{\rightarrow}}}s_{i+1}\cdots s_{o}Bs_{p+1} \cdots s_{j}$. We will call a Rytter proposal realized if we have determined that it is, in fact, realizable.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072110336.png)

In fact, a Rytter proposal looks like a CYK hypothesis with a gap. The gap is speculative: if this Rytter proposal is realized, and the CYK hypothesis $(B,o,p)$ is realized, then $(A,i,j)$ will be realized. This is just logic: if $U$ implies $V$, and $U$ is true, then $V$ is true.

The first building block of the algorithm is what we call the "propose phase". Based on the grammar rules and the current set of CYK hypotheses, a Rytter proposal is constructed as follows: if we have a grammar rule $C\to AB$ and $S_{CYK}$ contains a hypothesis $(A,i,j)$, then we know that if the CYK hypothesis $(B,j,k)$ is realized, the hypothesis $(C,i,k)$ will also be realized. We can formulate this as a speculation: $(C,i,k)$ could be realized, in the case that $(B,j,k)$ turns out to be realized. This is the Rytter proposal $(C,i,k;B,j,k)$, which now turns out to be realized. It is depicted below, where the speculative part is indicated by dashed lines. Such a speculation can be made for any $k$ such that $j<k\leq n$.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072111036.png)

Likewise, with the same grammar rule, if $S_{CYK}$ contains a hypothesis $(B,j,k)$, then we know that if the CYK hypothesis $(A,i,j)$ turns out to be realized, the hypothesis $(C,i,k)$ will also be realized. Again we can formulate this as a speculation: $(C,i,k)$ could be realized, in the case that $(A,i,j)$ turns out to be realized, and we can denote this as follows: $(C,i,k;A,i,j)$. This speculation can be made for any $i$ such that $0\leq i<j$.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072111309.png)

This completes the propose phase. In general, there are many possible Rytter proposals. For instance, the hypotheses in the initial $S_{CYK}$ set give rise to the following realized Rytter proposals:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072112094.png)

We will call the set of realized Rytter proposals $S_{Rytter}$.
The clever thing about Rytter proposals is that they can be combined, with CYK hypotheses, but also with other Rytter proposals. Brent and Goldschlager [224] combine Rytter proposals with CYK hypotheses to construct new Rytter proposals: if there is a grammar rule $C\to DE$ and both the Rytter proposal $(D,i,j;B,o,p)$ and the CYK hypothesis $(E,j,k)$ are realized, then $(C,i,k;B,o,p)$ will be realized.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072119278.png)

Likewise, with the same grammar rule, if both $(E, j, k ; B, o, p)$ and $(D, i, j)$ are realized, $(C, i, k ; B, o, p)$ will be realized.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072118559.png)

In our example, this gives the follow¥ing realized Rytter proposals:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072119276.png)

Rytter proposals can be combined too, as follows: If we have two realized Rytter proposals $(D,i,j;E,k,l)$ and $(F,o,p;D,i,j)$, we can construct a new Rytter proposal $(F,o,p;E,k,l)$, which is then realized. Again, this is just logic: if $A$ implies $B$, and $B$ implies $C$, then $A$ implies $C$, so if $(E,k,l)$ implies $(D,i,j)$, and $(D,i,j)$ implies $(F,o,p)$, then $(E,k,l)$ implies $(F,o,p)$.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072119511.png)

In Rytter's description, only Rytter proposals are combined.
This combination of Rytter proposals (also with CYK hypotheses) is the second building block, and we call it the "combine phase". This phase computes a new set of realized Rytter proposals, by first trying all possible combinations of Rytter proposals in $S_{Rytter}$ with CYK hypotheses in $S_{CYK}$, adding the result to $S_{Rytter}$, and then trying all possible combinations of Rytter proposals in $S_{Rytter}$. In the end, this set too is added to $S_{Rytter}$.

In our example $\mathbf{(D,0,4;D,1,3)}$ (which was found above) and $\mathbf{(D,1,3;A,1,2)}$ (which was found in the propose phase) can, for instance, be combined to $\mathbf{(D,0,4;A,1,2)}$. To the set of realized Rytter proposals above the following combinations can be added:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072120044.png)

Brent and Goldschlager [224] include a third stage in the combine phase: a CYK combine step on $S_{CYK}$ set, as we saw earlier Section 14.4.2. Rytter does not include such a phase. In our example, this stage results in the addition of the following CYK hypotheses to $S_{CYK}$:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072121995.png)

Perhaps not surprisingly, the third building block consists of the combination of Rytter proposals with CYK hypotheses. If we have a realized Rytter proposal $(A,i,j;B,o,p)$ and the CYK hypothesis $(B,o,p)$ turns out to be realized, then the CYK hypothesis $(A,i,j)$ will also be realized.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072121720.png)

All combinations of Rytter proposals in $S_{Rytter}$ and CYK hypotheses in $S_{CYK}$ are tried. In the end, all CYK hypotheses that turn out to be realized are added to $S_{CYK}$. This is called the "recognition phase".

In our example, the presence of $\mathbf{(D,0,3;A,1,2)}$ in $S_{Rytter}$ and $\mathbf{(A,1,2)}$ in $S_{CYK}$ results in the CYK hypothesis $\mathbf{(D,0,3)}$ turning out to be realized. From the initial $S_{CYK}$, and all proposals in $S_{Rytter}$, we can now determine that the following CYK hypotheses turn out to be realized:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072122426.png)

Now that we have the building blocks, we are ready to describe the algorithm itself. After initialization of the $S_{CYK}$ set, as described above, the following sequence of phases is performed repeatedly:

* the proposal phase, which takes the current $S_{CYK}$ and adds the Rytter proposals that can be derived from it to $S_{Rytter}$;
* the combine phase, which tries all possible combinations of proposals in $S_{Rytter}$ and hypotheses in $S_{CYK}$, adds the newly found proposals to $S_{Rytter}$, and then tries all possible combinations of proposals in $S_{Rytter}$, and again adds the newly found proposals to $S_{Rytter}$; also, realized CYK hypotheses are combined to find new realized CYK hypotheses, which are in the end placed in $S_{CYK}$.
* and the recognition phase, which combines hypotheses in $S_{CYK}$ with proposals in $S_{Rytter}$ to find new realized CYK hypotheses, which are in the end placed in $S_{CYK}$.

Now the question arises when to stop this repetition. The answer to that is easy: when a sequence of phases does not deliver any new realized Rytter proposals and no new CYK hypotheses, the next sequence will not deliver any new items either. This will happen at some point, because the number of CYK hypotheses and Rytter proposals is finite. So, the system stabilizes at some point, which can easily be detected.

In the end, when $S_{CYK}$ contains $(S,0,n)$ where $S$ is the start symbol of the grammar and $n$ is the length of the input sentence, the sentence is recognized. In fact, this may also be an added stop criterion: we can also stop the repetition when $S_{CYK}$ contains $(S,0,n)$. In this case, the sentence is recognized. This may suffice when a recognizer is all that is needed. For a parser, however, this is not a good stop-criterion, because some parsings may be missed.

Pursuing the example above, we see that the sentence is already recognized after one iteration, but the system is not quite stabilized yet: the propose phase of the second iteration adds the following Rytter proposals:

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072123093.png)

The combine phase adds the single proposal

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072123085.png)

after which the system is stable.

#### 14.4.3.1 Time Requirements of the Rytter Recognizer

The reader may by now be wondering what is gained by all this. After all, if we compare this to CYK, we only seem to do extra work. A CYK derivation step that combines $(A,i,j)$ with $(B,j,k)$ to $(C,i,k)$ if we have a rule $C\to AB$ seems to have been split in two: $(A,i,j)$ leads to $(C,i,k;B,j,k)$ (from the propose phase), and $(C,i,k;B,j,k)$ and $(B,j,k)$ together lead to $(C,i,k)$ (from the recognition phase). Indeed, without the combining of Rytter proposals we would not have gained anything. However, from our example above we learned that after one iteration of the algorithm, there already are realized CYK hypotheses of length 4. In fact, the sentence has been recognized after a single iteration. The algorithm is so powerful that sentences of length $2^{n}$ can be recognized in $O(n)$ iterations, in other words, sentences of length $n$ can be recognized in $O(^{2}\log\ n)$ iterations. To see why this is so, we first need to introduce some terminology.

We define the *size of a CYK hypothesis*$(A,i,j)$ as $(j-i)$, the length of the substring covered by the hypothesis. We also define the *size of a Rytter proposal*$(A,i,j;B,o,p)$ as $size(A,i,j)-size(B,o,p)$, which is the length of the covered substring minus the length of the gap. So, the size is the length of the recognized part.

In the remainder of this section, we will show that

$I_{k}$: If $(A,i,j)$ is a realizable CYK hypothesis with size $\leq 2^{k}$, it will be realized (be a member of $S_{CYK}$) after at most $k$ iterations of the algorithm. $II_{k}$: If $(A,i,j;B,o,p)$ is a realizable Rytter proposal with size $\leq 2^{k-1}$, it will be realized (be a member of $S_{Rytter}$) after at most $k$ iterations of the algorithm.

We will do so by showing that $II_{k}$ and $I_{k}$, together, imply $II_{k+1}$ and $I_{k+1}$.

For a complete proof, we also need $I_{0}$ and $II_{0}$ but these are trivial.

First, we will show that $II_{k}$ and $I_{k}$ imply $II_{k+1}$. Suppose that $(A,i,j;B,o,p)$ is a realizable Rytter proposal with $2^{k-1}<size(A,i,j;B,o,p)\leq 2^{k}$. We then have to show that it is a member of $S_{Rytter}$ after $k+1$ iterations. Since $(A,i,j;B,o,p)$ is realizable, there exists a (partial) parse tree with root $A$ for the substring $s_{i+1}\cdots s_{j}$, and somewhere in this tree there is a leaf node $B$ which is supposed to fill the gap. As there is a path from $A$ to every leaf in this tree, there is also a path to node $B$. The list of non-terminals on this path is denoted $C$, with $C_{0}=A,\ldots,C_{l}=B$, and each $C_{q}$ on this path derives a substring $s_{m_{q}+1}\cdots s_{n_{q}}$ that contains the substring derived from $C_{q+1}$: $s_{m_{q+1}+1}\cdots s_{n_{q+1}}$ (all with a gap for non-terminal $B$). This means that all Rytter proposals $(C_{r},m_{r},n_{r};C_{q},m_{q},n_{q})$ with $0\leq r<q\leq l$ are realizable. Such a list of CYK hypotheses $(C_{q},m_{q},n_{q})$ is called a *hypothesis path* for the realizable Rytter proposal $(A,i,j;B,o,p)$. See Figure 14.14, where we only show the non-terminals in the path.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072124241.png)

A step from $r$ to $r-1$ in this hypothesis path is called a *critical step* if $size(A,i,j;C_{r-1},m_{r-1},n_{r-1})\leq 2^{k-1}$ and $size(C_{r},m_{r},n_{r};B,o,p)\leq 2^{k-1}$. We will now first show that any hypothesis path of every realizable Rytter proposal has a critical step. The size of the Rytter proposals $(A,i,j;C_{q},m_{q},n_{q})$ is increasing monotonically with $q$, because the gap is shrinking. In the limits, $size(A,i,j;C_{0},m_{0},n_{0})=size(A,i,j;A,i,j)=0$, and $2^{(}k-1)<size(A,i,j;B,o,p)=size(A,i,j;C_{l},m_{l},n_{l})\leq 2^{k}$. This means that there exists an $r$ such that $size(A,i,j;C_{r-1},m_{r-1},n_{r-1})\leq 2^{k-1}$ and $2^{k-1}<size(A,i,j;C_{r},m_{r},n_{r})\leq 2^{k}$. Now, we have the following:

* since $2^{k-1}<size(A,i,j;C_{r},m_{r},n_{r})\leq 2^{k}$, this means that $2^{k-1}<(j-i)-(n_{r}-m_{r})\leq 2^{k}$;
* since $2^{k-1}<size(A,i,j;B,o,p)\leq 2^{k}$, this means that $2^{k-1}<(j-i)-(p-o)\leq 2^{k}$.
* $n_{r}-m_{r}\geq p-o$.

So, $2^{k-1}<(j-i)-(n_{r}-m_{r})\leq(j-i)-(p-o)\leq 2^{k}$, and thus $2^{k-1}<(j-i)-(n_{r}-m_{r})\leq(j-i)-(n_{r}-m_{r})+(n_{r}-m_{r})-(p-o)\leq 2 ^{k}$. This also means that $2^{k-1}+(n_{r}-m_{r})-(p-o)\leq 2^{k}$, so that $(n_{r}-m_{r})-(p-o)\leq 2^{k}-2^{k-1}=2\times 2^{k-1}-2^{k-1}=2^{k-1}$. This proves that $size(C_{r},m_{r},n_{r};B,o,p)=(n_{r}-m_{r})-(p-o)\leq 2^{k-1}$. This means that the step from $r$ to $r-1$ is a critical step.

Now suppose $r$ to $r-1$ is a critical step in the hypothesis path $(C_{q},m_{q},n_{q})$ for the realizable Rytter proposal $(A,i,j;B,o,p)$. Then there is either a realizable CYK hypothesis $(D,m_{r-1},m_{r})$ that combines with $(C_{r},m_{r},n_{r})$ to form $(C_{r-1},m_{r-1},n_{r})$ (in which case $n_{r-1}=n_{r}$ and there is a grammar rule $C_{r-1}{\rightarrow}DC_{r}$), or there is a realizable CYK hypothesis $(D,n_{r},n_{r-1})$ that combines with $(C_{r},m_{r},n_{r})$ to form $(C_{r-1},m_{r},n_{r-1})$ (in which case $m_{r-1}=m_{r}$ and there is a grammar rule $C_{r-1}{\rightarrow}C_{r}D$). In either case, we call the CYK hypothesis with non-terminal $D$$hyp_{D}$.

Figure 14.15 illustrates a hypothesis path with its critical step; some consequences are shown to the right of the figure. Since $D$ represents the filled

Figure 14.14: A hypothesis path for the realizable Rytter proposal $(A,i,j;B,o,p)$ part in the Rytter proposal $(C_{r-1},m_{r-1},n_{r-1};C_{r},m_{r},n_{r})$, we have $size(hyp_{D})=size(C_{r-1},m_{r-1},n_{r-1};C_{r},m_{r},n_{r})$, so that $size(hyp_{D})\leq 2^{k}$. Therefore, we know from $I_{k}$ that $hyp_{D}$ is a member of $S_{CYK}$ after $k$ iterations. Also, from $II_{k}$ we know that both $(A,i,j;C_{r-1},m_{r-1},n_{r-1})$ and $(C_{r},m_{r},n_{r};B,o,p)$ are a member of $S_{Rytter}$ after $k$ iterations.

![](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310072125308.png)

Now let us consider the steps of iteration $k+1$. The presence of $hyp_{D}$ in $S_{CYK}$ and $(C_{r},m_{r},n_{r};B,o,p)$ in $S_{Rytter}$ prompts the first part of the combine phase to propose, among others, the Rytter proposal $(C_{r-1},m_{r-1},n_{r-1};B,o,p)$. Next, the second stage of the combine phase of this iteration kicks in and combines this proposal with the proposal $(A,i,j;C_{r-1},m_{r-1},n_{r-1})$, which was already a member of $S_{Rytter}$, to produce the Rytter proposal $(A,i,j;B,o,p)$. This proves that $II_{k}$ and $I_{k}$ imply $II_{k+1}$.

Now, we have to show that $I_{k}$ and $II_{k+1}$ imply $I_{k+1}$. Again, we look more closely at how a CYK hypothesis $(A,i,j)$, with $2^{k-1}<size(A,i,j)\leq 2^{k}$ ends up in $S_{CYK}$: except for the initialization hypotheses, it must have come from a recognize step: some Rytter proposal $(A,i,j;B,o,p)$ must be a member of $S_{Rytter}$, and $(B,o,p)$ a member of $S_{CYK}$. The hypothesis $(B,o,p)$ is called a *critical hypothesis* for $(A,i,j)$ if $2^{k-1}<size(B,o,p)\leq 2^{k}$, and there is a grammar rule $B\to CE$ and there are realizable CYK hypotheses $(C,o,l)$ and $(E,l,p)$, both with size less than or equal to $2^{k-1}$. Of course, $(A,i,j)$ can also be its own critical hypothesis. In Problem 14.7 the reader is invited to prove that every realizable CYK hypothesis with size $\geq 2$ has a critical hypothesis. Also, if $(A,i,j;B,o,p)$ is a realizable Rytter proposal, and $(B,o,p)$ is a critical hypothesis to $(A,i,j)$, and $2^{k-1}<size(A,i,j)\leq 2^{k}$, then $size(A,i,j;B,o,p)\leq 2^{k-1}$.

Now, suppose $(A,i,j)$ is a realizable CYK hypothesis with $2^{k}<size(A,i,j)\leq 2^{k+1}$, and $(B,o,p)$ is a critical hypothesis for $(A,i,j)$. This means that there exist two realizable CYK hypotheses $(C,o,l)$ and $(E,l,p)$, each of size $\leq 2^{k}$ and there is a grammar rule $B\to CE$.

Figure 14.15: A critical step in a hypothesis path

Let us first assume that $(A,i,j)\neq(B,o,p)$. $\mathit{size}(A,i,j;B,o,p)\leq 2^{k}$. As we have seen above, this means that it gets proposed at the latest in the combine phase of iteration $k+1$.

Since $S_{CYK}$ contains $(C,o,l)$ and $(E,l,p)$, the CYK combination in the combine phase of iteration $k+1$ adds $(B,o,p)$ to $S_{CYK}$. Next, the recognize phase of the same iteration combines this with $(A,i,j;B,o,p)$ to produce $(A,i,j)$.

The case that $(A,i,j)=(B,o,p)$ is left as an exercise to the reader, as is the case that $(A,i,j)$ did not end up in $S_{CYK}$ as the result of a recognize step combining a Rytter proposal with a CYK hypothesis, but was the result of combining two CYK hypotheses.

Since Rytter [226, 227] does not combine CYK hypotheses, and also does not combine Rytter proposals with CYK hypotheses to produce new Rytter proposals, Rytter's version of the algorithm actually needs more than ${}^{2}\log n$, but still $O(\log n)$ iterations. Sikkel [158] proves that if you change the order of the steps in the iteration to "recognize, propose, combine, combine" (do the combine twice), the algorithm needs at most ${}^{2}\log n$ iterations. This order, however, seems less intuitive.

All in all, we now have seen that the algorithm needs $O(\log(n))$ iterations to determine if $(S_{S},0,n)$ is a member of $S_{CYK}$. What remains to be determined is that each iteration can be executed in constant time, given enough processors. In the next section, we will show how to build a Boolean circuit that will do just that.

#### 14.4.3.2 A Rytter Recognizer on a Boolean Circuit

We will now examine how each phase of the Rytter algorithm translates into a part of a Boolean circuit, but first we need to clear up some notation. Instead of using $A_{i,l}$, as we did earlier for denoting a node that represents the hypothesis that non-terminal $A$ recognizes a string of length $l$, starting at index $i$, we now use the indices as used in the description of the Rytter algorithm, so the notation would be $A_{i-1,i+l-1}$ for the same substring.

In the following, we switch to Rytter's description of the algorithm, because it is a bit simpler: it does not have an explicit combine of CYK hypotheses, and no combination of Rytter proposals and CYK hypotheses to produce a new Rytter proposal.

As in the CYK circuit, for each non-terminal and substring $s_{i+1}\cdots s_{j}$ we have an OR-node $A_{i,j}$. We also have OR-nodes for all possible Rytter proposals $(A,i,j;B,o,p)$. We will denote these nodes as $A_{i,j}\backslash B_{o,p}$.

Let us first look at the inputs of a node $A_{i,j}$. We know that the CYK hypothesis $(A,i,j)$ is realizable if a Rytter proposal $(A,i,j;B,o,p)$ is realizable and $(B,o,p)$ is realizable as well. So, an input of $A_{i,j}$ is the output of an AND-node with inputs from both $A_{i,j}\backslash B_{o,p}$ and $B_{o,p}$. This means that we have AND-nodes for each Rytter proposal $A_{i,j}\backslash B_{o,p}$ and CYK hypothesis $B_{o,p}$. This completes the recognize phase of the Rytter algorithm.

Now we turn our attention to the inputs of a node $A_{i,j}\backslash B_{o,p}$. A Rytter proposal can be realized in two ways: the proposal phase of the Rytter algorithm can propose it, or it can be constructed in the combine phase. Rytter proposals that are the result of the proposal phase have either the form $(A,i,j;B,p,j)$ or $(A,i,j;B,i,o)$. In the first case, there is a grammar rule $A\to CB$ and the "origin" of the proposal is a CYK hypothesis $(C,i,p)$, so the output of $C_{i,p}$ must be an input of $A_{i,j}\setminus B_{p,j}$. In the second case, there is a grammar rule $A\to BC$ and the "origin" of the proposal is a CYK hypothesis $(C,o,j)$, so the output of $C_{o,j}$ must be an input of $A_{i,j}\setminus B_{i,o}$.

The combine phase combines two Rytter proposals to one of a larger size. The outputs of the two Rytter proposals go to an AND-node, whose output serves as input for the result proposal: if both $A_{i,j}\setminus B_{o,p}$ and $B_{o,p}\setminus C_{k,l}$ are "on", both inputs of an AND-node, tentatively named $A_{i,j}\setminus B_{o,p}\setminus C_{k,l}$ are "on", so its output will be "on" in the next time step, turning node $A_{i,j}\setminus C_{k,l}$ on in the time step after that. We will call such a node a *Rytter combine node*.

This completes the building blocks of the Rytter recognizing circuit. As was the case with the CYK recognizing circuit, when the circuit is stabilized, the output of node $S_{0,n}$ determines whether a sentence $s_{1}\cdots s_{n}$ has been recognized or not.

We will not work out a practical example here. The next section will show the reader why not.

#### 14.4.3 Node Requirements for the Rytter Recognizer Circuit

In the previous section, we have seen the kinds of nodes that are needed for the Rytter recognizer circuit. There are

* OR-nodes for each CYK hypothesis $(A,i,j)$, so for each non-terminal $A$ and each $i$ and $j$ such that $0\leq i<j\leq n$. The number of these nodes is $O(n^{2})$.
* OR-nodes for each Rytter proposal $(A,i,j;B,o,p)$, so for all non-terminals $A$ and $B$ and each $i$, $j$, $o$, and $p$, such that $0\leq i\leq o<p\leq j\leq n$. So, there are $O(|V_{N}|^{2}n^{4})$ such nodes, where $V_{N}$ is the set of non-terminals.
* AND-nodes that we tentatively named $A_{i,j}\setminus B_{o,p}\setminus C_{k,l}$, the Rytter combine nodes. There are many of those, for each non-terminal $A$, $B$, $C$, and $i,j,o,p,k,l$ such that $0\leq i\leq o\leq k<l\leq p\leq j\leq n$, so as many as $|V_{N}|^{3}n^{6}$ nodes.
* AND-nodes for the recognize phase, there is one for each Rytter proposal.
* and finally, the nodes that are needed to initialize the circuit, exactly as with the CYK recognizing circuit. This are $O(n)$ nodes.

So, the total number of nodes is dominated by the AND-nodes that are needed for the combine phase, and thus the total number of nodes is $O(n^{6}|V_{N}|^{3})$. Again, although the time requirement is independent of $|V_{N}|$, the number of nodes needed depends on it, even more heavily than in the simple CYK recognizer.

The number of nodes required is considerable. Even a very small example, with an input of length 3, and a grammar like the one from Figure 14.9, with 4 non-terminals, would need the following nodes (where $C(k,n)$ is the number of combinations of $i_{1}\ldots i_{k}$ that fulfill $0\leq i_{1}\leq\ldots\leq i_{k}\leq n$): $4\times C(2,3)$ OR-nodes for the CYK hypotheses, + $4^{2}\times C(4,3)$ OR-nodes for the Rytter proposals, + $4^{3}\times C(6,3)$ AND-nodes for the combination phase, + $4^{2}\times C(4,3)$ AND-nodes for the recognition phase, + $4^{1}$ initialization nodes = $4\times 10+16\times 35+64\times 84+16\times 35+4=6540$ nodes. So a detailed picture like the one in Figure 14.13 is out of the question, and it would not be very interesting for the reader! Note, however, that it is less than 1/7of the number suggested by the order-of-magnitude formula above: $4^{3}\times 3^{6}=46656$ nodes. An Application-Specific Integrated Circuit (ASIC) for parsing a sentence of length 10 with a grammar with 10 non-terminals would have $O(10^{9})$ gates.

The data structures of the Rytter algorithm show interesting similarities to those used in the parser for tree-adjoining grammars in Section 15.4.2.

#### 14.4.3.4 Turning the Rytter Recognizer into a Parser

When the recognizer algorithm is finished (no more items are added during a complete iteration of the algorithm), the set of CYK hypotheses generally contains many hypotheses that did not contribute to the recognizing of the sentence. So, let us filter out the CYK hypotheses that should be part of the parse forest. These hypotheses are particularly easy to find: they are exactly the members $(A,i,j)$ of $S_{CYK}$ for which $(S,0,n;A,i,j)$ is a realized Rytter proposal. On the one hand, if both $(S,0,n;A,i,j)$ and $(A,i,j)$ are realized, $(S,0,n)$ will be realized, so the realizability of $(A,i,j)$ contributes to the recognition of the input. On the other hand, if $(A,i,j)$ contributes to the recognition of the input, this means two things:

* non-terminal $A$ derives substring $s_{i+1}\cdots s_{j}$, so $(A,i,j)$ will be realized at some stage of the algorithm;
* $S$ derives the sentential form $s_{1}\cdots s_{i}As_{j+1}\cdots s_{n}$, so $(S,0,n;A,i,j)$ will be realized at some stage of the algorithm.

So, this gives us all nodes in the parse forest, which is, strictly speaking, together with the grammar rules, enough to build the actual parse forest.

In our Boolean circuit for sentences of length $n$, we add an AND-node $A^{\prime}_{i,j}$ for each node $A_{i,j}$, with two inputs: one from node $A_{i,j}$ and one from $S_{0,n}\setminus A_{i,j}$. Perhaps amazingly, it only takes one time-step extra (over just recognizing) to determine all CYK hypotheses that are part of the parse forest. If this result is not quite satisfactory, for instance because it does not represent a "real" parse forest, the reader is referred to exercise 14.8.

## 14.5 Conclusion

We have discussed three methods of parallelizing the parsing process: multiple serial parsers, process-configuration parsers, and connectionist parsers. We have also seen examples of each. The boundaries between the three parallelizing methods are not always that clear, however; the difference between process-configuration and connectionist is more a matter of scale than anything else. The CYK parser, and even the Rytter parser, both presented here as connectionist parsers, could also be considered process-configuration parsers, albeit with extremely simple agents, and a large number of them. The literature references in (Web)Section 18.2.5 contain more examples, as do the references in the parallel parsing bibliography by Alblas et al. [234].

The parallelizing techniques presented here are quite parse-method specific. Janssen et al. [232] and Sikkel [158] describe the "primordial soup algorithm", a mechanism that allows for the specification of various parallel parsing algorithms without specifying flow control or data structures. This gives an abstract, elegant, and compact mathematical basis for the design of a parallel implementation.

## Problems

**Problem 14.1**: What should a Fischer parser do when it accepts its part as a complete sentence (in the example of Section 14.2: reduces its part to $\mathfrak{s}$)?

**Problem 14.2**: Apply the Fischer parser on the LR(0) example of Figure 9.18, on two processors, the sentence **n-(n-n) -(n) $\mathfrak{\hat{s}}$**, using two different splits: 1. **n-(n-n and ) -(n) $\mathfrak{\hat{s}}$** (equal length split), and 2. **n-(n-n)** and -(n) $\mathfrak{\hat{s}}$. Comment.

**Problem 14.3**: Read the paper by Bar-On and Vishkin [225] and write a program implementing their algorithm.

**Problem 14.4**: *Project:* Analyze the performance of the Sikkel and Lankhorst algorithm of Section 14.3.1. In your analysis, also consider the dependency on the size of the grammar.

**Problem 14.5**: Refer to Section 14.4.2.2. Explain why metaparsing and a reversed scan does not in general result in a circuit that represents a parse forest.

**Problem 14.6**: Proving something by calling it trivial is sometimes called "proof by intimidation". 2 In Section 14.4.3.1 we called $I_{0}$ and $II_{0}$ trivial. The reader is invited to check.

Footnote 2: A math professor did this while teaching, and was asked why the proof was trivial. He then left the room, to come back 15 minutes later and say: “Indeed, it is trivial”, and then proceeded, without further clarification.

**Problem 14.7**: Refer to Section 14.4.3.1. Why does every realizable CYK hypothesis $(A,i,j)$ of size $2^{k}$ with $k\geq 1$ always have a critical hypothesis?

**Problem 14.8**: Refer to Section 14.4.3.4, where a Boolean circuit is discussed that determines the CYK hypotheses that are actually nodes in the parse forest. Show how to extend this circuit so that it also reflects which grammar rules are used, along the lines of Figure 14.13. However, the circuit must still stabilize in $O(^{2}\log n)$ time steps.
