# Chapter 8. Deterministic Top-Down Parsing


In Chapter 6 we discussed two general top-down methods: one using breadth-ﬁrst search and one using depth-ﬁrst search.

These methods have in common the need to search to ﬁnd derivations, and thus are not efﬁcient. In this chapter and the next we will concentrate on parsers that do not have to search: there will always be only one possibility to choose from.

Parsers with this property are called _deterministic_. Deterministic parsers have several advantages over non-deterministic ones: they are much faster; they produce only one parse tree, so ambiguity is no longer a problem; and this parse tree can be constructed on the ﬂy rather than having to be retrieved afterwards. But there is a penalty: the class of grammars that the deterministic parsing methods are suitable for, while depending on the method chosen, is more restricted than that of the grammars suitable for non-deterministic parsing methods. In particular, only non-ambiguous grammars can be used.

In this chapter we will focus on deterministic top-down methods. As has been explained in Section 3.5.5, there is only one such method, this in contrast with the deterministic bottom-up methods, which will be discussed in the next chapter. From Chapters 3 and 6 we know that in a top-down parser we have a prediction for the rest of the input, and that this prediction has either a terminal symbol in front, in which case we “match”, or a non-terminal, in which case we “predict”.

It is the predict step that, until now, has caused us so much trouble. The predict step consists of replacing a non-terminal by one of its right-hand sides, and if we have no means to decide which right-hand side to select, we have to try them all. One restriction we could impose on the grammar, one that immediately comes to mind, is limiting the number of alternatives for each non-terminal to one. Then we would need no search, because no selection would be needed. However, such a restriction is far too severe, as it would leave us only with languages that consist of one word. So, limiting the number of right-hand sides per non-terminal to one is not a solution.

There are two sources of information that could help us in selecting the right right-hand side. First there is the partial derivation as it has been constructed so far. However, apart from the prediction this does not give us any information about the rest of the input. The other source of information is the rest of the input. We will see that looking ahead at the next symbol or the next few symbols will, for certain grammars, tell us which choice to make.

## 8.1 Replacing Search by Table Look-Up

As a first step we will consider a simple form of grammars which make it particularly easy to limit the search, grammars in which each right-hand side starts with a terminal symbol. In this case, a predict step is always immediately followed by a match step, matching the next input symbol with the symbol starting the right-hand side selected in the prediction. This match step can only succeed for right-hand sides that start with this input symbol. The other right-hand sides will immediately lead to a match step that will fail. We can use this fact to limit the number of predictions as follows: only the right-hand sides that start with a terminal symbol that is equal to the next input symbol will be considered. For example, consider the grammar of Figure 6.1, repeated in Figure 8.1, and the input sentence aabb. Using the breadth-first
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070238966.png)

top-down method of Chapter 6, extended with the observation described above, results in the steps of Figure 8.2: Frame a presents the start of the automaton; we have appended the \# end marker both to the initial prediction and the input. Only one right-hand side of \mathbf{S} starts with an a, so this is the only applicable right-hand side; this leads to frame b. Next, a match step leads to c. The next input symbol is again an a, so only one right-hand side of B is applicable, resulting in frame d. Frame e is the result of a match step; this time, the next input symbol is a b, so two right-hand sides of \mathbf{B} are applicable; this leads to f. Frame g is the result of a match step; again, the next input symbol is a b, so two right-hand sides of \mathbf{B} and one right-hand side of \mathbf{S} are applicable; this leads to frame h. This again calls for a match step, leading to i. Now there are no applicable right-hand sides for \mathbf{S} and \mathbf{A}, because there are no right-hand sides starting with a \#; thus, these predictions are dead ends. This leaves a match step for the only remaining prediction, leading to frame j.
We could enhance the efficiency of this method even further by precomputing the applicable right-hand sides for each non-terminal/terminal combination, and enter these in a table. For the grammar of Figure 8.1, this would result in the table of Figure 8.3. Such a table is called a parse table or a parsing table.
Despite its title, most of this chapter concerns the construction of these parse tables. Once such a parse table is obtained, the actions of the parser are obvious. The parser does not need the grammar any more. Instead, every time a predict step is called for, the parser uses the next input symbol and the non-terminal at hand as

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070238679.png)
indices in the parse table. The corresponding table entry contains the right-hand sides that have to be considered. For example, in Figure 8.2(e), the parser would use input symbol \mathrm{b} and non-terminal \mathrm{B} to determine that it has to consider the right-hand sides \mathbf{B}_{1} and \mathbf{B}_{2}. If the corresponding table entry is empty, we have found an error in the input and the input sentence cannot be derived from the grammar. Using the parse table of Figure 8.3 instead of the grammar of Figure 8.1 for parsing the sentence aabb will again lead to Figure 8.2. The advantage of using a parse table is that we do not have to check all right-hand sides of a non-terminal any more to see if they start with the right terminal symbol.
We still have a search process, albeit a more limited one than we had before. Given a prediction A and an input token a, the search is now confined to the elements of the parse table entry for [A, a]. This means that we now only need a search because of the [A, a] and the [B, b] entries of the table. These entries have more than one element, so we need the search to determine which one results in a derivation of the input sentence.
This last observation is an important one: it immediately leads to a restriction that we could impose on the grammar, to make the parsing deterministic: we could require that each parse table entry contain at most one element. In terms of the grammar, this means that all right-hand sides of a non-terminal start with a different terminal symbol. A grammar which fulfills this requirement is called a simple L L(1) grammar (SLL(1)), or an s-grammar. "LL(1)" means that the grammar allows a deterministic parser that operates from L eft to right, produces a Left-most derivation, using a look-ahead of one (1) symbol.
Consider for example the grammar of Figure 8.4. This grammar generates all

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070239253.png)
sentences starting with a number of as, followed by an equal number of bs. The grammar is clearly SLL(1). It leads to the parse table of Figure 8.5. The parsing

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070239724.png)
of the sentence aabb is presented in Figure 8.6. Again we have added the \# end marker to signal termination. As expected, there is always only one prediction, so
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070239925.png)
So, SLL(1) grammars lead to simple and very efficient parsers. However, the restrictions that we have placed on the grammar are severe. Not many practical grammars are SLL(1), although many can be transformed into SLL(1) form. In the next section, we will consider a more general class of grammars that still allows the same kind of parser.

## 8.2 LL(1) Parsing

For the deterministic top-down parser described in the previous section, the crucial restriction placed on the grammar is that all right-hand sides of a non-terminal start with a different terminal symbol. This ensures that each parse table entry contains at most one element. In this section, we will drop the requirement that right-hand sides start with a terminal symbol. We will see that we can still construct a parse table in that case. Later on, we will see that we can even construct a parse table for grammars with \varepsilon-rules.

### 8.2.1 LL(1) Parsing without \varepsilon-Rules

If a grammar has no \varepsilon-rules, there are no non-terminals that derive the empty string. In other words, each non-terminal ultimately derives strings of terminal symbols of length at least one, and this also holds for each right-hand side. The terminal symbols that start these strings are the ones that we are interested in. Once we know for each right-hand side which terminal symbols can start a string derived from this righthand side, we can construct a parse table, just as we did in the previous section. So, we have to compute this set of terminal symbols for each right-hand side.

#### 8.2.1.1 $FIRST_{1}$ Sets

These sets of terminal symbols are called the "FIRST 1 sets": if we have a non-empty sentential form x, then \operatorname{FIRST}_{1}(x) is the set of terminal symbols that can start a sentential form derived from x in zero or more production steps. The subscript { }_{1} indicates that the set contains single terminal symbols only. Later, we will see FIRST k sets, consisting of strings of terminal symbols of length at most k. For now, we will symbol, then \operatorname{FIRST}(x) is a set that has this symbol as its only member. If x starts with a non-terminal A, then \operatorname{FIRST}(x) is equal to \operatorname{FIRST}(A), because A cannot produce \varepsilon. So, if we can compute the FIRST set for any non-terminal A, we can compute it for any sentential form x. However, \operatorname{FIRST}(A) depends on the right-hand sides of the A-rules: it is the union of the FIRST sets of these right-hand sides. These FIRST sets may again depend on the FIRST set of some non-terminal. This could even be A itself, if the rule is directly or indirectly left-recursive. This observation suggests the iterative process described below to compute the FIRST sets of all non-terminals:

- We first initialize the FIRST sets to the empty set.

- Then we process each grammar rule in the following way: if the right-hand side starts with a terminal symbol, we add this symbol to the FIRST set of the lefthand side, since it can be the first symbol of a sentential form derived from the left-hand side. If the right-hand side starts with a non-terminal symbol, we add all symbols of the present FIRST set of this non-terminal to the FIRST set of the left-hand side. These are all symbols that can be the first terminal symbol of a sentential form derived from the left-hand side.
- The previous step is repeated until no more new symbols are added to any of the FIRST sets.
Eventually, no more new symbols can be added, because the maximum number of elements in a FIRST set is the number of symbols, and the number of FIRST sets is equal to the number of non-terminals. Therefore, the total number of times that a new symbol can be added to any FIRST set is limited by the product of the number of symbols and the number of non-terminals. This is an example of a transitive closure algorithm.

#### 8.2.1.2 Producing the Parse Table

With the help of these FIRST sets, we can now construct a parse table for the grammar. We process each grammar rule A \rightarrow \alpha in the following way: if \alpha starts with a terminal symbol a, we add the right-hand side \alpha to the (A, a) entry of the parse table; if \alpha starts with a non-terminal, we add \alpha to the (A, a) entry of the parse table for all symbols a in FIRST (\alpha). This parse table can then be used for parsing as described in Section 8.1.
Now let us compute the parse table for the example grammar of Figure 8.7. This
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070240091.png)
grammar describes a simple language that could be used as the input language for a rudimentary consulting system: the user enters some facts, and then asks a question. There is also a facility for sub-sessions. The contents of the facts and questions are of no concern here. They are represented by the word STRING, which is regarded as a terminal symbol.
We first compute the FIRST sets. Initially, the FIRST sets are all empty. Then, we process all grammar rules in the order of Figure 8.7. The grammar rule Session \rightarrow Fact Session results in adding the symbols from FIRST(Fact) to FIRST(Session), but FIRST(Fact) is still empty. The grammar rule Session \rightarrow Question results in adding the symbols from FIRST(Question)
to FIRST(Session), but FIRST(Question) is still empty too. The grammar rule Session \rightarrow (Session) Session results in adding ( to FIRST(Session). The grammar rule Fact \rightarrow ! STRING results in adding! to FIRST(Fact), and the grammar rule Question \rightarrow ? STRING results in adding ? to FIRST(Question). So, after processing all right-hand sides once, we have the following:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070241760.png)
Next, we process all grammar rules again. This time, the grammar rule Session \rightarrow Fact Session will result in adding ! (from FIRST(Fact)) to FIRST(Session), the grammar rule Session \rightarrow Question will result in adding ? to FIRST(Session), and no other changes will take place. So now we get:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070242812.png)
There were some changes, so we have to repeat this process once more. This time, there are no changes, so the table above presents the FIRST sets of the non-terminals.
Now we have all the information we need to create the parse table. We have to add Fact Session to the [Session, a ] entry for all terminal symbols a in FIRST(Fact Session). The only terminal symbol in FIRST(Fact session) is !, so we add Fact Session to the [Session,!] entry. Likewise, we add Question to the [Session,?] entry. Next we add ( Session ) Session to the [Session, (] entry, ! STRING to the [Fact,!] entry, and ? STRING to the [Question,?] entry. This results in the parse table of Figure 8.8, where we show just the right-hand sides of the predicted rules in the entries, since the left-hand sides are already shown as the indexes on the left. All parse table entries have at
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070242498.png)
most one right-hand side, so the parser is deterministic. A grammar without \varepsilon-rules is called L L(1) if all entries of the parse table, as constructed above, have at most one element, or, in other words, if for every non-terminal A the FIRST sets of A are pairwise disjoint (no symbol occurs in more than one). If two or more such FIRST sets contain the same symbol, we have a FIRST/FIRST conflict and the grammar is not LL(1).
We have lost the S (simplicity) of SLL(1), but the parser is still as simple as before. Producing the parse table has become more difficult, but we have gained a lot: many practical grammars are \operatorname{LL}(1), or are easily transformed into an \operatorname{LL}(1) grammar.

### 8.2.2 LL(1) Parsing with \varepsilon-Rules

Not allowing \varepsilon-rules is, however, still a major drawback. Certain language constructs are difficult, if not impossible, to describe with an LL(1) grammar without \varepsilon-rules. For example, non-terminals that describe lists of terminals or non-terminals are difficult to express without \varepsilon-rules. Of course, we could write

$$
\mathbf{A} \rightarrow aA \mid \mathrm{a}
$$
for a list of as, but this is not LL(1). Compare also the grammar of Figure 8.7 with the one of Figure 8.9. They describe the same language, but the one of Figure 8.9 is much clearer.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070243008.png)

#### 8.2.2.1 Extending the FIRST Sets

The main problem with allowing \varepsilon-rules is that the FIRST sets, as we have discussed them in the previous section, are not sufficient any more. For example, the Facts non-terminal in the grammar of Figure 8.9 has an \varepsilon-rule. The FIRST set for this right-hand side is empty, so it does not tell us on which look-ahead symbols we should choose this right-hand side. Also, in the presence of \varepsilon-rules, the computation of the FIRST sets itself needs some revision. For example, if we compute the FIRST set of the first right-hand side of Session using the method of the previous section, ? will not be a member, but it should, because Facts can derive \varepsilon (it is transparent), and then ? starts a sentential form that can be derived from Session.
Let us first extend the FIRST definition to also deal with \varepsilon-rules. This time, in addition to terminal symbols, \varepsilon will also be allowed as a member of a FIRST set. We will now also have to deal with empty sentential forms, so we will sometimes need the FIRST (\varepsilon) set; we will define it as the set containing only the empty string \varepsilon. We will also add \varepsilon to the FIRST set of a sentential form if this sentential form derives \varepsilon.
These may seem minor changes, but the presence of \varepsilon-rules affects the computation of the FIRST sets. FIRST \left(u_{1} u_{2} \cdots u_{n}\right), which was simply equal to \operatorname{FIRST}\left(u_{1}\right), is now computed as follows. We take FIRST \left(u_{1}\right), examine if it contains \varepsilon, and if so, we remove the \varepsilon and replace it by \operatorname{FIRST}\left(u_{2} \cdots u_{n}\right). Apart from this, the computation of the revised FIRST sets proceeds in exactly the same way as before, using the same transitive closure technique.
The treatment of \varepsilon is easy to understand: if \operatorname{FIRST}\left(u_{1}\right) contains \varepsilon, it is transparent, so the tokens in \operatorname{FIRST}\left(u_{2} \cdots u_{n}\right) show up through it, and the original \varepsilon disappears in the process. Of course the same algorithm is used to compute \operatorname{FIRST}\left(u_{2} \cdots u_{n}\right), etc. This chain of events ends at the first u_{i} whose \operatorname{FIRST}\left(u_{i}\right) does not contain \varepsilon. If all of the \operatorname{FIRST}\left(u_{1}\right), \operatorname{FIRST}\left(u_{2}\right), \ldots \operatorname{FIRST}\left(u_{n}\right) contain \varepsilon, the last step is the addition of \operatorname{FIRST}(\varepsilon) to \operatorname{FIRST}\left(u_{1} u_{2} \cdots u_{n}\right), thus showing that the whole alternative is transparent.
For some algorithms we need to know whether a non-terminal A derives \varepsilon. Although we could compute this information separately, using the method described in Section 4.2.1, we can more easily see if \varepsilon is a member of the \operatorname{FIRST}(A) set as computed. This method uses the fact that if a non-terminal derives \varepsilon, \varepsilon will ultimately be a member of its FIRST set.
Now let us compute the FIRST sets for the grammar of Figure 8.9. They are first initialized to the empty set. Then, we process each grammar rule: the rule Session \rightarrow Facts Question results in adding the terminal symbols from FIRST(Facts) to FIRST(Session). However, FIRST(Facts) is still empty. The rule Session \rightarrow ( Session) Session results in adding ( to FIRST(Session). Then, the rule Facts \rightarrow Fact Facts results in adding the symbols from FIRST(Fact) (still empty) to FIRST(Facts), and the rule Facts \rightarrow \varepsilon results in adding \varepsilon to FIRST(Facts). Then, the rule Fact \rightarrow ! STRING results in adding ! to FIRST(Fact), and the rule Question \rightarrow ? STRING adds ? to FIRST(Question). This completes the first pass over the grammar rules, resulting in:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070244033.png)
The second pass is more interesting: this time, we know that Facts derives \varepsilon, and therefore the rule Session \rightarrow Facts Question adds the symbols from FIRST(Question) (?) to FIRST(Session). The rule Facts \rightarrow Fact Facts adds! to FIRST(Facts). So we get:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070244712.png)
In the third pass, the only change is the addition of ! to FIRST(Session), because it is now a member of FIRST(Facts). So we have:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070245338.png)
The fourth pass does not result in any new additions.
The question remains how to decide when an \varepsilon right-hand side or, for that matter, a right-hand side which derives \varepsilon is to be predicted. Suppose that we have a grammar rule
$$
A \rightarrow \alpha_{1}\left|\alpha_{2}\right| \cdots \mid \alpha_{n}
$$
where \alpha_{m} is or derives \varepsilon. Now suppose we find A at the front of a prediction, as in
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070246011.png)

where we again have added the # end marker. A breadth-first parser would have to investigate the following predictions:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070247476.png)
We know how to compute the FIRST sets of these predictions, and we know that none of them contains \varepsilon, because of the end marker (\#). If the next input symbol is not a member of any of these FIRST sets, either the prediction we started with (A x \#) is wrong, or there is an error in the input sentence. Otherwise, the next input symbol is a member of one or more of these FIRST sets, and we can strike out the predictions that do not have the symbol in their FIRST set. Now, if none of these FIRST sets have a symbol in common with any of the other FIRST sets, the next input symbol can only be a member of at most one of these FIRST sets, so at most one prediction remains, and the parser is deterministic at this point.
A context-free grammar is called L L(1) if this is always the case. In other words, a grammar is LL(1) if for any prediction A x \#, with A a non-terminal with righthand sides \alpha_{1}, \ldots, \alpha_{n}, the sets FIRST \left(\alpha_{1} x \#\right), \ldots, \operatorname{FIRST}\left(\alpha_{n} x \#\right) are pairwise disjoint (no symbol is a member of more than one set). This definition of LL(1) does not conflict with the one that we gave on page 241 for grammars without \varepsilon-rules. In that case \operatorname{FIRST}\left(\alpha_{i} x \#\right) is equal to \operatorname{FIRST}\left(\alpha_{i}\right) since \alpha_{1} is not transparent, so the above definition reduces to the requirement that all \operatorname{FIRST}\left(\alpha_{1}\right), \ldots, \operatorname{FIRST}\left(\alpha_{n}\right) be pairwise disjoint.
The above is the official definition of an LL(1) grammar (possibly with \varepsilon-rules), but since it requires dynamic computation of FIRST sets, it is usually replaced in practice by a simplified form which allows precomputation at parser generation time. This version is described in the next section. Unfortunately it is often also called "LL(1)", although the official term is "strong-LL(1)"; to avoid confusion we will often use the term "full LL(1)" for the dynamic version described above. Section 8.2.4 gives a reasonably efficient implementation of full LL(1), which allows a large degree of precomputation.

#### 8.2.2.2 The Need for FOLLOW Sets

With the above building blocks we can in principle construct a deterministic parser for any LL(1) grammar. This parser operates by starting with the prediction S \#, and its prediction steps consist of replacing the non-terminal at hand with each of its right-hand sides, computing the FIRST sets of the resulting predictions, and checking whether the next input symbol is a member of any of these sets. We then continue with the predictions for which this is the case. If there is more than one prediction, the parser announces that the grammar is not LL(1) and stops.
Although this is a deterministic parser, it is far from ideal. First, it does not use a parse table like the one in Figure 8.8, and it does not check the LL(1) property of the grammar until parsing time; we would like to check that property during parser generation time, while computing the parse tables. Second, it is not very efficient, because it has to compute several FIRST sets at each prediction step. We cannot precompute these FIRST sets, because in the presence of \varepsilon-rules such a FIRST set depends on all of the predictions (of which there are infinitely many), not just on the first non-terminal. So we still do not know whether, and if so, how we can construct a parse table for an LL(1) grammar with \varepsilon-rules, nor do we have a method to determine if such a grammar is LL(1).
Now suppose we have a prediction A x \# and a rule A \rightarrow \alpha, and \alpha is or derives \varepsilon. The input symbols that lead to the selection of A \rightarrow \alpha are the symbols in the set \operatorname{FIRST}(\alpha x \#), and as we have seen this set is formed by the symbols in \operatorname{FIRST}(\alpha), extended with the symbols in FIRST (x \#), because of the transparency of \alpha. The set \operatorname{FIRST}(x \#) is the problem: we cannot compute it at parser generation time. What we can precompute, though, is the union of all \operatorname{FIRST}(x \#) sets such that x \# can follow A in any prediction. This is just the set of all terminal symbols that can follow A in any sentential form derivable from S \# (not just the present prediction) and is called, quite reasonably, the FOLLOW set of A, FOLLOW (A).
It would seem that such a gross approximation would seriously weaken the parser or even make it incorrect, but this is not so. Suppose that this set contains a symbol a that is not a member of \operatorname{FIRST}(x \#), and a is the next input symbol. If a is not a member of \operatorname{FIRST}(A), we will predict A \rightarrow \alpha, and we will ultimately end up with a failing match, because \alpha x \# does not derive a string starting with an a. So the input string will (correctly) be rejected, although the error will be detected a bit later than before, because the parser may make some \varepsilon-predictions before finding out that something is wrong. If a is a member of \operatorname{FIRST}(A) then we may have a problem if a is a member of one of the FIRST sets of the other right-hand sides of A. We will worry about this a bit later.
The good thing about FOLLOW sets is that we can compute them at parser generation time. Each non-terminal has a FOLLOW set, and they can be computed as follows:

- as with the computation of the FIRST sets, we start with the FOLLOW sets all empty.
- Next we process all right-hand sides, including the S \# one. Whenever a righthand side contains a non-terminal, as in A \rightarrow \cdots B y, we add all symbols from FIRST (y) to FOLLOW (B), since these symbols can follow a B. In addition, if y derives \varepsilon, we add all symbols from FOLLOW (A) to FOLLOW (B).
- The previous step is repeated until no more new symbols can be added to any of the FOLLOW sets.
This is again an example of a transitive closure algorithm.
Now let us go back to our example and compute the FOLLOW sets. Starting with Session \#, \# is added to FOLLOW(Session). Next, the symbols of FIRST(Question) (?) are added to FOLLOW(Facts), because of the rule Session \rightarrow Facts Question. This rule also adds all symbols of FOLLOW(Session) (\#) to FOLLOW(Question).
The rule Session \rightarrow ( Session) session results in adding the ) symbol to FOLLOW(Session) and in a spurious addition of all symbols of FOLLOW(Session) to FOLLOW(Session). The next rule is the rule Facts \rightarrow Fact Facts. All symbols from FIRST(Facts) (!) are added to FOLLOW(Fact), and since Facts produces empty, all symbols from FOLLOW(Facts) (?) are added to FOLLOW(Fact). The other rules do not result in any additions. So, after the first pass we have:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070248952.png)
In the second pass, ) is added to FOLLOW(Question), because it is now a member of FOLLOW(session), and all members of FOLLOW(Session) become a member of FOLLOW(Question) because of the rule Session \rightarrow Facts Question.
In the third pass no changes take place. The resulting FOLLOW sets are presented below:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070248756.png)

#### 8.2.2.3 Using the FOLLOW Sets to Produce a Parse Table

Once we know the FOLLOW set for each non-terminal that derives \varepsilon, we can construct a parse table. First we compute the FIRST set of each non-terminal. This also tells us which non-terminals derive \varepsilon. Next, we compute the FOLLOW set of each non-terminal. Then, starting with an empty parse table, we process each grammar rule A \rightarrow \alpha as follows: we add the right-hand side \alpha to the [A, a] entry of the parse table for all terminal symbols a in \operatorname{FIRST}(\alpha), as we did before. This time however, we also add \alpha to the [A, a] entry of the parse table for all terminal symbols a in FOLLOW (A) when \alpha is or derives \varepsilon (when \operatorname{FIRST}(\alpha) contains \varepsilon ). A shorter way of saying this is that we add \alpha to the [A, a] entry of the parse table for all terminal symbols a in FIRST( \alpha FOLLOW (A) ). This last set consists of the union of the FIRST sets of the sentential forms \alpha b for all symbols b in FOLLOW (A).
If a token in a FOLLOW set causes the addition of a right-hand side to an entry that already contains a right-hand side due to a token in a FIRST set, we have a FIRST/FOLLOW conflict, and the grammar is not LL(1). It is even possible to have a FOLLOW/FOLLOW conflict: an entry receives two right-hand sides, both brought in by tokens from FOLLOW sets. This happens if more than one alternative of a non-terminal can produce \varepsilon.
Now let us produce a parse table for our example. The Session \rightarrow Facts Question rule does not derive \varepsilon, because Question does not. Therefore, only the terminal symbols in FIRST(Facts Question) lead to addition of this rule to the table. FIRST(Facts Question) contains ! from FIRST(Facts) and ? from FIRST(Question) because Facts derives \varepsilon. So the right-hand side Facts Question must be entered in the entries [Session,!] and [Session,?]; the right-hand side ( Session ) Session should be added to entry [Session, (].
FIRST(Fact Facts) is \{!\}, so this right-hand side is entered in [Facts, !]. Since the right-hand side of Facts \rightarrow \varepsilon produces \varepsilon but has an otherwise empty FIRST set, its look-ahead set is FOLLOW(Facts), which contains just ?; so the right-hand side \varepsilon is entered in entry [Facts,?].
Similarly, all other rules are added, resulting in the parse table presented in Figure 8.10 .
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070249376.png)

### 8.2.3 LL(1) versus Strong-LL(1)

If all entries of the resulting parse table have at most one element, the parser is again deterministic. In this case, the grammar is called strong- L L(1) and the parser is called a strong-LL(1) parser. In the literature, strong- \operatorname{LL}(1) is often referred to as "strong LL(1)", without a hyphen between the words "strong" and "LL". This is misleading because it indicates that "strong" belongs to "grammar" rather than to "LL(1)", which in turn suggests that the class of strong-LL(1) grammars is more powerful than the class of LL(1) grammars. This is not the case: every strong-LL(1) grammar is LL(1).
It is perhaps more surprising that every LL(1) grammar is strong-LL(1). In other words, every grammar that is not strong-LL(1) is not LL(1), and this is demonstrated with the following argument: if a grammar is not strong-LL(1), there is a parse table entry, say (A, a), with at least two elements, say \alpha and \beta. This means that a is a member of both FIRST (\alpha \operatorname{FOLLOW}(A)) and \operatorname{FIRST}(\beta \operatorname{FOLLOW}(A)). Now there are three possibilities:

- \quad a is a member of both FIRST (\alpha) and FIRST( \beta). In this case, the grammar cannot be LL(1), because for any prediction A x \#, a is a member of both \operatorname{FIRST}(\alpha x \#) and \operatorname{FIRST}(\beta x \#).
- a is a member of either \operatorname{FIRST}(\alpha) or \operatorname{FIRST}(\beta), but not both. Let us assume, without loss of generality, that a is a member of \operatorname{FIRST}(\alpha). In this case, a is still a member of \operatorname{FIRST}(\beta \operatorname{FOLLOW}(A)), so there is a prediction A x \#, such that a is a member of \operatorname{FIRST}(\beta x \#). However, a is also a member of \operatorname{FIRST}(\alpha x \#), so the grammar is not \mathrm{LL}(1). In other words, in this case there is a prediction in which an LL(1) parser cannot decide which right-hand side to choose either.
- a is neither a member of \operatorname{FIRST}(\alpha), nor a member of \operatorname{FIRST}(\beta). In this case \alpha and \beta must derive \varepsilon and a must be a member of \operatorname{FOLLOW}(A). This means that there is a prediction A x \# such that a is a member of \operatorname{FIRST}(x \#) and thus a is a member of both FIRST (\alpha x \#) and FIRST( \beta x \#), so the grammar is not LL(1). This means that in an LL(1) grammar at most one right-hand side of any non-terminal derives \varepsilon.

### 8.2.4 Full LL(1) Parsing

We already mentioned briefly that an important difference between LL(1) parsing and strong-LL(1) parsing is that the strong-LL(1) parser sometimes makes \varepsilon predictions before detecting an error. Consider for example the following grammar:

$$
\begin{array}{l}S_{S} \rightarrow a A b \mid b A \text { a }
 A \rightarrow C S \mid \varepsilon
\end{array}
$$
The strong-LL(1) parse table of this grammar is:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070300704.png)
Now, on input sentence $\boldsymbol{aacabb}$ , the strong-LL(1) parser makes the following moves:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310070301044.png)

The problem here is that the prediction is destroyed by the time the error is detected. In contrast, a full-LL(1) parser would not do the last step, because neither $FIRST(b\#)$, nor $FIRST(cSb\#)$ contain a, so the full-LL(1) parser would detect the error before choosing a right-hand side for $\mathrm{A}$. A full-LL(1) parser has the immediate error detection property, which means that an error is detected as soon as the erroneous symbol is first examined, whereas a strong-LL(1) parser only has the correctprefix property, which means that the parser detects an error as soon as an attempt is made to match (or shift) the erroneous symbol. In Chapter 16, we will see that the immediate error detection property will help improve error recovery.

Given a prediction $A \cdots \#$, a full-LL(1) parser bases its parsing decisions on $\operatorname{FIRST}(A \cdots \#)$ rather than on the approximation $\operatorname{FIRST}(A FOLLOW (A))$; this avoids any parsing decisions on erroneous input symbols (which can never occur in $\operatorname{FIRST}(A \cdots \#)$ but may occur in $\operatorname{FIRST}(A \operatorname{FOLLOW}(A)))$. So, if we have prediction $A \cdots \#$ and input symbol a, we first have to determine if a is a member of $FIRST (A \cdots \#)$, before consulting the parse table to choose a right-hand side for A. The penalty for this is in efficiency: every time that parse table has to be consulted, a FIRST set has to be computed and a check made that the input symbol is a member. Fortunately, we can do better than this. A first step to improvement is the following: suppose that we maintain between all symbols in the prediction a set of terminal symbols that are correct at this point, like this:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071529659.png)
Here, (1) is the set of symbols that are legal at this point; this is just the FIRST set of the remaining part of the prediction: FIRST(\#); likewise, (2) is \operatorname{FIRST}(Z \#), (3) is FIRST(YZ\#), and (4) is FIRST(XYZ\#). These sets can easily be computed, from right to left. For example, (3) consists of the symbols in FIRST (Y), with the symbols from (2) added if Y derives \varepsilon (if \varepsilon is a member of \operatorname{FIRST}(Y) ). When a non-terminal is replaced by one of its right-hand sides, the set behind this right-hand side is available, and we can use this to compute the sets within this right-hand side and in front of it. Since none of these sets contain \varepsilon, they give an immediate answer to the question which prediction to choose.
Now let us see how this works for our example. As the reader can easily verify,

$$
\begin{array}{l}\operatorname{FIRST}(\mathbf{s})=\{\mathbf{a}, \mathbf{b}\}, \text { and }
 \operatorname{FIRST}(\mathbf{A})=\{\mathbf{c}, \boldsymbol{\varepsilon}\} .\end{array}
$$
The parser starts with the prediction S\#. We have to find a starting point for the sets: it makes sense to start with an empty one to the right of the \#, because no symbols are correct after the \#. So the parser starts in the following state:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071530423.png)
The first input symbol is a member of the current FIRST set, so it is correct. The (s, a) entry of the parse table contains a A b, so we get parser state

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071530532.png)
Computing the sets marked with a question mark from right to left results in the following parser state:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071530419.png)
Note thatbnow is a member of the set in front of $\mathbf{A}$, but $\mathbf{a}$ is not, although it is a member of ${FOLLOW(A)}$. After the match step, the parser is in the following state:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071530771.png)
The next input symbol is not a member of the current FIRST set, so an error is detected, and no right-hand side of \mathbf{A} is chosen. Instead, the prediction is left intact, so error recovery can profit from it.
It is not clear that all this is more efficient than computing the FIRST set of a prediction to determine the correctness of an input symbol before choosing a righthand side. However, it does suggest that we can do this at parser generation time, by combining non-terminals with the FIRST sets that can follow it in a prediction. For our example, we always start with non-terminal $\mathbf{S}$ and the set \{\#\}. We will indicate this with the pair $[\mathbf{S},\{\#\}]$. Starting with this pair, we will try to make rules for the behavior of each pair that turns up, for each valid look-ahead. We know from the FIRST sets of the alternatives for $\mathbf{S}$ that on look-ahead symbol a, $[$\mathbf{S}$,\{\#\}]$ results in right-hand side $\mathbf{a} $\mathbf{A b}$. Now the only symbol that can follow $\mathbf{A}$ here is $\mathbf{a} \mathbf{b}$. So in fact, we have:

on look-ahead symbol $\mathbf{a},[\mathbf{S},\{\#\}]$ results in right-hand side $\mathbf{a}[\mathbf{A},\{\mathbf{b}\}]$ \mathbf{b}.
Similarly we find:
on look-ahead symbol $\mathbf{b},[\mathbf{S},\{\#\}]$ results in right-hand side $\mathbf{b}[\mathbf{A},\{\mathbf{a}\}] \mathbf{a}$.
We have now obtained pairs for $\mathbf{A}$ followed by $\mathbf{a} \mathbf{b}$, and $\mathbf{A}$ followed by an $\mathbf{a}$. So we have to make rules for them: We know that on look-ahead symbol $\mathbf{c},[\mathbf{A},\{\mathbf{b}\}]$ results in right-hand side $\mathbf{C S}$. Because $\mathbf{A}$ can only be followed by \mathbf{a} \mathbf{b} in this context, the same holds for this \mathbf{S}. This gives:
on look-ahead symbol \mathbf{c},[\mathbf{A},\{\mathbf{b}\}]$ results in right-hand side $\mathbf{c}[\mathbf{S},\{\mathbf{b}\}]$.
Likewise, we get the following rules:
on look-ahead symbol $\mathbf{b},[\mathbf{A},\{\mathbf{b}\}]$ results in right-hand side $\varepsilon$;
on look-ahead symbol $\mathbf{c},[\mathbf{A},\{\mathbf{a}\}]$ results in right-hand side $\mathbf{c}[\mathbf{S},\{\mathbf{a}\}]$;
on look-ahead symbol a, $[\mathbf{A},\{\mathbf{a}\}]$ results in right-hand side \varepsilon.
Now we have to make rules for the pairs \mathbf{s} followed by an \mathbf{a}, and \mathbf{s} followed by \mathbf{a} \mathbf{b} :
on look-ahead symbol $\mathbf{a},[\mathbf{S},\{\mathbf{a}\}]$ results in right-hand side $\mathbf{a}[\mathbf{A},\{\mathbf{b}\}] \mathbf{b}$; on look-ahead symbol $\mathbf{b},[\mathbf{S},\{\mathbf{a}\}]$ results in right-hand side $\mathbf{b}[\mathbf{A},\{\mathbf{a}\}] \mathbf{a}$;
on look-ahead symbol $\mathbf{a},[\mathbf{S},\{\mathbf{b}\}]$ results in right-hand side $\mathbf{a}[\mathbf{A},\{\mathbf{b}\}] \mathbf{b}$; on look-ahead symbol $\mathbf{b},[\mathbf{S},\{\mathbf{b}\}]$ results in right-hand side \$mathbf{b}[\mathbf{A},\{\mathbf{a}\}] \mathbf{a}$.

In fact, we find that we have rewritten the grammar, using the (non-terminal, followed-by set) pairs as non-terminals, into the following form:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071605900.png)

For this grammar, the following parse table can be produced:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071607555.png)

The entries for the different [\mathbf{S}, \ldots] rules are identical so we can merge them. After that, the only change with respect to the original parse table is the duplication of the A-rule: now there is one copy for each context in which \mathbf{A} has a different set behind it in a prediction.
Now, after accepting the first \mathbf{a} of \mathbf{a} a \mathbf{c a b} \mathbf{b}, the prediction is [\mathbf{A},\{\mathbf{b}\}] \mathbf{b} \#; since the parse table entry ([\mathbf{A},\{\mathbf{b}\}], \mathbf{a}) is empty, parsing will stop here and now.
The resulting parser is exactly the same as the strong-LL(1) one. Only the parse table is different. Often, the LL(1) table is much larger than the strong-LL(1) one. As the benefit of having an LL(1) parser only lies in that it detects some errors a bit earlier, this usually is not considered worth the extra cost, and thus most parsers that are advertised as LL(1) parsers are actually strong-LL(1) parsers.
In summary, confronted with a prediction stack A \alpha and a grammar rule A \rightarrow \beta,

- a (full) LL(1) parser bases its decisions on the FIRST set of \beta \alpha and the first token of the input;
- a strong-LL(1) parser bases its decisions on the FIRST set of \beta, the FOLLOW set of A when \beta produces \varepsilon, and the first token of the input;
- a simple-LL(1) parser bases its decisions on the first token of \beta and the first token of the input.

### 8.2.5 Solving LL(1) Conflicts

If a parse table entry has more than one element, we have an " LL(1) conflict". In this section, we will discuss how to deal with them. We have already seen one way to deal with conflicts: use a depth-first or a breadth-first parser with a one symbol look-ahead. This, however, has several disadvantages: the resulting parser is not deterministic any more, it is less efficient (often to such an extent that it becomes
unacceptable), and it still does not work for left-recursive grammars. Therefore, we want to try and eliminate these conflicts, so we can use an ordinary LL(1) parser.

Two techniques that can help us here are left recursion elimination and leftfactoring. These can be performed by hand relatively easily and are described in the next two sections. Grammars for which these two techniques are sufficient are called "kind grammars"; see Žemlička and Král [106, 107, 108] for their precise definition and processing.

#### 8.2.5.1 Left-Recursion Elimination

The first step to take is the elimination of left recursion. Left-recursive grammars always lead to LL(1) conflicts, because the right-hand side causing the left recursion has a FIRST set that contains all symbols from the FIRST set of the non-terminal. Therefore, it also contains all terminal symbols of the FIRST sets of the other right-hand sides of the non-terminal. Eliminating left recursion has already been discussed in Section 6.4.

#### 8.2.5.1 Left-Recursion Elimination

The first step to take is the elimination of left recursion. Left-recursive grammars always lead to LL(1) conflicts, because the right-hand side causing the left recursion has a FIRST set that contains all symbols from the FIRST set of the non-terminal. Therefore, it also contains all terminal symbols of the FIRST sets of the other right- hand sides of the non-terminal. Eliminating left recursion has already been discussed in Section 6.4.

#### 8.2.5.2 Left-Factoring

A further technique for removing LL(1) conflicts is $\textit{left-factoring}$ . Left-factoring of grammar rules is like factoring arithmetic expressions:
$$
a \times b+a \times c=a \times(b+c).
$$
The grammatical equivalent to this is a rule
$$
A \rightarrow x y \mid x z
$$
which clearly has an LL(1) conflict on the terminal symbols in $FIRST(x)$. We replace this grammar rule with the two rules
$$
\begin{array}{l}A \rightarrow x N
 N \rightarrow y \mid z\end{array}
$$
where $N$ is a new non-terminal. There have been some attempts to automate this process; see Foster [405], Hammer [406], and Rosenkrantz and Hunt [408].

#### 8.2.5.3 Conflict Resolvers

Sometimes, these techniques do not help much. We could for example be dealing with a language for which no LL(1) grammar exists. In fact, many languages can be described by a context-free grammar, but not by an LL(1) grammar. Another method of handling conflicts is to resolve them by so-called disambiguating rules. An ex- ample of such a disambiguating rule is: “on a conflict, the textually first of the con- flicting right-hand sides is chosen”. With this disambiguating rule, the order of the right-hand sides within a grammar rule becomes crucial, and unexpected results may occur if the grammar-processing program does not clearly indicate where conflicts occur and how they are resolved.
A better method is to have the grammar writer specify explicitly how each con- flict must be resolved, using so-called conflict resolvers. One option is to resolve conflicts at parser generation time. Parser generators that allow this kind of conflict resolver usually have a mechanism that enables the user to indicate (at parser gener- ation time) which right-hand side must be chosen on a conflict. Another, much more flexible method is to have conflicts resolved at parse time. When the parser meets a conflict, it calls a user-specified conflict resolver. Such a conflict resolver has the complete left-context at its disposal, so it could base its choice on this left context. It is also possible to have the parser look further ahead in the input, and then resolve the conflict based on the symbols found. See Milton, Kirchhoff and Rowland [337] and Grune and Jacobs [362], for similar approaches using attribute grammars. (Attribute grammars are discussed in Section 15.3.1.)

### 8.2.6 LL(1) and Recursive Descent

Most hand-written parsers are LL(1) parsers. They usually are written in the form of a non-backtracking compiled recursive-descent parser (see Section 6.6). In fact, this is a very simple way to implement a strong-LL(1) parser. For a non-terminal A with grammar rule
$$
A \rightarrow \alpha_{1}|\cdots| \alpha_{n}
$$
the parsing routine has the following structure:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071559697.png)

The look-ahead symbol always resides in a variable called “look_ahead”. The pro- cedure ERROR announces an error and stops the parser.

The code for a right-hand side consists of the code for the symbols of the right- hand side. A non-terminal symbol results in a call to the parsing routine for this non-terminal, and a terminal symbol results in a call to a MATCH routine with this symbol as parameter. This MATCH routine has the following structure:
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071558743.png)

The NEXTSYM procedure reads the next symbol from the input.
Several LL(1) parser generators produce a recursive descent parser instead of a parse table that is to be interpreted by a grammar-independent parser. The advantages of generating a recursive descent parser are numerous:
- Semantic actions are easily embedded in the parsing routines.
- A parameter mechanism or attribute mechanism comes virtually for free: the parser generator can use the parameter mechanism of the implementation language.
- Non-backtracking recursive descent parsers are quite efficient, often more efficient than the table-driven ones.
- Dynamic conflict resolvers are implemented easily.
The most important disadvantage of generating a recursive descent parser is the size of the parser. A recursive descent parser is usually larger than a table-driven one (including the table). With present computer memories this is no longer a problem, however.

## 8.3 Increasing the Power of Deterministic LL Parsing

There are many situations in which a look-ahead of one token is not enough. A prime example is the definition of an element of an expression in a programming language:
$$\mathbf{\ \  element_{S} \ \  \rightarrow \ idf \  | \ \  idf \ \  ( \  parameters \ ) \ | \ idf \ [ \ \  indexes \ \   ]\ } $$
where idf produces identifiers. This grammar fragment defines expression elements like $\mathbf{\mathbf{x}, \sin (0.41)}$, and $\mathbf{\mathrm{T}[3,1]}$, each of which starts with an identifier; only the second token allows us to distinguish between the alternatives.
There are several ways to increase the power of deterministic LL parsing, and we have already seen one above: conflict resolvers. This section concentrates on extending the look-ahead, first to a bounded number of tokens and then to an unbounded number. In between we treat an efficient compromise.

### 8.3.1 LL (k) Grammars

It is possible and occasionally useful to have a look-ahead of k symbols with k>1, leading to $\mathrm{LL}(k)$ grammars. To achieve this, we need a definition of $\mathrm{FIRST}_{k}$ sets: if x is a sentential form, then $\operatorname{FIRST}_{k}(x)$ is the set of terminal strings w such that |w| (the length of w ) is less than k and $x \stackrel{*}\rightarrow{} w$, or $|w|$ is equal to k, and $x\stackrel{*} \rightarrow wy$, for some sentential form y. For k=1 this definition coincides with the definition of the FIRST sets as we have seen it before.
We now have the instruments needed to define $\operatorname{LL}(k)$ : a grammar is $LL(k)$ if for any prediction $A x \#^{k}$, with A a non-terminal with right-hand sides $\alpha_{1}, \ldots, \alpha_{n}$, the sets $\operatorname{FIRST}_{k}\left(\alpha_{1} x \#^{k}\right), \ldots, \operatorname{FIRST}_{k}\left(\alpha_{n} x \#^{k}\right)$ are pairwise disjoint. (Here $\#^{k}$ represents a sequence of $k \# \mathrm{~s}$; they are required to supply enough look-ahead tokens for checking near the end of the input string.) Obviously, for any k, the set of $\operatorname{LL}(k)$ grammars is

a subset of the set of $\operatorname{LL}(k+1)$ grammars, and in fact, for any k there are $\operatorname{LL}(k+1)$ grammars that are not $\operatorname{LL}(k)$. A trivial example of this is given in Figure 8.11. Less

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071557420.png)

obvious is that for any k there are languages that are $\operatorname{LL}(k+1)$, but not $\operatorname{LL}(k)$. An example of such a language is given in Figure 8.12. See Kurki-Suonio [42] for more details.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071556123.png)

With $\operatorname{LL}(k)$ grammars we have the same problem as with the LL(1) grammars: producing a parse table is difficult. In the LL(1) case, we solved this problem with the aid of the FOLLOW sets, obtaining strong-LL(1) parsers. We can try the same with $\mathrm{LL}(k)$ grammars using FOLLOW k sets. For any non-terminal A, $\mathrm{FOLLOW}_{k}(A)$ is now defined as the union of the sets $\operatorname{FIRST}_{k}\left(x \#^{k}\right)$, for any prediction $A x \#^{k}$.
Once we have the FIRST k sets and the FOLLOW k sets, we can produce a parse table for the grammar. Like the LL(1) parse table, this parse table will be indexed with pairs consisting of a non-terminal and a terminal string of length equal to k. Every grammar rule $A \rightarrow \alpha$ is processed as follows: $\alpha$ is added to the (A, w) entry of the table for every w in $\operatorname{FIRST}_{k}\left(\alpha \operatorname{FOLLOW}_{k}(A)\right. )$ (as we have seen before, this last set denotes the union of several FIRST k sets: it is the union of all $\operatorname{FIRST}_{k}(\alpha v)$ sets with v an element of $FOLLOW \left._{k}(A)\right)$. All this is just an extension to k look-ahead symbols of what we did earlier with one look-ahead symbol.
If this results in a parse table where all entries have at most one element, the grammar is strong-LL(k). Unlike the LL(1) case however, for k>1 there are grammars that are $\operatorname{LL}(k)$, but not strong-LL (k). An example of such a grammar is given in Figure 8.13.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071556347.png)

This raises an interesting question, one that has kept the authors busy for quite a while: why is it different for k=1 ? If we try to repeat our proof from Section 8.2.3 for a look-ahead k>1, we see that we fail at the very last step: let us examine a strong-LL (k) conflict: suppose that the right-hand sides $\alpha$ and $\beta$ both end up in the (A, w) entry of the parse table. This means that w is a member of both $\operatorname{FIRST}_{k}(\alpha \left.\mathrm{FOLLOW}_{k}(A)\right)$ and $\mathrm{FIRST}_{k}\left(\beta \mathrm{FOLLOW}_{k}(A)\right)$. Now there are three cases:

-  $w$ is a member of both $\operatorname{FIRST}_{k}(\alpha)$ and $\operatorname{FIRST}_{k}(\beta)$. In this case, the grammar cannot be $\operatorname{LL}(k)$, because for any prediction $A x \#^{k}$, w is a member of both $\operatorname{FIRST}_{k}\left(\alpha x \#^{k}\right)$ and $\operatorname{FIRST}_{k}\left(\beta x \#^{k}\right)$.
- $w$ is a member of either $\operatorname{FIRST}_{k}(\alpha)$ or $\operatorname{FIRST}_{k}(\beta)$, but not both. Let us say that w is a member of $\operatorname{FIRST}_{k}(\alpha)$. In this case, w still is a member of $\operatorname{FIRST}_{k}(\beta FOLLOW _{k}(A) )$ so there is a prediction $A x \#^{k}$, such that w is a member of $\operatorname{FIRST}_{k}\left(\beta x \#^{k}\right)$. However, w is also a member of $\operatorname{FIRST}_{k}\left(\alpha x \#^{k}\right)$, so the grammar is not $\operatorname{LL}(k)$. In other words, in this case there is a prediction in which an $\mathrm{LL}(k)$ parser cannot decide which right-hand side to choose either.
- $w$ is neither a member of $\operatorname{FIRST}_{k}(\alpha)$ nor a member of $\operatorname{FIRST}_{k}(\beta)$. Here, we have to deviate from the reasoning we used in the LL(1) case. As w is an element of $\operatorname{FIRST}_{k}\left(\alpha \mathrm{FOLLOW}_{k}(A)\right)$, w can now be split into two parts $w_{1.1}$ and $w_{1.2}$, such that $w_{1.1}$ is an element of $\operatorname{FIRST}_{k}(\alpha)$ and $w_{1.2}$ is a non-empty start of an element of FOLLOW k(A). Likewise, w can be split into two parts $w_{2.1}$ and $w_{2.2}$ such that $w_{2.1}$ is an element of $\operatorname{FIRST}_{k}(\beta)$ and $w_{2.2}$ is a non-empty start of an element of $\mathrm{FOLLOW}_{k}(A)$. So we have the following situation:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071555687.png)

Now, if w_{1.1}=w_{2.1}, w_{1.1} is a member of $\operatorname{FIRST}_{k}(\alpha)$, as well as $\operatorname{FIRST}_{k}(\beta)$, and there is a prediction $A x \#^{k}$ such that $x \#^{k}>^{*} w_{1.2} \cdots$. So $\operatorname{FIRST}_{k}\left(\alpha x \#^{k}\right)$ contains w and so does $\operatorname{FIRST}_{k}\left(\beta x \#^{k}\right)$, and therefore, the grammar is not $\operatorname{LL}(k)$. So the only case left is that $w_{1.1} \neq w_{2.1}$. Neither $w_{1.2}$ nor $w_{2.2}$ are $\varepsilon$, and this is just impossible if $|w|=1$.

Strong-LL (k) parsers with k>1 are seldom used in practice, partly because the gain is marginal and the same effect can often be obtained by using conflict resolvers, and partly because the parse tables can be large. That problem may, however, have been exaggerated in the literature, since the table entries are mostly empty and the tables lend themselves very well to table compression.
To obtain a full-LL(k) parser, the method that we used to obtain a full-LL(1) parser can be extended to deal with pairs (A, L), where L is a FIRST k set of $\mathbf{x \#^{k}}$ in some prediction $\mathbf{A x \#^{k}}$. This extension is straightforward and will not be discussed further.

### 8.3.2 Linear-Approximate LL(k)

terminals productions of a non-terminal or alternative. We then choose an alternative A if the first token of the input is in $\operatorname{FIRST}(A)$ and the second token of the input is in $\operatorname{SECOND}(A)$. Rather than a table of size $O\left(t^{2}\right)$, where t is the number of terminals in the grammar, this technique needs two tables of size O(t); also, the tables are easier to generate. This gives us a poor man's version of LL(2), called "linear-approximate LL(2)", and, with the introduction of THIRD, FOURTH, etc. sets, linear-approximate L L(k).
If the first non-terminal in an alternative produces only one token, its FOLLOW set must be called upon to create the correct SECOND set for that alternative. With this provision, it is easy to see that linear-approximate LL(2) efficiently and cheaply handles the grammar fragment for expression elements at the beginning of Section 8.3 on page 254 .
In principle linear-approximate $\operatorname{LL}(2)$ is weaker than $\operatorname{LL}(2)$, because it breaks the relationships between the two tokens in the look-ahead sets. If two alternatives in an LL(2) parser have look-ahead sets of $\{a b, c d\}$ and $\{a d, c b\}$ respectively, they are disjoint; but under linear-approximate LL(2) both have a FIRST set of a c and a SECOND set of b d, so they are no longer disjoint. In practice this effect is rare, though.
Linear-approximate LL was first described by Parr and Quong [51], who also give implementation details.

### 8.3.3 LL-Regular

$\mathrm{LL}(k)$ provides bounded look-ahead, but grammar rules like $\mathrm{A} \rightarrow \mathrm{Bb} \mid \mathrm{BC}$ with $\mathrm{B}$ producing for example $a^{*}$ show that bounded look-ahead will not always suffice: the discriminating token can be arbitrarily far away.
This suggests unbounded look-ahead, but that is easier said than done. Unbounded look-ahead is much more important and has been investigated much more extensively in LR parsing, and is treated in depth in Section 9.13.2. We will give here just an outline of the LL version; for details see Jarzabek and Krawczyk [44], Nijholt [45], Poplawski [47], and Nijholt [48].
If bounded look-ahead is not enough, we need a way to describe the set of unbounded look-ahead sequences, which suggests a grammar. And indeed it turns out that each alternative defines its own context-free look-ahead grammar. But this has two problems: it solves the parsing problem by almost doing the same parsing, and we cannot decide if the look-ahead grammars of two alternatives are disjoint. To solve both problems we approximate the $\mathrm{CF}$ grammars by regular grammars (hence the term "LL-regular"): disjointness of regular expressions can be decided, and there is a trick to do regular analysis of the input only once for the entire parsing.
There is no hard and fast algorithm for the approximation of the CF grammars with regular grammars, but there are many heuristics.
LL-regular is probably of theoretical interest only; if a parser writer goes through that much trouble, the effort is more wisely spent on LR-regular. Still, it offers many interesting insights, and the parser landscape would be incomplete without it.

## 8.4 Getting a Parse Tree Grammar from LL(1) Parsing

Getting a parse tree grammar from LL(1) parsing is straightforward. The basic idea is to create a new grammar rule for each prediction; the non-terminals in the right-hand side are numbered using an increasing global counter, and the resulting right-hand side is also inserted in the prediction stack. This produces numbered non-terminals in the prediction stack, which then lead to the creation of more newly numbered rules. The created rules then form the parse tree grammar. Since the parser is deterministic, there is only one parse, and we obtain a parse _tree_ grammar rather than a parse forest grammar.

To see how it works in some more detail we refer to the grammar in Figure 8.9 and parse table 8.10. We start with a prediction stack $\mathbf{Session\_1  \ \#}$ , a lookahead ! and a global counter which now stands at 2. For non-terminal $\mathbf{Session}$ and look-ahead! the table predicts $\mathbf{Session \  \rightarrow \ Facts \  Question}$. So we generate the parse tree grammar rule $\mathbf{Session\_1 \rightarrow Facts\_2 Question\_3}$ where $\mathbf{Session\_1}$ obtains its number from the prediction and the $Facts\_2$ and $\mathbf{Question\_3}$ obtain their numbers from the global counter. Next we turn the prediction stack into $\mathbf{Facts\_2 Question\_3 \ \#}$. For Facts and ! the parse table yields the prediction Facts \rightarrow Fact Facts which gives us the parse tree grammar rule $\mathbf{Facts\_2 \rightarrow Fact\_4 Facts\_5}$ and a stack $\mathbf{Fact\_4 \ Facts\_5 \ Question\_3 \ \#}$. The next step is similar and produces the grammar rule $\mathbf{Fact\_4 \ \rightarrow \ ! \ STRING}$ and a stack $\mathbf{\ ! \ \ STRING \ Facts\_5 \ Question\_3 \ \#}$. Now we are ready to match the !.
This process generates successive layers of the parse tree, using non-terminal names like Question_3 and Facts_5 as forward pointers. See Figure 8.14, where the leaves of the tree spell the absorbed input followed by the prediction stack. When the parsing is finished, the leaves spell the input string.

This process generates successive layers of the parse tree, using non-terminal names like **Question_3** and **Facts_5** as forward pointers. See Figure 8.14, where the leaves of the tree spell the absorbed input followed by the prediction stack. When the parsing is finished, the leaves spell the input string.
![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071621162.png)
There is no need to clean the resulting grammar. It cannot have undefined non- terminals: each non-terminal created in a right-hand side is also put on the prediction stack and a subsequent prediction will create a rule for it. It cannot have unreachable

non-terminals either: rules are only created for non-terminals in the prediction stack, and these all derive from predictions that ultimately derive from the start symbol.
## 8.5 Extended LL(1) Grammars

Several parser generators accept an extended context-free grammar instead of an ordinary one. See for example Lewi et al. [46], Heckmann [49], and Grune and Jacobs [362]. Extended context-free grammars have been discussed in Chapter 2. To check that an extended context-free grammar is LL(1), we have to transform the extended context-free grammar into an ordinary one, in a way that will avoid introducing LL(1) conflicts. For example, the transformation for $\mathsf{Something}^{+}$ given in Chapter 2:
$$
\mathbf{Something ^{+} \rightarrow Something  |  Something Something { }^{+}}
$$
will not do, because it will result in an $LL(1)$ conflict on the symbols in $\mathbf{FIRST(Something)}$. Instead, we will use the following transformations:

$$
\begin{array}{lll}
\mathbf{Something ^{*} \rightarrow \varepsilon \mid Something Something* } \\
\mathbf{Something ^{+} \rightarrow Something Something } \\
\mathbf{Something? }
\end{array}
$$

If the resulting grammar is LL(1), the original extended context-free grammar was ELL(1) (Extended LL(1)). This is the recursive interpretation of Chapter 2. Parser generation usually proceeds as follows: first transform the grammar to an ordinary context-free grammar, and then produce a parse table for it.
Extended LL(1) grammars allow a more efficient implementation in recursive  descent parsers. In this case, $\mathbf{Something^?}$ can be implemented as an if statement:

![image.png](https://blog-1314253005.cos.ap-guangzhou.myqcloud.com/202310071547641.png)
Here procedure calls have been replaced by much more efficient repetitive constructs.

## Conclusion

LL(1) parsing is a method with a strong intuitive appeal: the parser reads the input from left to right, making decisions on its next step based on its expectation (prediction stack) and the next token in the input. In some sense it "just follows the signs". The process can be implemented conveniently as a set of mutually recursive routines, one for each non-terminal. If there were no $\epsilon$-rules, that would be about the whole story.

An $\epsilon$-rule does not produce tokens, so it does not provide signs to follow. Instead it is transparent, which makes us consider the set of tokens that can occur after it. This set is dynamic, and it cannot be precomputed, but derives from the prediction. It can, however, be approximated from above, by using the FOLLOW set; a precomputable linear-time parser results.

The power of deterministic LL parsing can be increased by extending the look-ahead, to bounded length, resulting in LL($k$) parsing, or to unbounded length, in LL-regular parsing. Linear-approximate LL(2) is a convenient and simplified form of LL(2) parsing.

## Problems

**Problem 8.1**: Under what conditions is a grammar LL(0)? What can be said about the language it produces?

**Problem 8.2**: An LL(1) grammar is converted to CNF, as in Section 4.2.3. Is it still LL(1)?

**Problem 8.3**: In an LL(1) grammar all non-terminals that have only one alternative are substituted out. Is the resulting grammar still LL(1)?

**Problem 8.4**: What does it mean when a column for a token $t$ in an LL(1) parse table is completely empty (for example # in Figure 8.5)?

**Problem 8.5**: $a$. Is the following grammar LL(1)?

$$\begin{array}{ccc}\mathbf{s}_{\mathrm{S}}&\rightarrow&\mathbf{A}\ \mathbf{b}\ \ |\ \ \mathbf{A}\ \mathbf{c}
 \mathbf{A}&\rightarrow&\epsilon\end{array}$$

Check with your local LL(1) parser generator. $b$. Same question for

$$\begin{array}{ccc}\mathbf{s}_{\mathrm{S}}&\rightarrow&\mathbf{A}\ \mathbf{b}\ \ |\ \ \mathbf{A}\ \mathbf{c}
 \mathbf{A}&\rightarrow&\mathbf{a}\ \ |\ \ \epsilon\end{array}$$

$c$. Same question for

$$\begin{array}{ccc}\mathbf{s}_{\mathrm{S}}&\rightarrow&\mathbf{A}
 \mathbf{A}&\rightarrow&\mathbf{a}\ \mathbf{A}\end{array}$$

**Problem 8.6**: In Section 8.2.5.1 we give a simple argument showing that no left-recursive grammar can be LL(1): the union of the FIRST sets of the non-left-recursive alternatives would be equal the FIRST set of the left-recursive alternative, thus causing massive FIRST/FIRST conflicts. But what about the grammar which is left-recursive, obviously not LL(1), but the FIRST sets of both alternatives contain only $\varepsilon$ ?
$$
\begin{array}{l}\mathrm{s}_{\mathrm{S}} \rightarrow \mathrm{S} \text { B } \mid \varepsilon \\ \text { B } \rightarrow \varepsilon \\\end{array}
$$

**Problem 8.7**: Devise an algorithm to check if a given parse table could have originated from an LL(1) grammar through the LL(1) parse table construction process.

**Problem 8.8**: Devise an efficient table structure for an LL($k$) parser where $k$ is fairly large, say between 5 and 20. (Such grammars may arise in grammatical data compression, Section 17.5.1.)
